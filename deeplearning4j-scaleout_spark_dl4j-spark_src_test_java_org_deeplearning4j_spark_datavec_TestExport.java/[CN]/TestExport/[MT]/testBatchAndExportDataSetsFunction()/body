{
  System.out.println("HADOOP TEMP DIRECTORY: " + sc.hadoopConfiguration().get("hadoop.tmp.dir"));
  String baseDir=System.getProperty("java.io.tmpdir");
  baseDir=FilenameUtils.concat(baseDir,"dl4j_spark_testBatchAndExport/");
  baseDir=baseDir.replaceAll("\\\\","/");
  File f=new File(baseDir);
  if (f.exists())   FileUtils.deleteDirectory(f);
  f.mkdir();
  f.deleteOnExit();
  int minibatchSize=5;
  int nIn=4;
  int nOut=3;
  List<DataSet> dataSets=new ArrayList<>();
  dataSets.add(new DataSet(Nd4j.create(10,nIn),Nd4j.create(10,nOut)));
  for (int i=0; i < 98; i++) {
    if (i % 2 == 0) {
      dataSets.add(new DataSet(Nd4j.create(5,nIn),Nd4j.create(5,nOut)));
    }
 else {
      dataSets.add(new DataSet(Nd4j.create(1,nIn),Nd4j.create(1,nOut)));
      dataSets.add(new DataSet(Nd4j.create(1,nIn),Nd4j.create(1,nOut)));
      dataSets.add(new DataSet(Nd4j.create(3,nIn),Nd4j.create(3,nOut)));
    }
  }
  Collections.shuffle(dataSets,new Random(12345));
  JavaRDD<DataSet> rdd=sc.parallelize(dataSets);
  rdd=rdd.repartition(1);
  JavaRDD<String> pathsRdd=rdd.mapPartitionsWithIndex(new BatchAndExportDataSetsFunction(minibatchSize,"file:///" + baseDir),true);
  List<String> paths=pathsRdd.collect();
  assertEquals(100,paths.size());
  File[] files=f.listFiles();
  assertNotNull(files);
  int count=0;
  for (  File file : files) {
    if (!file.getPath().endsWith(".bin"))     continue;
    System.out.println(file);
    DataSet ds=new DataSet();
    ds.load(file);
    assertEquals(minibatchSize,ds.numExamples());
    count++;
  }
  assertEquals(100,count);
  FileUtils.deleteDirectory(f);
}
