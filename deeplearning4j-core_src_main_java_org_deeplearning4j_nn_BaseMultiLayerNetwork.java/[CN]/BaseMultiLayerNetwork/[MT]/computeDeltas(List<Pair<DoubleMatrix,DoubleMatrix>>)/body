{
  DoubleMatrix[] gradients=new DoubleMatrix[nLayers + 2];
  DoubleMatrix[] deltas=new DoubleMatrix[nLayers + 2];
  ActivationFunction derivative=getSigmoidLayers()[0].getActivationFunction();
  ActivationFunction softMaxDerivative=Activations.softmax();
  DoubleMatrix delta=null;
  List<DoubleMatrix> activations=feedForward(getInput());
  List<DoubleMatrix> weights=new ArrayList<>();
  for (int j=0; j < getLayers().length; j++)   weights.add(getLayers()[j].getW());
  weights.add(getLogLayer().getW());
  for (int i=nLayers + 1; i >= 0; i--) {
    if (i >= nLayers + 1) {
      delta=labels.sub(activations.get(i)).neg().mul(softMaxDerivative.applyDerivative(activations.get(i)));
      deltas[i]=delta;
    }
 else {
      deltas[i]=deltas[i + 1].mmul(weights.get(i).transpose()).muli(derivative.applyDerivative(activations.get(i)));
      DoubleMatrix newGradient=deltas[i + 1].transpose().mmul(activations.get(i));
      gradients[i]=newGradient;
    }
  }
  for (int i=0; i < gradients.length; i++)   deltaRet.add(new Pair<>(gradients[i],deltas[i]));
}
