{
  Double lastEntropy=this.negativeLogLikelihood();
  BaseMultiLayerNetwork revert=clone();
  if (forceNumEpochs) {
    for (int i=0; i < epochs; i++) {
      backPropStep(revert,lr,i);
    }
  }
 else {
    boolean train=true;
    int count=0;
    int numOver=0;
    int tolerance=3;
    double changeTolerance=1e-5;
    while (train) {
      count++;
      backPropStep(revert,lr,count);
      getLogLayer().trainTillConvergence(lr,epochs);
      Double entropy=negativeLogLikelihood();
      if (lastEntropy == null || entropy < lastEntropy) {
        double diff=Math.abs(entropy - lastEntropy);
        if (diff < changeTolerance) {
          log.info("Not enough of a change on back prop...breaking");
          break;
        }
 else         lastEntropy=entropy;
        log.info("New negative log likelihood " + lastEntropy);
        getLogLayer().trainTillConvergence(lr,epochs);
        revert=clone();
      }
 else       if (entropy >= lastEntropy) {
        train=false;
      }
    }
  }
}
