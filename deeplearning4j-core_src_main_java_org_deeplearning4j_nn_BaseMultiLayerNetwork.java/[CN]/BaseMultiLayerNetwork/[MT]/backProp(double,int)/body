{
  Double lastEntropy=null;
  BaseMultiLayerNetwork revert=clone();
  if (forceNumEpochs) {
    for (int i=0; i < epochs; i++) {
      backPropStep(revert,lr,i);
      lastEntropy=negativeLogLikelihood();
    }
  }
 else {
    logLayer.trainTillConvergence(lr,epochs);
    boolean train=true;
    int count=0;
    int numOver=0;
    int tolerance=3;
    while (train) {
      count++;
      backPropStep(revert,lr,count);
      logLayer.trainTillConvergence(count,epochs);
      Double entropy=this.negativeLogLikelihood();
      if (lastEntropy == null || entropy < lastEntropy)       lastEntropy=entropy;
 else       if (entropy > lastEntropy) {
        update(revert);
        numOver++;
        if (numOver >= tolerance)         train=false;
      }
 else       if (entropy == lastEntropy)       train=false;
    }
  }
}
