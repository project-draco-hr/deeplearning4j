{
  Double lastEntropy=this.negativeLogLikelihood();
  BaseMultiLayerNetwork revert=clone();
  if (forceNumEpochs) {
    for (int i=0; i < epochs; i++) {
      backPropStep(revert,lr,i);
      lastEntropy=negativeLogLikelihood();
    }
  }
 else {
    boolean train=true;
    int count=0;
    int numOver=0;
    int tolerance=3;
    while (train) {
      count++;
      backPropStep(revert,lr,count);
      DoubleMatrix newSample=getSigmoidLayers()[nLayers - 1].sampleHiddenGivenVisible();
      logLayer.trainTillConvergence(newSample,logLayer.getLabels(),lr,epochs);
      Double entropy=this.negativeLogLikelihood();
      if (lastEntropy == null || entropy < lastEntropy)       lastEntropy=entropy;
 else       if (entropy > lastEntropy) {
        update(revert);
        numOver++;
        if (numOver >= tolerance)         train=false;
      }
 else       if (entropy == lastEntropy)       train=false;
    }
  }
}
