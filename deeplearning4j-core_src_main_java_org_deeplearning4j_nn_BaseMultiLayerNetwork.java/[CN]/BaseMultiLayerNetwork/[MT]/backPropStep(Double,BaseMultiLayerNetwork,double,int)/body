{
  double error=this.negativeLogLikelihood();
  log.info("Current back prop error " + error);
  if (lastEntropy == null) {
    lastEntropy=error;
  }
 else   if (error == lastEntropy) {
    log.info("Converged; no more stepping appears to do anything");
    return false;
  }
 else   if (error > lastEntropy || Double.isNaN(error) || Double.isInfinite(error)) {
    log.info("Error greater than previous; found global minima; converging");
    update(revert);
    return false;
  }
 else   if (error < lastEntropy) {
    lastEntropy=error;
    revert=clone();
    log.info("Found better error on epoch " + epoch + " "+ lastEntropy);
  }
  List<Pair<DoubleMatrix,DoubleMatrix>> deltas=new ArrayList<>();
  computeDeltas(deltas);
  for (int l=0; l < nLayers; l++) {
    DoubleMatrix add=deltas.get(l).getFirst().div(input.rows);
    if (isUseAdaGrad())     add.muli(layers[l].getAdaGrad().getLearningRates(add));
 else     add.muli(lr);
    add.divi(input.rows);
    if (useRegularization) {
      add.muli(layers[l].getW().mul(l2));
    }
    layers[l].getW().subi(add);
    sigmoidLayers[l].setW(layers[l].getW());
    DoubleMatrix deltaColumnSums=deltas.get(l + 1).getSecond().columnSums();
    deltaColumnSums.divi(input.rows);
    getLayers()[l].gethBias().subi(deltaColumnSums.mul(lr));
    getSigmoidLayers()[l].setB(getLayers()[l].gethBias());
  }
  getLogLayer().getW().subi(deltas.get(nLayers).getFirst());
  return true;
}
