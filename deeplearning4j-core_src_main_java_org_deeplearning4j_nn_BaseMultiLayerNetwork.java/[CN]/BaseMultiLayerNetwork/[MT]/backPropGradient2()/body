{
  List<Pair<INDArray,INDArray>> deltas=computeDeltas2();
  List<Pair<Pair<INDArray,INDArray>,Pair<INDArray,INDArray>>> list=new ArrayList<>();
  List<Pair<INDArray,INDArray>> grad=new ArrayList<>();
  List<Pair<INDArray,INDArray>> preCon=new ArrayList<>();
  for (int l=0; l < deltas.size(); l++) {
    INDArray gradientChange=deltas.get(l).getFirst();
    INDArray preConGradientChange=deltas.get(l).getSecond();
    if (l < layers.length && gradientChange.length() != layers[l].getW().length())     throw new IllegalStateException("Gradient change not equal to weight change");
 else     if (l == getNeuralNets().length && gradientChange.length() != getOutputLayer().getW().length())     throw new IllegalStateException("Gradient change not equal to weight change");
    INDArray deltaColumnSums=deltas.get(l).getFirst().mean(0);
    INDArray preConColumnSums=deltas.get(l).getSecond().mean(0);
    grad.add(new Pair<>(gradientChange,deltaColumnSums));
    preCon.add(new Pair<>(preConGradientChange,preConColumnSums));
    if (l < layers.length && deltaColumnSums.length() != layers[l].getB().length())     throw new IllegalStateException("Bias change not equal to weight change");
 else     if (l == getLayers().length && deltaColumnSums.length() != getOutputLayer().getB().length())     throw new IllegalStateException("Bias change not equal to weight change");
  }
  INDArray g=pack(grad);
  INDArray con=pack(preCon);
  INDArray theta=params();
  if (mask == null)   initMask();
  g.addi(theta.mul(defaultConfiguration.getL2()).muli(mask));
  INDArray conAdd=Transforms.pow(mask.mul(defaultConfiguration.getL2()).add(Nd4j.valueArrayOf(g.rows(),g.columns(),layerWiseConfigurations.getDampingFactor())),3.0 / 4.0);
  con.addi(conAdd);
  List<Pair<INDArray,INDArray>> gUnpacked=unPack(g);
  List<Pair<INDArray,INDArray>> conUnpacked=unPack(con);
  for (int i=0; i < gUnpacked.size(); i++)   list.add(new Pair<>(gUnpacked.get(i),conUnpacked.get(i)));
  return list;
}
