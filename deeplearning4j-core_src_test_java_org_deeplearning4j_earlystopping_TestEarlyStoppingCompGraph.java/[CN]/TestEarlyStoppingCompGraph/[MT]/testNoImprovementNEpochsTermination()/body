{
  Nd4j.getRandom().setSeed(12345);
  ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).iterations(1).updater(Updater.SGD).learningRate(0.0).weightInit(WeightInit.XAVIER).graphBuilder().addInputs("in").addLayer("0",new OutputLayer.Builder().nIn(4).nOut(3).lossFunction(LossFunctions.LossFunction.MCXENT).build(),"in").setOutputs("0").pretrain(false).backprop(true).build();
  ComputationGraph net=new ComputationGraph(conf);
  net.setListeners(new ScoreIterationListener(1));
  DataSetIterator irisIter=new IrisDataSetIterator(150,150);
  EarlyStoppingModelSaver<ComputationGraph> saver=new InMemoryModelSaver<>();
  EarlyStoppingConfiguration<ComputationGraph> esConf=new EarlyStoppingConfiguration.Builder<ComputationGraph>().epochTerminationConditions(new MaxEpochsTerminationCondition(100),new ScoreImprovementEpochTerminationCondition(5)).iterationTerminationConditions(new MaxTimeIterationTerminationCondition(3,TimeUnit.SECONDS),new MaxScoreIterationTerminationCondition(7.5)).scoreCalculator(new DataSetLossCalculatorCG(irisIter,true)).modelSaver(saver).build();
  IEarlyStoppingTrainer trainer=new EarlyStoppingGraphTrainer(esConf,net,irisIter);
  EarlyStoppingResult result=trainer.fit();
  assertEquals(6,result.getTotalEpochs());
  assertEquals(0,result.getBestModelEpoch());
  assertEquals(EarlyStoppingResult.TerminationReason.EpochTerminationCondition,result.getTerminationReason());
  String expDetails=new ScoreImprovementEpochTerminationCondition(5).toString();
  assertEquals(expDetails,result.getTerminationDetails());
}
