{
  double lr=0.01;
  double biasLr=0.02;
  int[] nIns={4,3,3};
  int[] nOuts={3,3,3};
  int oldScore=1;
  int newScore=1;
  int iteration=3;
  INDArray gradientW=Nd4j.ones(nIns[0],nOuts[0]);
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().list().layer(0,new DenseLayer.Builder().nIn(nIns[0]).nOut(nOuts[0]).updater(org.deeplearning4j.nn.conf.Updater.SGD).learningRate(lr).biasLearningRate(biasLr).build()).layer(1,new BatchNormalization.Builder().nIn(nIns[1]).nOut(nOuts[1]).learningRate(0.7).build()).layer(2,new OutputLayer.Builder().nIn(nIns[2]).nOut(nOuts[2]).updater(org.deeplearning4j.nn.conf.Updater.SGD).build()).backprop(true).pretrain(false).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  net.init();
  ConvexOptimizer opt=new StochasticGradientDescent(net.getDefaultConfiguration(),new NegativeDefaultStepFunction(),null,net);
  opt.checkTerminalConditions(gradientW,oldScore,newScore,iteration);
  assertEquals(lr,net.getLayer(0).conf().getLearningRateByParam("W"),1e-4);
  assertEquals(biasLr,net.getLayer(0).conf().getLearningRateByParam("b"),1e-4);
  assertEquals(0.7,net.getLayer(1).conf().getLearningRateByParam("gamma"),1e-4);
  assertEquals(0.1,net.getLayer(1).conf().getLearningRateByParam("b"),1e-4);
}
