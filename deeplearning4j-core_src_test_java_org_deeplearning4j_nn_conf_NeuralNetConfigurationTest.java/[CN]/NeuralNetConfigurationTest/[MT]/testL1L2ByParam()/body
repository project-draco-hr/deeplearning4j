{
  double l1=0.01;
  double l2=0.07;
  int[] nIns={4,3,3};
  int[] nOuts={3,3,3};
  int oldScore=1;
  int newScore=1;
  int iteration=3;
  INDArray gradientW=Nd4j.ones(nIns[0],nOuts[0]);
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().learningRate(8).regularization(true).l1(l1).l2(l2).list().layer(0,new DenseLayer.Builder().nIn(nIns[0]).nOut(nOuts[0]).updater(org.deeplearning4j.nn.conf.Updater.SGD).build()).layer(1,new BatchNormalization.Builder().nIn(nIns[1]).nOut(nOuts[1]).l2(0.5).build()).layer(2,new OutputLayer.Builder().nIn(nIns[2]).nOut(nOuts[2]).updater(org.deeplearning4j.nn.conf.Updater.SGD).build()).backprop(true).pretrain(false).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  net.init();
  ConvexOptimizer opt=new StochasticGradientDescent(net.getDefaultConfiguration(),new NegativeDefaultStepFunction(),null,net);
  opt.checkTerminalConditions(gradientW,oldScore,newScore,iteration);
  assertEquals(l1,net.getLayer(0).conf().getL1ByParam("W"),1e-4);
  assertEquals(0.0,net.getLayer(0).conf().getL1ByParam("b"),1e-4);
  assertEquals(0.5,net.getLayer(1).conf().getL2ByParam("gamma"),1e-4);
  assertEquals(l2,net.getLayer(2).conf().getL2ByParam("W"),1e-4);
  assertEquals(0.0,net.getLayer(2).conf().getL2ByParam("b"),1e-4);
}
