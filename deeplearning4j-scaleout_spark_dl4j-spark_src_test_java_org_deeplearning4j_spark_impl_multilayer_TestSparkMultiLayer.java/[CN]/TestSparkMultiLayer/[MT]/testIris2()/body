{
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().momentum(0.9).seed(123).lossFunction(LossFunctions.LossFunction.RMSE_XENT).optimizationAlgo(OptimizationAlgorithm.LINE_GRADIENT_DESCENT).iterations(100).visibleUnit(org.deeplearning4j.nn.conf.layers.RBM.VisibleUnit.GAUSSIAN).hiddenUnit(org.deeplearning4j.nn.conf.layers.RBM.HiddenUnit.RECTIFIED).weightInit(WeightInit.XAVIER).maxNumLineSearchIterations(10).constrainGradientToUnitNorm(true).nIn(4).nOut(3).layer(new org.deeplearning4j.nn.conf.layers.RBM.Builder().activation("relu").build()).list(2).layer(1,new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation("softmax").weightInit(WeightInit.XAVIER).build()).hiddenLayerSizes(3).backprop(false).build();
  MultiLayerNetwork network=new MultiLayerNetwork(conf);
  network.init();
  System.out.println("Initializing network");
  SparkDl4jMultiLayer master=new SparkDl4jMultiLayer(sc,conf);
  DataSet d=new IrisDataSetIterator(150,150).next();
  d.normalizeZeroMeanZeroUnitVariance();
  d.shuffle();
  List<DataSet> next=d.asList();
  JavaRDD<DataSet> data=sc.parallelize(next);
  MultiLayerNetwork network2=master.fitDataSet(data);
  INDArray params=network2.params();
  File writeTo=new File(UUID.randomUUID().toString());
  Nd4j.writeTxt(params,writeTo.getAbsolutePath(),",");
  INDArray load=Nd4j.read(new FileInputStream(writeTo.getAbsolutePath()));
  assertEquals(params,load);
  writeTo.delete();
  Evaluation evaluation=new Evaluation();
  evaluation.eval(d.getLabels(),network2.output(d.getFeatureMatrix()));
  System.out.println(evaluation.stats());
}
