{
  int nIn=4;
  int nOut=3;
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().list(2).layer(0,new org.deeplearning4j.nn.conf.layers.DenseLayer.Builder().nIn(nIn).nOut(3).activation("tanh").build()).layer(1,new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).nIn(3).nOut(nOut).activation("softmax").build()).build();
  int nRows=200;
  Random r=new Random(12345);
  INDArray labels=Nd4j.create(nRows,nOut);
  INDArray input=Nd4j.rand(nRows,nIn);
  INDArray rowSums=input.sum(1);
  input.diviColumnVector(rowSums);
  for (int i=0; i < nRows; i++) {
    int x1=r.nextInt(nOut);
    labels.putScalar(new int[]{i,x1},1.0);
  }
  SparkDl4jMultiLayer sparkNet=new SparkDl4jMultiLayer(sc,conf);
  MultiLayerNetwork netCopy=sparkNet.getNetwork().clone();
  Evaluation evalExpected=new Evaluation();
  INDArray outLocal=netCopy.output(input,Layer.TrainingMode.TEST);
  evalExpected.eval(labels,outLocal);
  List<DataSet> list=new ArrayList<>();
  for (int i=0; i < nRows; i++) {
    INDArray inRow=input.getRow(i).dup();
    INDArray outRow=labels.getRow(i).dup();
    DataSet ds=new DataSet(inRow,outRow);
    list.add(ds);
  }
  JavaRDD<DataSet> ds=sc.parallelize(list);
  Evaluation evalActual=sparkNet.evaluate(ds);
  assertEquals(evalExpected.accuracy(),evalActual.accuracy(),1e-3);
  assertEquals(evalExpected.f1(),evalActual.f1(),1e-3);
  assertEquals(evalExpected.getNumRowCounter(),evalActual.getNumRowCounter(),1e-3);
  assertEquals(evalExpected.falseNegatives(),evalActual.falseNegatives(),1e-3);
  assertEquals(evalExpected.falsePositives(),evalActual.falsePositives(),1e-3);
  assertEquals(evalExpected.trueNegatives(),evalActual.trueNegatives(),1e-3);
  assertEquals(evalExpected.truePositives(),evalActual.truePositives(),1e-3);
  assertEquals(evalExpected.precision(),evalActual.precision(),1e-3);
  assertEquals(evalExpected.recall(),evalActual.recall(),1e-3);
  assertEquals(evalExpected.getConfusionMatrix(),evalActual.getConfusionMatrix());
}
