{
  Nd4j.ENFORCE_NUMERICAL_STABILITY=true;
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().nIn(4).nOut(3).layer(new org.deeplearning4j.nn.conf.layers.RBM.Builder().activation("tanh").build()).list(2).layer(1,new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation("softmax").build()).hiddenLayerSizes(3).build();
  DataSet dataSet=new IrisDataSetIterator(150,150).next();
  List<DataSet> list=dataSet.asList();
  JavaRDD<DataSet> data=sc.parallelize(list);
  JavaRDD<LabeledPoint> mllLibData=MLLibUtil.fromDataSet(sc,data);
  MultiLayerNetwork network=SparkDl4jMultiLayer.train(mllLibData,conf);
  INDArray params=network.params();
  File writeTo=new File(UUID.randomUUID().toString());
  Nd4j.writeTxt(params,writeTo.getAbsolutePath(),",");
  INDArray load=Nd4j.readTxt(writeTo.getAbsolutePath());
  assertEquals(params,load);
  writeTo.delete();
  String json=network.getLayerWiseConfigurations().toJson();
  MultiLayerConfiguration conf2=MultiLayerConfiguration.fromJson(json);
  assertEquals(conf,conf2);
  MultiLayerNetwork network3=new MultiLayerNetwork(conf2);
  network3.init();
  network3.setParameters(params);
  INDArray params4=network3.params();
  assertEquals(params,params4);
}
