{
  Nd4j.ENFORCE_NUMERICAL_STABILITY=true;
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().nIn(4).nOut(3).layer(new org.deeplearning4j.nn.conf.layers.RBM()).activationFunction("tanh").list(2).hiddenLayerSizes(3).override(1,new ConfOverride(){
    @Override public void overrideLayer(    int i,    NeuralNetConfiguration.Builder builder){
      if (i == 1) {
        builder.activationFunction("softmax");
        builder.layer(new org.deeplearning4j.nn.conf.layers.OutputLayer());
        builder.lossFunction(LossFunctions.LossFunction.MCXENT);
      }
    }
  }
).build();
  DataSet dataSet=new IrisDataSetIterator(150,150).next();
  List<DataSet> list=dataSet.asList();
  JavaRDD<DataSet> data=sc.parallelize(list);
  JavaRDD<LabeledPoint> mllLibData=MLLibUtil.fromDataSet(sc,data);
  MultiLayerNetwork network=SparkDl4jMultiLayer.train(mllLibData,conf);
  INDArray params=network.params();
  File writeTo=new File(UUID.randomUUID().toString());
  Nd4j.writeTxt(params,writeTo.getAbsolutePath(),",");
  INDArray load=Nd4j.readTxt(writeTo.getAbsolutePath(),",");
  assertEquals(params,load);
  writeTo.delete();
  String json=network.getLayerWiseConfigurations().toJson();
  MultiLayerConfiguration conf2=MultiLayerConfiguration.fromJson(json);
  assertEquals(conf,conf2);
  MultiLayerNetwork network3=new MultiLayerNetwork(conf2);
  network3.init();
  network3.setParameters(params);
  INDArray params4=network3.params();
  assertEquals(params,params4);
}
