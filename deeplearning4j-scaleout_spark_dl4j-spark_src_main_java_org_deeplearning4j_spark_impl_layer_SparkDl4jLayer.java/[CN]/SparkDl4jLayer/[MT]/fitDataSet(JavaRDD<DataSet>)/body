{
  int iterations=conf.getNumIterations();
  long count=rdd.count();
  log.info("Running distributed training averaging each iteration " + averageEachIteration + " and "+ rdd.partitions().size()+ " partitions");
  if (!averageEachIteration) {
    int numParams=LayerFactories.getFactory(conf).initializer().numParams(conf,true);
    final INDArray params=Nd4j.create(1,numParams);
    Layer layer=LayerFactories.getFactory(conf.getLayer()).create(conf,null,0,params,true);
    layer.setBackpropGradientsViewArray(Nd4j.create(1,numParams));
    this.params=sc.broadcast(params);
    log.info("Broadcasting initial parameters of length " + params.length());
    int paramsLength=layer.numParams();
    if (params.length() != paramsLength)     throw new IllegalStateException("Number of params " + paramsLength + " was not equal to "+ params.length());
    JavaRDD<INDArray> results=rdd.sample(true,0.4).mapPartitions(new IterativeReduceFlatMap(conf.toJson(),this.params));
    log.debug("Ran iterative reduce...averaging results now.");
    INDArray newParams=results.fold(Nd4j.zeros(results.first().shape()),new Add());
    newParams.divi(rdd.partitions().size());
    layer.setParams(newParams);
    this.layer=layer;
  }
 else {
    conf.setNumIterations(1);
    int numParams=LayerFactories.getFactory(conf).initializer().numParams(conf,true);
    final INDArray params=Nd4j.create(1,numParams);
    Layer layer=LayerFactories.getFactory(conf.getLayer()).create(conf,null,0,params,true);
    layer.setBackpropGradientsViewArray(Nd4j.create(1,numParams));
    this.params=sc.broadcast(params);
    for (int i=0; i < iterations; i++) {
      JavaRDD<INDArray> results=rdd.sample(true,0.3).mapPartitions(new IterativeReduceFlatMap(conf.toJson(),this.params));
      int paramsLength=layer.numParams();
      if (params.length() != paramsLength)       throw new IllegalStateException("Number of params " + paramsLength + " was not equal to "+ params.length());
      INDArray newParams=results.fold(Nd4j.zeros(results.first().shape()),new Add());
      newParams.divi(rdd.partitions().size());
    }
    layer.setParams(this.params.value().dup());
    this.layer=layer;
  }
  return layer;
}
