{
  DataSetIterator iter=new MnistDataSetIterator(2,2);
  DataSet next=iter.next();
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).iterations(2).seed(123).weightInit(WeightInit.XAVIER).list().layer(0,new ConvolutionLayer.Builder(new int[]{1,1},new int[]{1,1}).nIn(1).nOut(6).activation("identity").build()).layer(1,new BatchNormalization.Builder().build()).layer(2,new ActivationLayer.Builder().activation("relu").build()).layer(3,new DenseLayer.Builder().nIn(28 * 28 * 6).nOut(10).activation("identity").build()).layer(4,new BatchNormalization.Builder().nOut(10).build()).layer(5,new ActivationLayer.Builder().activation("relu").build()).layer(6,new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation("softmax").nIn(10).nOut(10).build()).backprop(true).pretrain(false).cnnInputSize(28,28,1).build();
  MultiLayerNetwork network=new MultiLayerNetwork(conf);
  network.init();
  network.setInput(next.getFeatureMatrix());
  INDArray activationsActual=network.preOutput(next.getFeatureMatrix());
  assertEquals(10,activationsActual.shape()[1],1e-2);
  network.fit(next);
  INDArray actualGammaParam=network.getLayer(1).getParam(BatchNormalizationParamInitializer.GAMMA);
  INDArray actualBetaParam=network.getLayer(1).getParam(BatchNormalizationParamInitializer.BETA);
  assertTrue(actualGammaParam != null);
  assertTrue(actualBetaParam != null);
}
