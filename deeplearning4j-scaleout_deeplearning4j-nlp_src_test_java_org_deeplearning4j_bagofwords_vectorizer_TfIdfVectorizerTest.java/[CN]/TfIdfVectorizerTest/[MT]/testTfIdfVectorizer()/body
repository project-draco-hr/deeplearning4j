{
  File rootDir=new ClassPathResource("rootdir").getFile();
  LabelAwareSentenceIterator iter=new LabelAwareFileSentenceIterator(rootDir);
  List<String> docStrings=new ArrayList<>();
  while (iter.hasNext())   docStrings.add(iter.nextSentence());
  iter.reset();
  List<String> labels=Arrays.asList("label1","label2");
  TokenizerFactory tokenizerFactory=new UimaTokenizerFactory();
  TextVectorizer vectorizer=new LegacyTfidfVectorizer.Builder().minWords(1).index(index).cache(cache).stopWords(new ArrayList<String>()).tokenize(tokenizerFactory).labels(labels).iterate(iter).build();
  vectorizer.fit();
  try {
    vectorizer.vectorize("",null);
    fail("Vectorizer should receive non-null label.");
  }
 catch (  IllegalArgumentException e) {
    ;
  }
  VocabWord word=(VocabWord)vectorizer.vocab().wordFor("file");
  assumeNotNull(word);
  assertEquals(word,vectorizer.vocab().tokenFor("file"));
  int[] docs=vectorizer.index().allDocs();
  InvertedIndex<VocabWord> localIndex=vectorizer.index();
  for (  int i : docs) {
    StringBuilder sb=new StringBuilder();
    List<VocabWord> doc=localIndex.document(i);
    for (    VocabWord w : doc)     sb.append(" " + w.getWord());
    log.info("Doc " + sb.toString());
  }
  assertEquals(docStrings.size(),docs.length);
  assertEquals(docStrings.size(),localIndex.documents(word).length);
}
