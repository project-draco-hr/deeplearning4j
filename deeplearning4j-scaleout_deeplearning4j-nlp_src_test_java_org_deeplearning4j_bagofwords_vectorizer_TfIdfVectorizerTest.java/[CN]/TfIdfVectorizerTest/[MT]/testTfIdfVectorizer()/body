{
  File rootDir=new ClassPathResource("rootdir").getFile();
  LabelAwareSentenceIterator iter=new LabelAwareFileSentenceIterator(rootDir);
  List<String> docStrings=new ArrayList<>();
  while (iter.hasNext())   docStrings.add(iter.nextSentence());
  iter.reset();
  List<String> labels=Arrays.asList("label1","label2");
  TokenizerFactory tokenizerFactory=new UimaTokenizerFactory();
  VocabCache cache=new InMemoryLookupCache.Builder().vectorLength(100).build();
  InvertedIndex index=new LuceneInvertedIndex.Builder().cache(cache).batchSize(5).cacheInRam(false).build();
  TextVectorizer vectorizer=new TfidfVectorizer.Builder().minWords(1).index(index).cache(cache).stopWords(new ArrayList<String>()).tokenize(tokenizerFactory).labels(labels).iterate(iter).build();
  vectorizer.fit();
  VocabWord word=vectorizer.vocab().wordFor("file");
  assumeNotNull(word);
  assertEquals(word,vectorizer.vocab().tokenFor("file"));
  Collection<Integer> docs=vectorizer.index().allDocs();
  for (  int i : docs) {
    StringBuffer sb=new StringBuffer();
    List<VocabWord> doc=vectorizer.index().document(i);
    for (    VocabWord w : doc)     sb.append(" " + w.getWord());
    log.info("Doc " + sb.toString());
  }
  assertEquals(docStrings.size(),docs.size());
  assertEquals(docStrings.size(),vectorizer.index().documents(word).size());
  Iterator<List<VocabWord>> miniBatches=vectorizer.index().miniBatches();
  int count=0;
  while (miniBatches.hasNext()) {
    miniBatches.next();
    count++;
  }
  assertEquals(2,count);
  log.info("Count " + count);
}
