{
  int dataSetObjSize=5;
  int batchSizePerExecutor=10;
  String path=FilenameUtils.concat(System.getProperty("java.io.tmpdir"),"dl4j_testpreprocdata3");
  File f=new File(path);
  if (f.exists())   f.delete();
  f.mkdir();
  DataSetIterator iter=new IrisDataSetIterator(5,150);
  int i=0;
  while (iter.hasNext()) {
    File f2=new File(FilenameUtils.concat(path,"data" + (i++) + ".bin"));
    DataSet ds=iter.next();
    MultiDataSet mds=new MultiDataSet(ds.getFeatures(),ds.getLabels());
    mds.save(f2);
  }
  ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().updater(Updater.RMSPROP).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).iterations(1).graphBuilder().addInputs("in").addLayer("0",new org.deeplearning4j.nn.conf.layers.DenseLayer.Builder().nIn(4).nOut(3).activation("tanh").build(),"in").addLayer("1",new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).nIn(3).nOut(3).activation("softmax").build(),"0").setOutputs("1").pretrain(false).backprop(true).build();
  SparkComputationGraph sparkNet=new SparkComputationGraph(sc,conf,new ParameterAveragingTrainingMaster.Builder(numExecutors(),dataSetObjSize).batchSizePerWorker(batchSizePerExecutor).averagingFrequency(1).repartionData(Repartition.Always).build());
  sparkNet.setCollectTrainingStats(true);
  JavaPairRDD<String,PortableDataStream> pds=sc.binaryFiles(path);
  assertEquals(150 / dataSetObjSize,pds.count());
  sparkNet.fitMultiDataSet(path);
  SparkTrainingStats sts=sparkNet.getSparkTrainingStats();
  int expNumFits=12;
  assertTrue(Math.abs(expNumFits - sts.getValue("ParameterAveragingWorkerFitTimesMs").size()) < 3);
  assertEquals(3,sts.getValue("ParameterAveragingMasterMapPartitionsTimesMs").size());
}
