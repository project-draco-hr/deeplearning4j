{
  final MultiDimensionalMap<String,String,INDArray> binaryTD=MultiDimensionalMap.newTreeBackedMap();
  final MultiDimensionalMap<String,String,INDArray> binaryINDArrayTD=MultiDimensionalMap.newTreeBackedMap();
  final MultiDimensionalMap<String,String,INDArray> binaryCD=MultiDimensionalMap.newTreeBackedMap();
  final Map<String,INDArray> unaryCD=new TreeMap<>();
  final Map<String,INDArray> wordVectorD=new TreeMap<>();
  for (  MultiDimensionalMap.Entry<String,String,INDArray> entry : binaryTransform.entrySet()) {
    int numRows=entry.getValue().rows();
    int numCols=entry.getValue().columns();
    binaryTD.put(entry.getFirstKey(),entry.getSecondKey(),Nd4j.create(numRows,numCols));
  }
  if (!combineClassification) {
    for (    MultiDimensionalMap.Entry<String,String,INDArray> entry : binaryClassification.entrySet()) {
      int numRows=entry.getValue().rows();
      int numCols=entry.getValue().columns();
      binaryCD.put(entry.getFirstKey(),entry.getSecondKey(),Nd4j.create(numRows,numCols));
    }
  }
  if (useDoubleTensors) {
    for (    MultiDimensionalMap.Entry<String,String,INDArray> entry : binaryTensors.entrySet()) {
      int numRows=entry.getValue().size(1);
      int numCols=entry.getValue().size(2);
      int numSlices=entry.getValue().slices();
      binaryINDArrayTD.put(entry.getFirstKey(),entry.getSecondKey(),Nd4j.create(new int[]{numRows,numCols,numSlices}));
    }
  }
  for (  Map.Entry<String,INDArray> entry : unaryClassification.entrySet()) {
    int numRows=entry.getValue().rows();
    int numCols=entry.getValue().columns();
    unaryCD.put(entry.getKey(),Nd4j.create(numRows,numCols));
  }
  for (  String s : vocabCache.words()) {
    INDArray vector=featureVectors.vector(s);
    int numRows=vector.rows();
    int numCols=vector.columns();
    wordVectorD.put(s,Nd4j.create(numRows,numCols));
  }
  final List<Tree> forwardPropTrees=new CopyOnWriteArrayList<>();
  if (!forwardPropTrees.isEmpty())   Parallelization.iterateInParallel(trainingTrees,new Parallelization.RunnableWithParams<Tree>(){
    public void run(    Tree currentItem,    Object[] args){
      Tree trainingTree=new Tree(currentItem);
      trainingTree.connect(new ArrayList<>(currentItem.children()));
      forwardPropagateTree(trainingTree);
      forwardPropTrees.add(trainingTree);
    }
  }
,rnTnActorSystem);
  final AtomicDouble error=new AtomicDouble(0);
  if (!forwardPropTrees.isEmpty())   Parallelization.iterateInParallel(forwardPropTrees,new Parallelization.RunnableWithParams<Tree>(){
    public void run(    Tree currentItem,    Object[] args){
      backpropDerivativesAndError(currentItem,binaryTD,binaryCD,binaryINDArrayTD,unaryCD,wordVectorD);
      error.addAndGet(currentItem.errorSum());
    }
  }
,new Parallelization.RunnableWithParams<Tree>(){
    public void run(    Tree currentItem,    Object[] args){
    }
  }
,rnTnActorSystem,new Object[]{binaryTD,binaryCD,binaryINDArrayTD,unaryCD,wordVectorD});
  double scale=trainingTrees == null || trainingTrees.isEmpty() ? 1.0f : (1.0f / trainingTrees.size());
  value=error.doubleValue() * scale;
  value+=scaleAndRegularize(binaryTD,binaryTransform,scale,regTransformMatrix);
  value+=scaleAndRegularize(binaryCD,binaryClassification,scale,regClassification);
  value+=scaleAndRegularizeINDArray(binaryINDArrayTD,binaryTensors,scale,regTransformINDArray);
  value+=scaleAndRegularize(unaryCD,unaryClassification,scale,regClassification);
  value+=scaleAndRegularize(wordVectorD,featureVectors,scale,regWordVector);
  INDArray derivative=Nd4j.toFlattened(getNumParameters(),binaryTD.values().iterator(),binaryCD.values().iterator(),binaryINDArrayTD.values().iterator(),unaryCD.values().iterator(),wordVectorD.values().iterator());
  if (derivative.length() != numParameters)   throw new IllegalStateException("Gradient has wrong number of parameters " + derivative.length() + " should have been "+ numParameters);
  if (paramAdaGrad == null)   paramAdaGrad=new AdaGrad(1,derivative.columns());
  derivative=paramAdaGrad.getGradient(derivative);
  return derivative;
}
