{
  final MultiDimensionalMap<String,String,INDArray> binaryTD=MultiDimensionalMap.newTreeBackedMap();
  final MultiDimensionalMap<String,String,INDArray> binaryINDArrayTD=MultiDimensionalMap.newTreeBackedMap();
  final MultiDimensionalMap<String,String,INDArray> binaryCD=MultiDimensionalMap.newTreeBackedMap();
  final Map<String,INDArray> unaryCD=new TreeMap<>();
  final Map<String,INDArray> wordVectorD=new TreeMap<>();
  for (  MultiDimensionalMap.Entry<String,String,INDArray> entry : binaryTransform.entrySet()) {
    int numRows=entry.getValue().rows();
    int numCols=entry.getValue().columns();
    binaryTD.put(entry.getFirstKey(),entry.getSecondKey(),Nd4j.create(numRows,numCols));
  }
  if (!combineClassification) {
    for (    MultiDimensionalMap.Entry<String,String,INDArray> entry : binaryClassification.entrySet()) {
      int numRows=entry.getValue().rows();
      int numCols=entry.getValue().columns();
      binaryCD.put(entry.getFirstKey(),entry.getSecondKey(),Nd4j.create(numRows,numCols));
    }
  }
  if (useFloatTensors) {
    for (    MultiDimensionalMap.Entry<String,String,INDArray> entry : binaryINd4j.entrySet()) {
      int numRows=entry.getValue().rows();
      int numCols=entry.getValue().columns();
      int numSlices=entry.getValue().slices();
      binaryINDArrayTD.put(entry.getFirstKey(),entry.getSecondKey(),Nd4j.create(new int[]{numRows,numCols,numSlices}));
    }
  }
  for (  Map.Entry<String,INDArray> entry : unaryClassification.entrySet()) {
    int numRows=entry.getValue().rows();
    int numCols=entry.getValue().columns();
    unaryCD.put(entry.getKey(),Nd4j.create(numRows,numCols));
  }
  for (  Map.Entry<String,INDArray> entry : featureVectors.entrySet()) {
    int numRows=entry.getValue().rows();
    int numCols=entry.getValue().columns();
    wordVectorD.put(entry.getKey(),Nd4j.create(numRows,numCols));
  }
  final List<Tree> forwardPropTrees=new CopyOnWriteArrayList<>();
  Parallelization.iterateInParallel(trainingTrees,new Parallelization.RunnableWithParams<Tree>(){
    public void run(    Tree currentItem,    Object[] args){
      Tree trainingTree=new Tree(currentItem);
      trainingTree.connect(new ArrayList<>(currentItem.children()));
      forwardPropagateTree(trainingTree);
      forwardPropTrees.add(trainingTree);
    }
  }
,rnTnActorSystem);
  final AtomicDouble error=new AtomicDouble(0);
  Parallelization.iterateInParallel(forwardPropTrees,new Parallelization.RunnableWithParams<Tree>(){
    public void run(    Tree currentItem,    Object[] args){
      backpropDerivativesAndError(currentItem,binaryTD,binaryCD,binaryINDArrayTD,unaryCD,wordVectorD);
      error.addAndGet(currentItem.errorSum());
    }
  }
,new Parallelization.RunnableWithParams<Tree>(){
    public void run(    Tree currentItem,    Object[] args){
    }
  }
,rnTnActorSystem,new Object[]{binaryTD,binaryCD,binaryINDArrayTD,unaryCD,wordVectorD});
  float scale=(1.0f / trainingTrees.size());
  value=error.floatValue() * scale;
  value+=scaleAndRegularize(binaryTD,binaryTransform,scale,regTransformMatrix);
  value+=scaleAndRegularize(binaryCD,binaryClassification,scale,regClassification);
  value+=scaleAndRegularizeINDArray(binaryINDArrayTD,binaryINd4j,scale,regTransformINDArray);
  value+=scaleAndRegularize(unaryCD,unaryClassification,scale,regClassification);
  value+=scaleAndRegularize(wordVectorD,featureVectors,scale,regWordVector);
  INDArray derivative=Nd4j.toFlattened(getNumParameters(),binaryTD.values().iterator(),binaryCD.values().iterator(),binaryINDArrayTD.values().iterator(),unaryCD.values().iterator(),wordVectorD.values().iterator());
  if (paramAdaGrad == null)   paramAdaGrad=new AdaGrad(1,derivative.columns());
  derivative.muli(paramAdaGrad.getLearningRates(derivative));
  return derivative;
}
