{
  if (isUseGaussNewtonVectorProductBackProp()) {
    log.warn("WARNING; Gauss newton back vector back propagation is primarily used for hessian free which does not involve pretrain; just finetune. Use this at your own risk");
  }
  if (this.getInput() == null || this.getNeuralNets() == null || this.getNeuralNets()[0] == null || this.getNeuralNets() == null || this.getNeuralNets()[0] == null) {
    setInput(input);
    initializeLayers(input);
  }
 else   setInput(input);
  INDArray layerInput=null;
  for (int i=0; i < getnLayers(); i++) {
    if (i == 0)     layerInput=getInput();
 else {
      boolean activateOnly=layerWiseConfigurations.get(i).isUseHiddenActivationsForwardProp();
      if (useRBMPropUpAsActivations) {
        RBM r=(RBM)neuralNets[i - 1];
        layerInput=r.propUp(layerInput);
        if (sampleFromHiddenActivations)         layerInput=Sampling.binomial(layerInput,1,layers[i].conf().getRng());
      }
 else       if (activateOnly)       layerInput=getLayers()[i - 1].activate(layerInput);
 else       if (isSampleFromHiddenActivations())       layerInput=getNeuralNets()[i - 1].sampleHiddenGivenVisible(getNeuralNets()[i - 1].conf().getActivationFunction().apply(layerInput)).getSecond();
 else       layerInput=getNeuralNets()[i - 1].sampleHiddenGivenVisible(layerInput).getSecond();
    }
    log.info("Training on layer " + (i + 1));
    float realLearningRate=layers[i].conf().getLr();
    if (isForceNumEpochs()) {
      for (int epoch=0; epoch < epochs; epoch++) {
        log.info("Error on epoch " + epoch + " for layer "+ (i + 1)+ " is "+ getNeuralNets()[i].score());
        getNeuralNets()[i].iterate(layerInput,new Object[]{k,learningRate});
        getNeuralNets()[i].iterationDone(epoch);
      }
    }
 else     getNeuralNets()[i].fit(layerInput,new Object[]{k,realLearningRate,epochs});
  }
}
