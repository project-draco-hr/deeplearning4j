{
  int[] miniBatchSizes={1,2,5};
  int nIn=2;
  Random r=new Random(12345);
  for (  int nExamples : miniBatchSizes) {
    Nd4j.getRandom().setSeed(12345);
    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).iterations(1).updater(Updater.SGD).learningRate(0.1).seed(12345).list(4).layer(0,new DenseLayer.Builder().activation("tanh").nIn(2).nOut(2).build()).layer(1,new DenseLayer.Builder().activation("tanh").nIn(2).nOut(2).build()).layer(2,new GravesLSTM.Builder().activation("tanh").nIn(2).nOut(2).build()).layer(3,new RnnOutputLayer.Builder().lossFunction(LossFunctions.LossFunction.MSE).nIn(2).nOut(1).build()).inputPreProcessor(0,new RnnToFeedForwardPreProcessor()).inputPreProcessor(2,new FeedForwardToRnnPreProcessor()).build();
    MultiLayerNetwork net=new MultiLayerNetwork(conf);
    net.init();
    INDArray in1=Nd4j.rand(new int[]{nExamples,2,4});
    INDArray in2=Nd4j.rand(new int[]{nExamples,2,5});
    in2.put(new INDArrayIndex[]{NDArrayIndex.all(),NDArrayIndex.all(),NDArrayIndex.interval(0,3,true)},in1);
    assertEquals(in1,in2.get(NDArrayIndex.all(),NDArrayIndex.all(),NDArrayIndex.interval(0,4)));
    INDArray labels1=Nd4j.rand(new int[]{nExamples,1,4});
    INDArray labels2=Nd4j.create(nExamples,1,5);
    labels2.put(new INDArrayIndex[]{NDArrayIndex.all(),NDArrayIndex.all(),NDArrayIndex.interval(0,3,true)},labels1);
    assertEquals(labels1,labels2.get(NDArrayIndex.all(),NDArrayIndex.all(),NDArrayIndex.interval(0,4)));
    INDArray inputMask=Nd4j.ones(nExamples,5);
    for (int j=0; j < nExamples; j++) {
      inputMask.putScalar(new int[]{j,4},0);
    }
    net.setInput(in1);
    net.setLabels(labels1);
    net.computeGradientAndScore();
    double score1=net.score();
    Gradient g1=net.gradient();
    net.setInput(in2);
    net.setLabels(labels2);
    net.setLayerMaskArrays(inputMask,null);
    net.computeGradientAndScore();
    double score2=net.score();
    Gradient g2=net.gradient();
    List<INDArray> activations2=net.feedForward();
    assertNotEquals(score1,score2,0.01);
    Map<String,INDArray> g1map=g1.gradientForVariable();
    Map<String,INDArray> g2map=g2.gradientForVariable();
    for (    String s : g1map.keySet()) {
      INDArray g1s=g1map.get(s);
      INDArray g2s=g2map.get(s);
      assertNotEquals(s,g1s,g2s);
    }
    for (int j=0; j < nExamples; j++) {
      for (int k=0; k < nIn; k++) {
        in2.putScalar(new int[]{j,k,4},r.nextDouble());
      }
      net.setInput(in2);
      net.computeGradientAndScore();
      double score2a=net.score();
      Gradient g2a=net.gradient();
      assertEquals(score2,score2a,1e-12);
      for (      String s : g2.gradientForVariable().keySet()) {
        assertEquals(g2.getGradientFor(s),g2a.getGradientFor(s));
      }
      List<INDArray> activations2a=net.feedForward();
      for (int k=1; k < activations2.size(); k++) {
        assertEquals(activations2.get(k),activations2a.get(k));
      }
    }
    FeedForwardToRnnPreProcessor temp=new FeedForwardToRnnPreProcessor();
    INDArray l0Before=activations2.get(1);
    INDArray l1Before=activations2.get(2);
    INDArray l0After=temp.preProcess(l0Before,nExamples);
    INDArray l1After=temp.preProcess(l1Before,nExamples);
    for (int j=0; j < nExamples; j++) {
      for (int k=0; k < nIn; k++) {
        assertEquals(0.0,l0After.getDouble(j,k,4),0.0);
        assertEquals(0.0,l1After.getDouble(j,k,4),0.0);
      }
    }
  }
}
