{
  int nIn=3;
  int[] timeSeriesLengths={3,10};
  int[] outputSizes={1,2,5};
  int[] miniBatchSizes={1,4};
  Random r=new Random(12345);
  for (  int tsLength : timeSeriesLengths) {
    for (    int nOut : outputSizes) {
      for (      int miniBatch : miniBatchSizes) {
        for (int nToMask=0; nToMask < tsLength - 1; nToMask++) {
          String msg="tsLen=" + tsLength + ", nOut="+ nOut+ ", miniBatch="+ miniBatch;
          INDArray labelMaskArray=Nd4j.ones(miniBatch,tsLength);
          for (int i=0; i < miniBatch; i++) {
            int nMasked=0;
            while (nMasked < nToMask) {
              int tryIdx=r.nextInt(tsLength);
              if (labelMaskArray.getDouble(i,tryIdx) == 0.0)               continue;
              labelMaskArray.putScalar(new int[]{i,tryIdx},0.0);
              nMasked++;
            }
          }
          INDArray input=Nd4j.rand(new int[]{miniBatch,nIn,tsLength});
          INDArray labels=Nd4j.ones(miniBatch,nOut,tsLength);
          MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().regularization(false).seed(12345L).list().layer(0,new GravesLSTM.Builder().nIn(nIn).nOut(5).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).updater(Updater.NONE).build()).layer(1,new RnnOutputLayer.Builder(LossFunctions.LossFunction.MSE).activation("identity").nIn(5).nOut(nOut).weightInit(WeightInit.ZERO).updater(Updater.NONE).build()).pretrain(false).backprop(true).build();
          MultiLayerNetwork mln=new MultiLayerNetwork(conf);
          mln.init();
          double expScore=(tsLength - nToMask);
          mln.setLayerMaskArrays(null,labelMaskArray);
          mln.setInput(input);
          mln.setLabels(labels);
          mln.computeGradientAndScore();
          double score=mln.score();
          assertEquals(msg,expScore,score,0.1);
        }
      }
    }
  }
}
