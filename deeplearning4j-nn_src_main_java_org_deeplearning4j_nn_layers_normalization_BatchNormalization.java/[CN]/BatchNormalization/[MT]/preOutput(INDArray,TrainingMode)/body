{
  INDArray activations;
  org.deeplearning4j.nn.conf.layers.BatchNormalization layerConf=layerConf();
  int batchSize=x.size(0);
  shape=getShape(x);
  INDArray mean, var;
  if (training == TrainingMode.TRAIN) {
switch (x.rank()) {
case 2:
      mean=x.mean(0);
    var=x.var(false,0);
  break;
case 4:
mean=x.mean(0,2,3);
var=x.var(false,0,2,3);
break;
default :
throw new IllegalStateException("Batch normalization on activations of rank " + x.rank() + " not supported");
}
var.addi(layerConf.getEps());
}
 else {
mean=this.globalMean;
var=this.globalVar;
}
std=Transforms.sqrt(var,true);
INDArray gamma=null;
INDArray beta=null;
if (layerConf.isLockGammaBeta()) {
if (helper != null) {
int[] gammaBetaShape=new int[]{1,layerConf().getNOut()};
gamma=Nd4j.valueArrayOf(gammaBetaShape,layerConf().getGamma());
beta=Nd4j.valueArrayOf(gammaBetaShape,layerConf().getBeta());
}
}
 else {
gamma=getParam(BatchNormalizationParamInitializer.GAMMA);
beta=getParam(BatchNormalizationParamInitializer.BETA);
}
if (helper != null) {
double decay=setMeanVar ? 1 : layerConf.getDecay();
if (setMeanVar) {
this.globalMean=this.globalMean == null ? Nd4j.zeros(mean.shape()) : this.globalMean;
this.globalVar=this.globalVar == null ? Nd4j.ones(var.shape()) : this.globalVar;
setMeanVar=false;
}
INDArray ret=helper.preOutput(x,training == TrainingMode.TRAIN,shape,gamma,beta,this.globalMean,this.globalVar,decay,layerConf.getEps());
if (ret != null) {
return ret;
}
}
if (x.rank() == 2) {
xMu=x.subRowVector(mean);
xHat=xMu.divRowVector(std);
if (layerConf.isLockGammaBeta()) {
double g=layerConf.getGamma();
double b=layerConf.getBeta();
if (g != 1.0 && b != 0.0) {
activations=xHat.mul(g).addi(b);
}
 else {
activations=xHat;
}
}
 else {
activations=xHat.mulRowVector(gamma).addiRowVector(beta);
}
}
 else if (x.rank() == 4) {
if (!Shape.strideDescendingCAscendingF(x)) x=x.dup();
xMu=Nd4j.getExecutioner().execAndReturn(new BroadcastSubOp(x,mean,Nd4j.createUninitialized(x.shape(),x.ordering()),1));
xHat=Nd4j.getExecutioner().execAndReturn(new BroadcastDivOp(xMu,std,Nd4j.createUninitialized(x.shape(),x.ordering()),1));
if (layerConf.isLockGammaBeta()) {
double g=layerConf.getGamma();
double b=layerConf.getBeta();
if (g != 1.0 && b != 0.0) {
activations=xHat.mul(g).addi(b);
}
 else {
activations=xHat;
}
}
 else {
activations=Nd4j.getExecutioner().execAndReturn(new BroadcastMulOp(xHat,gamma,Nd4j.createUninitialized(x.shape(),x.ordering()),1));
activations=Nd4j.getExecutioner().execAndReturn(new BroadcastAddOp(activations,beta,activations,1));
}
}
 else {
throw new IllegalStateException("The layer prior to BatchNorm in the configuration is not currently supported.");
}
double decay;
if (training == TrainingMode.TRAIN) {
if (setMeanVar) {
this.globalMean=this.globalMean == null ? Nd4j.zeros(mean.shape()) : this.globalMean;
this.globalVar=this.globalVar == null ? Nd4j.ones(var.shape()) : this.globalVar;
setMeanVar=false;
}
if (layerConf.isMinibatch()) {
decay=layerConf.getDecay();
this.globalMean.muli(decay).addi(mean.muli(1 - decay));
this.globalVar.muli(decay).addi(var.muli(1 - decay));
}
 else {
this.globalMean=mean;
this.globalVar=var;
}
}
return activations;
}
