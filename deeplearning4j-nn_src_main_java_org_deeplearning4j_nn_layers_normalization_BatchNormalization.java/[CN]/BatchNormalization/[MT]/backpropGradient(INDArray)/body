{
  INDArray nextEpsilon;
  shape=getShape(epsilon);
  int batchSize=epsilon.size(0);
  org.deeplearning4j.nn.conf.layers.BatchNormalization layerConf=layerConf();
  INDArray gamma=null;
  INDArray beta=null;
  if (layerConf.isLockGammaBeta()) {
  }
 else {
    gamma=getParam(BatchNormalizationParamInitializer.GAMMA);
    beta=getParam(BatchNormalizationParamInitializer.BETA);
  }
  Gradient retGradient=new DefaultGradient();
  INDArray dGammaView=gradientViews.get(BatchNormalizationParamInitializer.GAMMA);
  INDArray dBetaView=gradientViews.get(BatchNormalizationParamInitializer.BETA);
  if (helper != null) {
    Pair<Gradient,INDArray> ret=helper.backpropGradient(input,epsilon,shape,gamma,dGammaView,dBetaView,layerConf.getEps());
    if (ret != null) {
      return ret;
    }
  }
  if (epsilon.rank() == 2) {
    INDArray dGamma=epsilon.mul(xHat).sum(0);
    INDArray dBeta=epsilon.sum(0);
    INDArray dxhat=epsilon.mulRowVector(gamma);
    INDArray dLdVar=dxhat.mul(xMu).sum(0).muli(-0.5).divi(Transforms.pow(std,3.0,true));
    INDArray dxmu1=dxhat.sum(0).divi(std);
    INDArray dxmu2=xMu.mul(2.0 / batchSize).muliRowVector(dLdVar);
    INDArray dLdmu=dxmu1.addi(dxmu2.sum(0).muli(dLdVar)).negi();
    INDArray dLdx=dxhat.divRowVector(std).addiRowVector(dLdVar.muli(dxmu2.sum(0)).addi(dLdmu.muli(1.0 / batchSize)));
    dGammaView.assign(dGamma);
    dBetaView.assign(dBeta);
    retGradient.setGradientFor(BatchNormalizationParamInitializer.GAMMA,dGammaView);
    retGradient.setGradientFor(BatchNormalizationParamInitializer.BETA,dBetaView);
    nextEpsilon=dLdx;
  }
 else   if (epsilon.rank() == 4) {
    INDArray dGamma=epsilon.mul(xHat).sum(0,2,3);
    INDArray dBeta=epsilon.sum(0,2,3);
    INDArray dxhat=Nd4j.getExecutioner().execAndReturn(new BroadcastMulOp(epsilon,gamma,epsilon.dup(),1));
    INDArray dsq=dxhat.mul(xMu).sum(0).mul(0.5).div(Transforms.pow(std,3)).neg().div(batchSize);
    INDArray dxmu1=Nd4j.getExecutioner().execAndReturn(new BroadcastDivOp(dxhat,std,dxhat,new int[]{1,2,3}));
    INDArray dxmu2=Nd4j.getExecutioner().execAndReturn(new BroadcastMulOp(xMu.mul(2),dsq,xMu.mul(2),new int[]{1,2,3}));
    INDArray dx1=dxmu1.add(dxmu2);
    INDArray dmu=dx1.sum(0).neg();
    INDArray dx2=dmu.div(batchSize);
    nextEpsilon=Nd4j.getExecutioner().execAndReturn(new BroadcastAddOp(dx1,dx2,dx1.dup(),new int[]{1,2,3}));
    dGammaView.assign(dGamma);
    dBetaView.assign(dBeta);
    retGradient.setGradientFor(BatchNormalizationParamInitializer.GAMMA,dGammaView);
    retGradient.setGradientFor(BatchNormalizationParamInitializer.BETA,dBetaView);
  }
 else {
    throw new IllegalStateException("The layer prior to BatchNorm in the configuration is not currently supported.");
  }
  return new Pair<>(retGradient,nextEpsilon);
}
