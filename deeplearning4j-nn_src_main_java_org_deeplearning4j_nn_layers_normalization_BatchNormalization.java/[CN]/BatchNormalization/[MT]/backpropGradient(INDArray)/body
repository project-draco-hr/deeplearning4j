{
  INDArray nextEpsilon;
  shape=getShape(epsilon);
  int batchSize=epsilon.size(0);
  org.deeplearning4j.nn.conf.layers.BatchNormalization layerConf=layerConf();
  INDArray gamma=null;
  INDArray beta=null;
  if (layerConf.isLockGammaBeta()) {
  }
 else {
    gamma=getParam(BatchNormalizationParamInitializer.GAMMA);
    beta=getParam(BatchNormalizationParamInitializer.BETA);
  }
  Gradient retGradient=new DefaultGradient();
  INDArray dGammaView=gradientViews.get(BatchNormalizationParamInitializer.GAMMA);
  INDArray dBetaView=gradientViews.get(BatchNormalizationParamInitializer.BETA);
  if (helper != null) {
    Pair<Gradient,INDArray> ret=helper.backpropGradient(input,epsilon,shape,gamma,dGammaView,dBetaView,layerConf.getEps());
    if (ret != null) {
      return ret;
    }
  }
  if (epsilon.rank() == 2) {
    INDArray dGamma=epsilon.mul(xHat).sum(0);
    INDArray dBeta=epsilon.sum(0);
    INDArray dxhat=epsilon.mulRowVector(gamma);
    INDArray dLdVar=dxhat.mul(xMu).sum(0).muli(-0.5).muli(Transforms.pow(std,-3.0,true));
    INDArray dxmu1=dxhat.sum(0).divi(std).negi();
    INDArray dxmu2=xMu.sum(0).muli(-2.0 / batchSize).muli(dLdVar);
    INDArray dLdmu=dxmu1.addi(dxmu2);
    INDArray dLdx=dxhat.diviRowVector(std).addi(xMu.muliRowVector(dLdVar.muli(2.0 / batchSize))).addiRowVector(dLdmu.muli(1.0 / batchSize));
    dGammaView.assign(dGamma);
    dBetaView.assign(dBeta);
    retGradient.setGradientFor(BatchNormalizationParamInitializer.GAMMA,dGammaView);
    retGradient.setGradientFor(BatchNormalizationParamInitializer.BETA,dBetaView);
    nextEpsilon=dLdx;
  }
 else   if (epsilon.rank() == 4) {
    if (!Shape.strideDescendingCAscendingF(epsilon))     epsilon=epsilon.dup();
    INDArray dGamma=epsilon.mul(xHat).sum(0,2,3);
    INDArray dBeta=epsilon.sum(0,2,3);
    INDArray dxhat=Nd4j.getExecutioner().execAndReturn(new BroadcastMulOp(epsilon,gamma,Nd4j.createUninitialized(epsilon.shape(),epsilon.ordering()),1));
    INDArray temp=Transforms.pow(std,-3.0,true);
    INDArray dLdVar=dxhat.mul(xMu).sum(0,2,3).muli(-0.5).muli(Transforms.pow(std,-3.0,true));
    int effectiveBatchSize=input.size(0) * input.size(2) * input.size(3);
    INDArray dxmu1=dxhat.sum(0,2,3).divi(std).negi();
    INDArray dxmu2=xMu.sum(0,2,3).muli(-2.0 / effectiveBatchSize).muli(dLdVar);
    INDArray dLdmu=dxmu1.addi(dxmu2);
    INDArray dLdx=Nd4j.getExecutioner().execAndReturn(new BroadcastDivOp(dxhat,std,dxhat,1)).addi(Nd4j.getExecutioner().execAndReturn(new BroadcastMulOp(xMu,dLdVar.muli(2.0 / effectiveBatchSize),xMu,1)));
    Nd4j.getExecutioner().execAndReturn(new BroadcastAddOp(dLdx,dLdmu.muli(1.0 / effectiveBatchSize),dLdx,1));
    dGammaView.assign(dGamma);
    dBetaView.assign(dBeta);
    retGradient.setGradientFor(BatchNormalizationParamInitializer.GAMMA,dGammaView);
    retGradient.setGradientFor(BatchNormalizationParamInitializer.BETA,dBetaView);
    nextEpsilon=dLdx;
  }
 else {
    throw new IllegalStateException("The layer prior to BatchNorm in the configuration is not currently supported.");
  }
  return new Pair<>(retGradient,nextEpsilon);
}
