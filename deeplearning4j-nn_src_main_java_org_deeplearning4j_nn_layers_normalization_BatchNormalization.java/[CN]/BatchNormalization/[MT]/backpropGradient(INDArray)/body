{
  INDArray nextEpsilon;
  shape=getShape(epsilon);
  int batchSize=epsilon.size(0);
  org.deeplearning4j.nn.conf.layers.BatchNormalization layerConf=layerConf();
  INDArray gamma=(layerConf.isLockGammaBeta()) ? Nd4j.ones(shape) : getParam(BatchNormalizationParamInitializer.GAMMA);
  Gradient retGradient=new DefaultGradient();
  INDArray dGammaView=gradientViews.get(BatchNormalizationParamInitializer.GAMMA);
  INDArray dBetaView=gradientViews.get(BatchNormalizationParamInitializer.BETA);
  if (helper != null) {
    Pair<Gradient,INDArray> ret=helper.backpropGradient(input,epsilon,shape,gamma,dGammaView,dBetaView,layerConf.getEps());
    if (ret != null) {
      return ret;
    }
  }
  if (epsilon.rank() == 2) {
    INDArray dGamma=epsilon.mul(xHat).sum(0);
    INDArray dBeta=epsilon.sum(0);
    INDArray dxhat=epsilon.mulRowVector(gamma);
    INDArray dsq=dxhat.mul(xMu).sum(0).mul(0.5).div(Transforms.pow(std,3)).neg().div(batchSize);
    INDArray dxmu1=dxhat.divRowVector(std);
    INDArray dxmu2=xMu.mul(2).mulRowVector(dsq);
    INDArray dx1=dxmu1.add(dxmu2);
    INDArray dmu=dx1.sum(0).neg();
    INDArray dx2=dmu.div(batchSize);
    nextEpsilon=Nd4j.getExecutioner().execAndReturn(new BroadcastAddOp(dx1,dx2,dx1.dup(),-1));
    INDArray r=xMu.divRowVector(Transforms.pow(std,2)).mulRowVector(epsilon.mul(xMu).sum(0));
    INDArray otherEp=epsilon.mul(2).subRowVector(dBeta).mulRowVector(gamma.div(std.mul(2))).sub(r);
    dGammaView.assign(dGamma);
    dBetaView.assign(dBeta);
    retGradient.setGradientFor(BatchNormalizationParamInitializer.GAMMA,dGammaView);
    retGradient.setGradientFor(BatchNormalizationParamInitializer.BETA,dBetaView);
  }
 else   if (epsilon.rank() == 4) {
    INDArray dGamma=epsilon.mul(xHat).sum(0,2,3);
    INDArray dBeta=epsilon.sum(0,2,3);
    INDArray dxhat=Nd4j.getExecutioner().execAndReturn(new BroadcastMulOp(epsilon,gamma,epsilon.dup(),1));
    INDArray dsq=dxhat.mul(xMu).sum(0).mul(0.5).div(Transforms.pow(std,3)).neg().div(batchSize);
    INDArray dxmu1=Nd4j.getExecutioner().execAndReturn(new BroadcastDivOp(dxhat,std,dxhat,new int[]{1,2,3}));
    INDArray dxmu2=Nd4j.getExecutioner().execAndReturn(new BroadcastMulOp(xMu.mul(2),dsq,xMu.mul(2),new int[]{1,2,3}));
    INDArray dx1=dxmu1.add(dxmu2);
    INDArray dmu=dx1.sum(0).neg();
    INDArray dx2=dmu.div(batchSize);
    nextEpsilon=Nd4j.getExecutioner().execAndReturn(new BroadcastAddOp(dx1,dx2,dx1.dup(),new int[]{1,2,3}));
    dGammaView.assign(dGamma);
    dBetaView.assign(dBeta);
    retGradient.setGradientFor(BatchNormalizationParamInitializer.GAMMA,dGammaView);
    retGradient.setGradientFor(BatchNormalizationParamInitializer.BETA,dBetaView);
  }
 else {
    throw new IllegalStateException("The layer prior to BatchNorm in the configuration is not currently supported.");
  }
  return new Pair<>(retGradient,nextEpsilon);
}
