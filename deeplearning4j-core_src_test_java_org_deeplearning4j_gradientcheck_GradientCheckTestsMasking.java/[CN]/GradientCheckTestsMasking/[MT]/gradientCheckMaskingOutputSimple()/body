{
  int timeSeriesLength=5;
  boolean[][] mask=new boolean[5][0];
  mask[0]=new boolean[]{true,true,true,true,true};
  mask[1]=new boolean[]{false,true,true,true,true};
  mask[2]=new boolean[]{false,false,false,false,true};
  mask[3]=new boolean[]{false,false,true,false,true};
  mask[4]=new boolean[]{true,true,true,false,true};
  int nIn=4;
  int layerSize=3;
  int nOut=2;
  Random r=new Random(12345L);
  INDArray input=Nd4j.zeros(1,nIn,timeSeriesLength);
  for (int m=0; m < 1; m++) {
    for (int j=0; j < nIn; j++) {
      for (int k=0; k < timeSeriesLength; k++) {
        input.putScalar(new int[]{m,j,k},r.nextDouble() - 0.5);
      }
    }
  }
  INDArray labels=Nd4j.zeros(1,nOut,timeSeriesLength);
  for (int m=0; m < 1; m++) {
    for (int j=0; j < timeSeriesLength; j++) {
      int idx=r.nextInt(nOut);
      labels.putScalar(new int[]{m,idx,j},1.0f);
    }
  }
  for (int i=0; i < mask.length; i++) {
    INDArray maskArr=Nd4j.create(1,timeSeriesLength);
    for (int j=0; j < mask[i].length; j++) {
      maskArr.putScalar(new int[]{0,j},mask[i][j] ? 1.0 : 0.0);
    }
    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().regularization(false).seed(12345L).list().layer(0,new GravesLSTM.Builder().nIn(nIn).nOut(layerSize).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).updater(Updater.NONE).build()).layer(1,new RnnOutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation("softmax").nIn(layerSize).nOut(nOut).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).updater(Updater.NONE).build()).pretrain(false).backprop(true).build();
    MultiLayerNetwork mln=new MultiLayerNetwork(conf);
    mln.init();
    mln.setLayerMaskArrays(null,maskArr);
    boolean gradOK=GradientCheckUtil.checkGradients(mln,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels,true);
    String msg="gradientCheckMaskingOutputSimple() - timeSeriesLength=" + timeSeriesLength + ", miniBatchSize="+ 1;
    assertTrue(msg,gradOK);
  }
}
