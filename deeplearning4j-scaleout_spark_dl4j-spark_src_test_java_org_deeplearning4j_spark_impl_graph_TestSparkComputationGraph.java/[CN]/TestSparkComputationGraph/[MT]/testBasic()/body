{
  JavaSparkContext sc=this.sc;
  RecordReader rr=new CSVRecordReader(0,",");
  rr.initialize(new FileSplit(new ClassPathResource("iris.txt").getFile()));
  MultiDataSetIterator iter=new RecordReaderMultiDataSetIterator.Builder(1).addReader("iris",rr).addInput("iris",0,3).addOutputOneHot("iris",4,3).build();
  List<MultiDataSet> list=new ArrayList<>(150);
  while (iter.hasNext())   list.add(iter.next());
  ComputationGraphConfiguration config=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).learningRate(0.1).graphBuilder().addInputs("in").addLayer("dense",new DenseLayer.Builder().nIn(4).nOut(2).build(),"in").addLayer("out",new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).nIn(2).nOut(3).build(),"dense").setOutputs("out").pretrain(false).backprop(true).build();
  ComputationGraph cg=new ComputationGraph(config);
  cg.init();
  TrainingMaster tm=new ParameterAveragingTrainingMaster(true,numExecutors(),1,10,1,0);
  SparkComputationGraph scg=new SparkComputationGraph(sc,cg,tm);
  scg.setListeners(Collections.singleton((IterationListener)new ScoreIterationListener(1)));
  JavaRDD<MultiDataSet> rdd=sc.parallelize(list);
  scg.fitMultiDataSet(rdd);
  DataSetIterator iris=new IrisDataSetIterator(1,150);
  List<DataSet> list2=new ArrayList<>();
  while (iris.hasNext())   list2.add(iris.next());
  JavaRDD<DataSet> rddDS=sc.parallelize(list2);
  scg.fit(rddDS);
}
