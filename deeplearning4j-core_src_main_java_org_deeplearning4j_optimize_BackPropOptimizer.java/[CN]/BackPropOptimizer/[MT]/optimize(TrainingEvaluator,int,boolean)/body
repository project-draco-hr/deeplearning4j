{
  if (!lineSearch) {
    log.info("BEGIN BACKPROP WITH SCORE OF " + network.score());
    Double lastEntropy=network.score();
    BaseMultiLayerNetwork revert=network.clone();
    if (network.isForceNumEpochs()) {
      for (int i=0; i < epochs; i++) {
        network.backPropStep(lr);
        log.info("Iteration " + i + " error "+ network.score());
      }
    }
 else {
      boolean train=true;
      int count=0;
      double changeTolerance=1e-5;
      int backPropIterations=0;
      while (train) {
        if (backPropIterations >= epochs) {
          log.info("Backprop number of iterations max hit; converging");
          break;
        }
        count++;
        network.backPropStep(lr);
        network.getOutputLayer().trainTillConvergence(lr,epochs);
        Double entropy=network.score();
        if (lastEntropy == null || entropy < lastEntropy) {
          double diff=Math.abs(entropy - lastEntropy);
          if (diff < changeTolerance) {
            log.info("Not enough of a change on back prop...breaking");
            break;
          }
 else           lastEntropy=entropy;
          log.info("New score " + lastEntropy);
          revert=network.clone();
        }
 else         if (entropy >= lastEntropy) {
          train=false;
          network.update(revert);
          log.info("Reverting to best score " + lastEntropy);
        }
        backPropIterations++;
      }
    }
  }
 else {
    NeuralNetwork.OptimizationAlgorithm optimizationAlgorithm=network.getOptimizationAlgorithm();
    if (optimizationAlgorithm == NeuralNetwork.OptimizationAlgorithm.CONJUGATE_GRADIENT) {
      VectorizedNonZeroStoppingConjugateGradient g=new VectorizedNonZeroStoppingConjugateGradient(this);
      g.setTolerance(1e-3);
      g.setTrainingEvaluator(eval);
      g.setMaxIterations(numEpochs);
      g.optimize(numEpochs);
    }
 else {
      VectorizedDeepLearningGradientAscent g=new VectorizedDeepLearningGradientAscent(this);
      g.setTolerance(1e-3);
      g.setTrainingEvaluator(eval);
      g.optimize(numEpochs);
    }
  }
}
