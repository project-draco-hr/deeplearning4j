{
  List<Integer> is=new ArrayList<>();
  List<DoubleMatrix> xs=new ArrayList<>();
  DoubleMatrix r=network.getBackPropRGradient(ch).subi(gradient);
  DoubleMatrix y=r.div(preCon);
  log.info("Precon mean " + preCon.mean());
  log.info("R  mean " + r.mean());
  log.info("Y mean " + y.mean());
  log.info("Gradient mean " + gradient.mean());
  p=y.neg();
  double mean=p.mean();
  DoubleMatrix x=ch;
  double deltaNew=r.mul(y).sum();
  for (int iterationCount=0; iterationCount < numIterations; iterationCount++) {
    DoubleMatrix Ap=network.getBackPropRGradient(p);
    log.info("P mean for iteration " + iterationCount + " is "+ p.mean());
    double pAp=Ap.mul(p).sum();
    log.info("pAp for iteration " + iterationCount + " is "+ pAp);
    if (pAp < 0)     log.warn("Negative curve!");
    double val=0.5 * SimpleBlas.dot(ch.neg().addi(r).transpose(),x);
    log.info("Iteration on conjugate gradient " + iterationCount + " with value "+ val);
    double alpha=deltaNew / pAp;
    x.addi(p.mul(alpha));
    DoubleMatrix rNew=r.add(Ap.mul(alpha));
    DoubleMatrix yNew=rNew.div(preCon);
    double deltaOld=deltaNew;
    deltaNew=rNew.mul(yNew).sum();
    double beta=deltaNew / deltaOld;
    log.info("Beta for iteration " + iterationCount + " is "+ beta);
    p=yNew.neg().add(p.mul(beta));
    r=rNew;
    val=0.5 * SimpleBlas.dot(ch.neg().addi(r).transpose(),x);
    log.info("Value for " + iterationCount + " is "+ val);
    is.add(iterationCount);
    xs.add(x);
  }
  return new Pair<>(is,xs);
}
