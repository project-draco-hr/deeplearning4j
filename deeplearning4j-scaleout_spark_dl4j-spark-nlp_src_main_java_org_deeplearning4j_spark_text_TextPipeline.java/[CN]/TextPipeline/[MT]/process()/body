{
  JavaSparkContext sc=new JavaSparkContext(corpus.context());
  Accumulator<Counter<String>> wordFreqAcc=sc.accumulator(new Counter<String>(),new WordFreqAccumulator());
  Accumulator<Double> wordCountAcc=sc.accumulator(0L);
  final Broadcast<List<String>> broadcast=sc.broadcast(stopWords);
  JavaRDD<Pair<List<String>,Long>> tokenizedRDD=corpus.map(new TokenizerFunction(DefaultTokenizerFactory.class.getName()));
  VocabCacheFunction accClass=new VocabCacheFunction(broadcast,wordFreqAcc,wordCountAcc);
  tokenizedRDD.foreach(accClass);
  Long totalWordCount=accClass.getWordCountAcc().value().longValue();
  Counter<String> wordFreq=accClass.getWordFreqAcc().value();
  Pair<VocabCache,Long> pair=filterMinWordAddVocab(wordFreq,totalWordCount);
  return pair;
}
