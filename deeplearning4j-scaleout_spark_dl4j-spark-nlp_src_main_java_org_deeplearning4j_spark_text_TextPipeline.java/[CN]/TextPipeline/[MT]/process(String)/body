{
  JavaSparkContext sc=new JavaSparkContext(corpus.context());
  Broadcast<List<String>> broadcast=sc.broadcast(stopWords);
  int nGrams=corpus.context().conf().getInt(Word2VecPerformer.N_GRAMS,1);
  JavaRDD<Pair<List<String>,Long>> tokenizedRDD=corpus.map(new TokenizerFunction(tokenizer,nGrams));
  Pair<VocabCache,Long> corpusCounterPair=tokenizedRDD.map(new VocabCacheFunction(new InMemoryLookupCache(),broadcast)).reduce(new ReduceVocabFunction());
  return filterMinWordAddVocab(corpusCounterPair);
}
