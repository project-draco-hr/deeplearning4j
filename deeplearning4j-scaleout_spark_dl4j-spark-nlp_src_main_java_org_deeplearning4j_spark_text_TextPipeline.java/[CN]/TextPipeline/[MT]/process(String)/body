{
  JavaSparkContext sc=new JavaSparkContext(corpus.context());
  Broadcast<List<String>> broadcast=sc.broadcast(stopWords);
  int nGrams=corpus.context().conf().getInt(Word2VecPerformer.N_GRAMS,1);
  return corpus.map(new TokenizerFunction(tokenizer,nGrams)).map(new VocabCacheFunction(minWordFrequency,new InMemoryLookupCache(),broadcast)).reduce(new Function2<Pair<VocabCache,Long>,Pair<VocabCache,Long>,Pair<VocabCache,Long>>(){
    public Pair<VocabCache,Long> call(    Pair<VocabCache,Long> a,    Pair<VocabCache,Long> b){
      InMemoryLookupCache bVocabCache=(InMemoryLookupCache)b.getFirst();
      InMemoryLookupCache aVocabCache=(InMemoryLookupCache)a.getFirst();
      Counter<String> bWordFreq=bVocabCache.getWordFrequencies();
      bWordFreq.incrementAll(aVocabCache.getWordFrequencies());
      bVocabCache.setWordFrequencies(bWordFreq);
      Long sumWordEncountered=b.getSecond() + a.getSecond();
      return new Pair<>((VocabCache)bVocabCache,sumWordEncountered);
    }
  }
);
}
