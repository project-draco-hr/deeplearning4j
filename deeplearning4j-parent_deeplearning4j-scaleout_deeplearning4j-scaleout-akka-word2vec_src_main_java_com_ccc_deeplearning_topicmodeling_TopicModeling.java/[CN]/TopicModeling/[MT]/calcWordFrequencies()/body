{
  Counter<String> tf=new Counter<String>();
  Counter<String> idf=new Counter<String>();
  for (  File f : rootDir.listFiles()) {
    for (    File doc : f.listFiles()) {
      Set<String> encountered=new HashSet<String>();
      LineIterator iter=FileUtils.lineIterator(doc);
      while (iter.hasNext()) {
        String line=iter.nextLine();
        StringTokenizer tokenizer=new StringTokenizer(new InputHomogenization(line).transform());
        while (tokenizer.hasMoreTokens()) {
          String token=tokenizer.nextToken();
          if (!stopWords.contains(token)) {
            words.incrementCount(token,f.getName(),1.0);
            tf.incrementCount(token,1.0);
            if (!encountered.contains(token)) {
              idf.incrementCount(token,1.0);
              encountered.add(token);
            }
          }
        }
      }
    }
  }
  Counter<String> tfidf=new Counter<String>();
  for (  String key : tf.keySet()) {
    double tfVal=tf.getCount(key);
    double idfVal=idf.getCount(key);
    double tfidfVal=MathUtils.tdidf(tfVal,idfVal);
    tfidf.setCount(key,tfidfVal);
  }
  tfidf.keepTopNKeys(200);
  log.info("Tfidf keys " + tfidf + " got rid of "+ (Math.abs(tfidf.size() - words.size()))+ " words");
  for (  String key : tfidf.getSortedKeys())   vocab.add(key);
}
