{
  InputStream is=FileUtils.openInputStream(new File(args[0]));
  LabelAwareListSentenceIterator iterator=new LabelAwareListSentenceIterator(is,",");
  iterator.setPreProcessor(new SentencePreProcessor(){
    @Override public String preProcess(    String sentence){
      return new InputHomogenization(sentence).transform();
    }
  }
);
  TokenizerFactory tokenizerFactory=new UimaTokenizerFactory();
  TextVectorizer vectorizor=new TfidfVectorizer(iterator,tokenizerFactory,Arrays.asList("0","1"),4000);
  DataSet data=vectorizor.vectorize();
  data.binarize();
  log.info("Vocab " + vectorizor.vocab());
  DataSetIterator iter=new ListDataSetIterator(data.asList(),10);
  HazelCastStateTracker tracker=new HazelCastStateTracker();
  if (!tracker.isPretrain())   throw new IllegalStateException("Tracker should be on pretrain");
  Conf c=new Conf();
  c.setFinetuneEpochs(10000);
  c.setPretrainEpochs(10000);
  c.setFinetuneLearningRate(1e-1);
  c.setLayerSizes(new int[]{iter.inputColumns() / 2,iter.inputColumns() / 4,iter.inputColumns() / 3});
  c.setnIn(iter.inputColumns());
  c.setUseAdaGrad(true);
  c.setMomentum(0.3);
  c.setNormalizeZeroMeanAndUnitVariance(false);
  c.setnOut(2);
  c.setSplit(5);
  c.setHiddenUnit(RBM.HiddenUnit.BINARY);
  c.setVisibleUnit(RBM.VisibleUnit.BINARY);
  c.setMultiLayerClazz(DBN.class);
  c.setUseRegularization(false);
  c.setDeepLearningParams(new Object[]{1,1e-1,1000});
  ActorNetworkRunner runner=new ActorNetworkRunner("master",iter);
  runner.setStateTracker(tracker);
  runner.setup(c);
  runner.train();
}
