{
  if (adaGrad == null)   adaGrad=new AdaGrad(gradient.shape());
  if (iteration != 0 && conf.getResetAdaGradIterations() > 0 && iteration % conf.getResetAdaGradIterations() == 0) {
    adaGrad.historicalGradient=null;
    log.info("Resetting adagrad");
  }
  double momentum=conf.getMomentum();
  if (conf.getMomentumAfter() != null && !conf.getMomentumAfter().isEmpty()) {
    int key=conf.getMomentumAfter().keySet().iterator().next();
    if (iteration >= key) {
      momentum=conf.getMomentumAfter().get(key);
    }
  }
  if (conf.isUseAdaGrad())   gradient=adaGrad.getGradient(gradient);
 else   gradient.muli(conf.getLr());
  if (momentum > 0) {
    gradient=lastStep.mul(momentum).subi(gradient);
    lastStep.assign(gradient);
  }
  if (conf.isUseRegularization() && conf.getL2() > 0)   gradient.subi(params.mul(conf.getL2() * conf.getLr()));
 else   if (conf.isUseRegularization() && conf.getL1() < 0)   gradient.subi(gradient.mul(Transforms.sign(params)).muli(conf.getL1() * conf.getLr()));
  if (conf.isConstrainGradientToUnitNorm())   gradient.divi(gradient.norm2(Integer.MAX_VALUE));
  gradient.divi(batchSize);
}
