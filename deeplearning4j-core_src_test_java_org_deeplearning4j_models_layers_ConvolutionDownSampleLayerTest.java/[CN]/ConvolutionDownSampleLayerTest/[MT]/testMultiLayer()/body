{
  LayerFactory layerFactory=LayerFactories.getFactory(ConvolutionDownSampleLayer.class);
  int batchSize=110;
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.LBFGS).dist(Nd4j.getDistributions().createNormal(0,1)).iterations(100).iterationListener(new ScoreIterationListener(1)).activationFunction("tanh").filterSize(5,1,2,2).constrainGradientToUnitNorm(true).nIn(4).nOut(3).batchSize(batchSize).layerFactory(layerFactory).dropOut(0.5).list(3).preProcessor(0,new ConvolutionPostProcessor()).inputPreProcessor(0,new ConvolutionInputPreProcessor(2,2)).preProcessor(1,new ConvolutionPostProcessor()).inputPreProcessor(1,new ConvolutionInputPreProcessor(3,3)).hiddenLayerSizes(new int[]{4,25}).override(0,new ConfOverride(){
    @Override public void overrideLayer(    int i,    NeuralNetConfiguration.Builder builder){
      if (i == 0)       builder.filterSize(5,1,2,2);
    }
  }
).override(1,new ConfOverride(){
    @Override public void overrideLayer(    int i,    NeuralNetConfiguration.Builder builder){
      if (i == 1)       builder.filterSize(5,1,3,3);
    }
  }
).override(2,new ConfOverride(){
    @Override public void overrideLayer(    int i,    NeuralNetConfiguration.Builder builder){
      if (i == 2) {
        builder.activationFunction("softmax");
        builder.weightInit(WeightInit.ZERO);
        builder.layerFactory(LayerFactories.getFactory(OutputLayer.class));
        builder.lossFunction(LossFunctions.LossFunction.MCXENT);
      }
    }
  }
).build();
  MultiLayerNetwork network=new MultiLayerNetwork(conf);
  DataSetIterator iter=new IrisDataSetIterator(150,150);
  org.nd4j.linalg.dataset.DataSet next=iter.next();
  next.normalizeZeroMeanZeroUnitVariance();
  SplitTestAndTrain trainTest=next.splitTestAndTrain(110);
  network.fit(trainTest.getTrain());
  Evaluation eval=new Evaluation();
  INDArray output=network.output(trainTest.getTest().getFeatureMatrix());
  eval.eval(trainTest.getTest().getLabels(),output);
  log.info("Score " + eval.stats());
}
