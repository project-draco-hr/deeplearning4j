{
  JavaSparkContext sc=new JavaSparkContext(conf);
  JavaRDD<String> corpusRDD=getCorpusRDD(sc);
  word2vec.setRemoveStop(false);
  Broadcast<Map<String,Object>> broadcastTokenizerVarMap=sc.broadcast(word2vec.getTokenizerVarMap());
  TextPipeline pipeline=new TextPipeline(corpusRDD,broadcastTokenizerVarMap);
  pipeline.buildVocabCache();
  pipeline.buildVocabWordListRDD();
  VocabCache vocabCache=pipeline.getVocabCache();
  Huffman huffman=new Huffman(vocabCache.vocabWords());
  huffman.build();
  Map<String,Object> word2vecVarMap=word2vec.getWord2vecVarMap();
  word2vecVarMap.put("totalWordCount",pipeline.getTotalWordCount());
  double[] expTable=word2vec.getExpTable();
  JavaRDD<AtomicLong> sentenceCountRDD=pipeline.getSentenceCountRDD();
  JavaRDD<List<VocabWord>> vocabWordListRDD=pipeline.getVocabWordListRDD();
  CountCumSum countCumSum=new CountCumSum(sentenceCountRDD);
  JavaRDD<Long> sentenceCountCumSumRDD=countCumSum.buildCumSum();
  JavaPairRDD<List<VocabWord>,Long> vocabWordListSentenceCumSumRDD=vocabWordListRDD.zip(sentenceCountCumSumRDD);
  Broadcast<Map<String,Object>> word2vecVarMapBroadcast=sc.broadcast(word2vecVarMap);
  Broadcast<double[]> expTableBroadcast=sc.broadcast(expTable);
  FirstIterationFunction firstIterationFunction=new FirstIterationFunction(word2vecVarMapBroadcast,expTableBroadcast);
  JavaRDD<Pair<Integer,INDArray>> pointSyn0Vec=vocabWordListSentenceCumSumRDD.mapPartitions(firstIterationFunction).map(new MapToPairFunction());
  final Accumulator<Pair<Integer,INDArray>> syn0Acc=sc.accumulator(new Pair<>(0,Nd4j.zeros(vocabCache.numWords(),word2vec.getVectorLength())),new Syn0Accumulator(vocabCache.numWords(),word2vec.getVectorLength()));
  pointSyn0Vec.foreach(new UpdateSyn0AccumulatorFunction(syn0Acc));
  INDArray syn0=syn0Acc.value().getSecond();
  InMemoryLookupTable inMemoryLookupTable=new InMemoryLookupTable();
}
