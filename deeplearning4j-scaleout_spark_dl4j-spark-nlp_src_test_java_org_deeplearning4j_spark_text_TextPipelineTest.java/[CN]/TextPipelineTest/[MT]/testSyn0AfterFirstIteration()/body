{
  JavaSparkContext sc=getContext();
  JavaRDD<String> corpusRDD=getCorpusRDD(sc);
  word2vec.setRemoveStop(false);
  Broadcast<Map<String,Object>> broadcastTokenizerVarMap=sc.broadcast(word2vec.getTokenizerVarMap());
  TextPipeline pipeline=new TextPipeline(corpusRDD,broadcastTokenizerVarMap);
  pipeline.buildVocabCache();
  pipeline.buildVocabWordListRDD();
  VocabCache<VocabWord> vocabCache=pipeline.getVocabCache();
  Huffman huffman=new Huffman(vocabCache.vocabWords());
  huffman.build();
  Map<String,Object> word2vecVarMap=word2vec.getWord2vecVarMap();
  word2vecVarMap.put("totalWordCount",pipeline.getTotalWordCount());
  double[] expTable=word2vec.getExpTable();
  JavaRDD<AtomicLong> sentenceCountRDD=pipeline.getSentenceCountRDD();
  JavaRDD<List<VocabWord>> vocabWordListRDD=pipeline.getVocabWordListRDD();
  CountCumSum countCumSum=new CountCumSum(sentenceCountRDD);
  JavaRDD<Long> sentenceCountCumSumRDD=countCumSum.buildCumSum();
  JavaPairRDD<List<VocabWord>,Long> vocabWordListSentenceCumSumRDD=vocabWordListRDD.zip(sentenceCountCumSumRDD);
  Broadcast<Map<String,Object>> word2vecVarMapBroadcast=sc.broadcast(word2vecVarMap);
  Broadcast<double[]> expTableBroadcast=sc.broadcast(expTable);
  FirstIterationFunction firstIterationFunction=new FirstIterationFunction(word2vecVarMapBroadcast,expTableBroadcast);
  JavaRDD<Pair<Integer,INDArray>> pointSyn0Vec=vocabWordListSentenceCumSumRDD.mapPartitions(firstIterationFunction).map(new MapToPairFunction());
}
