{
  JavaSparkContext sc=new JavaSparkContext(conf);
  JavaRDD<String> corpusRDD=getCorpusRDD(sc);
  Broadcast<Map<String,Object>> broadcastTokenizerVarMap=sc.broadcast(word2vec.getTokenizerVarMap());
  TextPipeline pipeline=new TextPipeline(corpusRDD,broadcastTokenizerVarMap);
  JavaRDD<List<String>> tokenizedRDD=pipeline.tokenize();
  JavaRDD<Pair<List<String>,AtomicLong>> sentenceWordsCountRDD=pipeline.updateAndReturnAccumulatorVal(tokenizedRDD);
  Counter<String> wordFreqCounter=pipeline.getWordFreqAcc().value();
  assertEquals(wordFreqCounter.getCount("STOP"),4,0);
  assertEquals(wordFreqCounter.getCount("strange"),2,0);
  assertEquals(wordFreqCounter.getCount("flowers"),1,0);
  assertEquals(wordFreqCounter.getCount("world"),1,0);
  assertEquals(wordFreqCounter.getCount("red"),1,0);
  List<Pair<List<String>,AtomicLong>> ret=sentenceWordsCountRDD.collect();
  assertEquals(ret.get(0).getFirst(),Arrays.asList("this","is","a","strange","strange","world"));
  assertEquals(ret.get(1).getFirst(),Arrays.asList("flowers","are","red"));
  assertEquals(ret.get(0).getSecond().get(),6);
  assertEquals(ret.get(1).getSecond().get(),3);
  sc.stop();
}
