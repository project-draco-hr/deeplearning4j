{
  int batchSize=epsilon.size(0);
  INDArray reshapeEp=epsilon.dup().reshape(batchSize,shape[1]);
  org.deeplearning4j.nn.conf.layers.BatchNormalization layerConf=layerConf();
  INDArray gBeta=reshapeEp.sum(0);
  if (trainingMode == TrainingMode.TRAIN && layerConf.isUseBatchMean()) {
    gGamma=reshapeEp.mul(xHat).sum(0);
  }
  INDArray gamma=(layerConf.isLockGammaBeta()) ? Nd4j.onesLike(reshapeEp) : getParam(BatchNormalizationParamInitializer.GAMMA);
  INDArray coefficients=Nd4j.getExecutioner().execAndReturn(new BroadcastDivOp(gamma,std,gamma,-1));
  INDArray tmp=Nd4j.getExecutioner().execAndReturn(new BroadcastMulOp(xHat,gGamma,xHat.dup(),-1));
  tmp.addiColumnVector(gBeta).divi(batchSize);
  INDArray gXHat=Nd4j.getExecutioner().execAndReturn(new BroadcastSubOp(tmp,coefficients,tmp,-1));
  INDArray nextEpsilon=reshapeEp.mul(gXHat).reshape(epsilon.shape());
  Gradient retGradient=new DefaultGradient();
  retGradient.setGradientFor(BatchNormalizationParamInitializer.GAMMA,gGamma);
  retGradient.setGradientFor(BatchNormalizationParamInitializer.BETA,gBeta);
  return new Pair<>(retGradient,nextEpsilon);
}
