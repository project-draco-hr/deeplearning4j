{
  int k=(int)params[0];
  double learningRate=(double)params[1];
  if (wAdaGrad != null)   wAdaGrad.setMasterStepSize(learningRate);
  if (hBiasAdaGrad != null)   hBiasAdaGrad.setMasterStepSize(learningRate);
  if (vBiasAdaGrad != null)   vBiasAdaGrad.setMasterStepSize(learningRate);
  Tensor chainStart=propUp(input);
  this.chainStart=chainStart;
  Tensor nvSamples=null;
  Tensor hiddenMeans=chainStart;
  for (int i=0; i < k; i++) {
    nvSamples=propDown(binomial(eHid,1,rng));
    hiddenMeans=propUp(nvSamples);
  }
  Tensor wGradient=new Tensor(W.rows(),W.columns(),W.slices());
  for (int i=0; i < numFilters; i++)   wGradient.setSlice(i,conv2d(input,chainStart.getSlice(i),VALID).sub(conv2d(nvSamples,reverse(hiddenMeans.getSlice(i)),VALID)));
  DoubleMatrix vBiasGradient=DoubleMatrix.scalar(chainStart.sub(hiddenMeans).columnSums().sum());
  DoubleMatrix hBiasGradient=DoubleMatrix.scalar((input.sub(nvSamples)).columnSums().sum());
  NeuralNetworkGradient ret=new NeuralNetworkGradient(wGradient,vBiasGradient,hBiasGradient);
  updateGradientAccordingToParams(ret,learningRate);
  triggerGradientEvents(ret);
  return ret;
}
