{
  INDArray inputData=Nd4j.ones(miniBatchSize,nIn,timeSeriesLength);
  NeuralNetConfiguration conf=new NeuralNetConfiguration.Builder().layer(new org.deeplearning4j.nn.conf.layers.GravesLSTM.Builder().nIn(nIn).nOut(lstmNHiddenUnits).weightInit(WeightInit.DISTRIBUTION).dist(new UniformDistribution(0,1)).activation("tanh").build()).build();
  GravesLSTM lstm=LayerFactories.getFactory(conf.getLayer()).create(conf);
  lstm.activate(inputData);
  assertNotNull(lstm.input());
  INDArray epsilon=Nd4j.ones(miniBatchSize,lstmNHiddenUnits,timeSeriesLength);
  Pair<Gradient,INDArray> out=lstm.backpropGradient(epsilon);
  Gradient outGradient=out.getFirst();
  INDArray nextEpsilon=out.getSecond();
  INDArray biasGradient=outGradient.getGradientFor(GravesLSTMParamInitializer.BIAS_KEY);
  INDArray inWeightGradient=outGradient.getGradientFor(GravesLSTMParamInitializer.INPUT_WEIGHT_KEY);
  INDArray recurrentWeightGradient=outGradient.getGradientFor(GravesLSTMParamInitializer.RECURRENT_WEIGHT_KEY);
  assertNotNull(biasGradient);
  assertNotNull(inWeightGradient);
  assertNotNull(recurrentWeightGradient);
  assertArrayEquals(biasGradient.shape(),new int[]{1,4 * lstmNHiddenUnits});
  assertArrayEquals(inWeightGradient.shape(),new int[]{nIn,4 * lstmNHiddenUnits});
  assertArrayEquals(recurrentWeightGradient.shape(),new int[]{lstmNHiddenUnits,4 * lstmNHiddenUnits + 3});
  assertNotNull(nextEpsilon);
  assertArrayEquals(nextEpsilon.shape(),new int[]{miniBatchSize,nIn,timeSeriesLength});
  for (  String s : outGradient.gradientForVariable().keySet()) {
    lstm.update(outGradient.getGradientFor(s),s);
  }
}
