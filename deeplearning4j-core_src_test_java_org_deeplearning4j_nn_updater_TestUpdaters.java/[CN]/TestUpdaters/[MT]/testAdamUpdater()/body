{
  INDArray m, v;
  double lr=0.01;
  int iteration=0;
  double beta1=0.8;
  double beta2=0.888;
  NeuralNetConfiguration conf=new NeuralNetConfiguration.Builder().learningRate(lr).iterations(iteration).adamMeanDecay(beta1).adamVarDecay(beta2).layer(new DenseLayer.Builder().nIn(nIn).nOut(nOut).updater(org.deeplearning4j.nn.conf.Updater.ADAM).build()).build();
  int numParams=LayerFactories.getFactory(conf).initializer().numParams(conf,true);
  INDArray params=Nd4j.create(1,numParams);
  Layer layer=LayerFactories.getFactory(conf).create(conf,null,0,params,true);
  Updater updater=UpdaterCreator.getUpdater(layer);
  int updaterStateSize=updater.stateSizeForLayer(layer);
  INDArray updaterState=Nd4j.create(1,updaterStateSize);
  updater.setStateViewArray(layer,updaterState,true);
  updater.update(layer,gradient,iteration,1);
  double beta1t=FastMath.pow(beta1,iteration);
  double beta2t=FastMath.pow(beta2,iteration);
  double alphat=lr * FastMath.sqrt(1 - beta2t) / (1 - beta1t);
  Gradient gradientDup=new DefaultGradient();
  gradientDup.setGradientFor(DefaultParamInitializer.WEIGHT_KEY,weightGradient);
  gradientDup.setGradientFor(DefaultParamInitializer.BIAS_KEY,biasGradient);
  for (  Map.Entry<String,INDArray> entry : gradientDup.gradientForVariable().entrySet()) {
    val=entry.getValue();
    m=Nd4j.zeros(val.shape());
    v=Nd4j.zeros(val.shape());
    m.muli(beta1).addi(val.mul(1.0 - beta1));
    v.muli(beta2).addi(val.mul(val).mul(1.0 - beta2));
    gradExpected=m.mul(alphat).divi(Transforms.sqrt(v).addi(epsilon));
    if (!gradExpected.equals(gradient.getGradientFor(entry.getKey()))) {
      System.out.println(Arrays.toString(gradExpected.dup().data().asFloat()));
      System.out.println(Arrays.toString(gradient.getGradientFor(entry.getKey()).dup().data().asFloat()));
    }
    assertEquals(gradExpected,gradient.getGradientFor(entry.getKey()));
  }
  assertEquals(beta1,layer.conf().getLayer().getAdamMeanDecay(),1e-4);
  assertEquals(beta2,layer.conf().getLayer().getAdamVarDecay(),1e-4);
}
