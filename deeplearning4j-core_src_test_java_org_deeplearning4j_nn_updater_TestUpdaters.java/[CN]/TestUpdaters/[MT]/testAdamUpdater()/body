{
  double lr=0.01;
  int iteration=0;
  double beta1=0.9;
  double beta2=0.999;
  NeuralNetConfiguration conf=new NeuralNetConfiguration.Builder().learningRate(lr).iterations(iteration).layer(new DenseLayer.Builder().nIn(nIn).nOut(nOut).updater(org.deeplearning4j.nn.conf.Updater.ADAM).build()).build();
  Layer layer=LayerFactories.getFactory(conf).create(conf,null,0);
  Updater updater=UpdaterCreator.getUpdater(layer);
  updater.update(layer,gradient,iteration);
  INDArray mW, vW, weightGradExpected, mB, vB, biasGradExpected;
  double beta1t=FastMath.pow(beta1,iteration);
  double beta2t=FastMath.pow(beta2,iteration);
  double alphat=lr * FastMath.sqrt(((1 - beta2t) / (1 - beta1t)));
  mW=Nd4j.zeros(weightGradient.shape());
  vW=Nd4j.zeros(weightGradient.shape());
  mW.muli(beta1).addi(weightGradient.mul(1.0 - beta1));
  vW.muli(beta2).addi(weightGradient.mul(weightGradient).mul(1.0 - beta2));
  weightGradExpected=mW.mul(alphat).divi(Transforms.sqrt(vW).addi(1e-8));
  mB=Nd4j.zeros(biasGradient.shape());
  vB=Nd4j.zeros(biasGradient.shape());
  mB.muli(beta1).addi(biasGradient.mul(1.0 - beta1));
  vB.muli(beta2).addi(biasGradient.mul(biasGradient).mul(1.0 - beta2));
  biasGradExpected=mB.mul(alphat).divi(Transforms.sqrt(vB).addi(1e-8));
  INDArray weightGradActual=gradient.getGradientFor(DefaultParamInitializer.WEIGHT_KEY);
  INDArray biasGradActual=gradient.getGradientFor(DefaultParamInitializer.BIAS_KEY);
  assertEquals(weightGradExpected,weightGradActual);
  assertEquals(biasGradExpected,biasGradActual);
  assertEquals(lr,layer.conf().getLr(),1e-4);
}
