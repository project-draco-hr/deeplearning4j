{
  INDArray dxSquared;
  Map<String,INDArray> msg=new HashMap<>();
  Map<String,INDArray> msdx=new HashMap<>();
  double rho=0.85;
  NeuralNetConfiguration conf=new NeuralNetConfiguration.Builder().rho(rho).layer(new DenseLayer.Builder().nIn(nIn).nOut(nOut).updater(org.deeplearning4j.nn.conf.Updater.ADADELTA).build()).build();
  int numParams=conf.getLayer().initializer().numParams(conf,true);
  INDArray params=Nd4j.create(1,numParams);
  Layer layer=conf.getLayer().instantiate(conf,null,0,params,true);
  Updater updater=UpdaterCreator.getUpdater(layer);
  int updaterStateSize=updater.stateSizeForLayer(layer);
  INDArray updaterState=Nd4j.create(1,updaterStateSize);
  updater.setStateViewArray(layer,updaterState,true);
  Gradient gradientDup=new DefaultGradient();
  gradientDup.setGradientFor(DefaultParamInitializer.WEIGHT_KEY,weightGradient.dup());
  gradientDup.setGradientFor(DefaultParamInitializer.BIAS_KEY,biasGradient.dup());
  for (int i=0; i < 2; i++) {
    updater.update(layer,gradient,i,1);
    for (    Map.Entry<String,INDArray> entry : gradientDup.gradientForVariable().entrySet()) {
      key=entry.getKey();
      val=entry.getValue();
      INDArray msgTmp=msg.get(key);
      INDArray msdxTmp=msdx.get(key);
      if (msgTmp == null) {
        msgTmp=Nd4j.zeros(val.shape());
        msdxTmp=Nd4j.zeros(val.shape());
      }
      msgTmp.muli(rho);
      msgTmp.addi(1 - rho).muli(val.mul(val));
      gradExpected=Transforms.sqrt(msdxTmp.add(Nd4j.EPS_THRESHOLD)).divi(Transforms.sqrt(msgTmp.add(Nd4j.EPS_THRESHOLD))).muli(val);
      gradientDup.setGradientFor(key,gradExpected);
      assertEquals(gradExpected,gradient.getGradientFor(entry.getKey()));
      msdxTmp.muli(rho);
      dxSquared=gradExpected.mul(gradExpected);
      msdxTmp.addi(dxSquared.muli(1 - rho));
      msg.put(key,msgTmp);
      msdx.put(key,msdxTmp);
    }
    assertEquals(rho,layer.conf().getLayer().getRho(),1e-4);
  }
}
