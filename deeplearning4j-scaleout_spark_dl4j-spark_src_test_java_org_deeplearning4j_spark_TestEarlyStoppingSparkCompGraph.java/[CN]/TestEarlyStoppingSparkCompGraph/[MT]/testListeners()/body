{
  ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).iterations(1).updater(Updater.SGD).weightInit(WeightInit.XAVIER).graphBuilder().addInputs("in").addLayer("0",new OutputLayer.Builder().nIn(4).nOut(3).lossFunction(LossFunctions.LossFunction.MCXENT).build(),"in").setOutputs("0").pretrain(false).backprop(true).build();
  ComputationGraph net=new ComputationGraph(conf);
  net.setListeners(new ScoreIterationListener(1));
  JavaRDD<DataSet> irisData=getIris();
  EarlyStoppingModelSaver<ComputationGraph> saver=new InMemoryModelSaver<>();
  EarlyStoppingConfiguration<ComputationGraph> esConf=new EarlyStoppingConfiguration.Builder<ComputationGraph>().epochTerminationConditions(new MaxEpochsTerminationCondition(5)).iterationTerminationConditions(new MaxTimeIterationTerminationCondition(1,TimeUnit.MINUTES)).scoreCalculator(new SparkLossCalculatorComputationGraph(irisData.map(new DataSetToMultiDataSetFn()),true,sc.sc())).modelSaver(saver).build();
  LoggingEarlyStoppingListener listener=new LoggingEarlyStoppingListener();
  IEarlyStoppingTrainer<ComputationGraph> trainer=new SparkEarlyStoppingGraphTrainer(getContext().sc(),esConf,net,irisData.map(new DataSetToMultiDataSetFn()),50,150,5,listener);
  trainer.fit();
  assertEquals(1,listener.onStartCallCount);
  assertEquals(5,listener.onEpochCallCount);
  assertEquals(1,listener.onCompletionCallCount);
}
