{
  Nd4j.getRandom().setSeed(12345);
  ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).updater(Updater.NONE).learningRate(1.0).graphBuilder().addInputs("input").addLayer("firstLayer",new DenseLayer.Builder().nIn(4).nOut(5).activation("tanh").build(),"input").addLayer("outputLayer",new OutputLayer.Builder().lossFunction(LossFunctions.LossFunction.MCXENT).activation("softmax").nIn(5).nOut(3).build(),"firstLayer").setOutputs("outputLayer").pretrain(false).backprop(true).build();
  ComputationGraph graph=new ComputationGraph(conf);
  graph.init();
  Nd4j.getRandom().setSeed(12345);
  int nParams=graph.numParams();
  INDArray newParams=Nd4j.rand(1,nParams);
  graph.setParams(newParams);
  DataSet ds=new IrisDataSetIterator(150,150).next();
  ds.normalizeZeroMeanZeroUnitVariance();
  INDArray input=ds.getFeatureMatrix();
  INDArray labels=ds.getLabels();
  if (PRINT_RESULTS) {
    System.out.println("testBasicIris()");
    for (int j=0; j < graph.getNumLayers(); j++)     System.out.println("Layer " + j + " # params: "+ graph.getLayer(j).numParams());
  }
  boolean gradOK=GradientCheckUtil.checkGradients(graph,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,new INDArray[]{input},new INDArray[]{labels});
  String msg="testBasicIris()";
  assertTrue(msg,gradOK);
}
