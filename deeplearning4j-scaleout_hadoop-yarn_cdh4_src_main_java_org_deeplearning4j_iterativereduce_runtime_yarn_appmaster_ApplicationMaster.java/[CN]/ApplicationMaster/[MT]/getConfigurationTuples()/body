{
  Path inputPath=new Path(props.getProperty(ConfigFields.APP_INPUT_PATH));
  FileSystem fs=FileSystem.get(conf);
  FileStatus f=fs.getFileStatus(inputPath);
  Set<ConfigurationTuple> configTuples=new HashSet<ConfigurationTuple>();
  int workerId=0;
  JobConf job=new JobConf(new Configuration());
  job.setInputFormat((Class<? extends InputFormat>)this.inputFormatClass);
  FileInputFormat.setInputPaths(job,inputPath);
  InputSplit[] splits=job.getInputFormat().getSplits(job,job.getNumMapTasks());
  for (  InputSplit split : splits) {
    FileSplit convertedToMetronomeSplit=new FileSplit();
    org.apache.hadoop.mapred.FileSplit hadoopFileSplit=(org.apache.hadoop.mapred.FileSplit)split;
    if (hadoopFileSplit.getLength() - hadoopFileSplit.getStart() > 0) {
      convertedToMetronomeSplit.length=hadoopFileSplit.getLength();
      convertedToMetronomeSplit.offset=hadoopFileSplit.getStart();
      convertedToMetronomeSplit.path=hadoopFileSplit.getPath().toString();
      StartupConfiguration config=StartupConfiguration.newBuilder().setBatchSize(batchSize).setIterations(iterationCount).setOther(appConfig).setSplit(convertedToMetronomeSplit).build();
      String wid="worker-" + workerId;
      ConfigurationTuple tuple=new ConfigurationTuple(split.getLocations()[0],wid,config);
      configTuples.add(tuple);
      workerId++;
      LOG.info("IR_AM_worker: " + wid + " added split: "+ convertedToMetronomeSplit.toString());
    }
 else {
      LOG.info("IR_AM: Culled out 0 length Split: " + convertedToMetronomeSplit.toString());
    }
  }
  LOG.info("Total Splits/Workers: " + configTuples.size());
  return configTuples;
}
