{
  INDArray next=Nd4j.rand(new int[]{1,200});
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).weightInit(WeightInit.XAVIER).iterations(1).seed(123).list().layer(0,new AutoEncoder.Builder().nIn(200).nOut(20).activation("sigmoid").build()).layer(1,new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(LossFunctions.LossFunction.RECONSTRUCTION_CROSSENTROPY).activation("softmax").nIn(20).nOut(10).build()).backprop(true).pretrain(false).build();
  MultiLayerNetwork network=new MultiLayerNetwork(conf);
  network.init();
  network.fit(next);
  MultiLayerConfiguration conf2=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).iterations(1).seed(123).list().layer(0,new AutoEncoder.Builder().nIn(200).nOut(20).activation("identity").build()).layer(1,new org.deeplearning4j.nn.conf.layers.ActivationLayer.Builder().activation("sigmoid").build()).layer(2,new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(LossFunctions.LossFunction.RECONSTRUCTION_CROSSENTROPY).activation("softmax").nIn(20).nOut(10).build()).backprop(true).pretrain(false).build();
  MultiLayerNetwork network2=new MultiLayerNetwork(conf2);
  network2.init();
  network2.fit(next);
  assertEquals(network.getLayer(0).getParam("W"),network2.getLayer(0).getParam("W"));
  assertEquals(network.getLayer(1).getParam("W"),network2.getLayer(2).getParam("W"));
  assertEquals(network.getLayer(0).getParam("b"),network2.getLayer(0).getParam("b"));
  assertEquals(network.getLayer(1).getParam("b"),network2.getLayer(2).getParam("b"));
  network.init();
  network.setInput(next);
  List<INDArray> activations=network.feedForward(true);
  network2.init();
  network2.setInput(next);
  List<INDArray> activations2=network2.feedForward(true);
  assertEquals(activations.get(1).reshape(activations2.get(2).shape()),activations2.get(2));
  assertEquals(activations.get(2),activations2.get(3));
}
