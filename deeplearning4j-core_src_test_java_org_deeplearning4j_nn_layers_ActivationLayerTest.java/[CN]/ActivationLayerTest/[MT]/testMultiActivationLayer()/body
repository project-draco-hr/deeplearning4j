{
  DataSetIterator iter=new MnistDataSetIterator(2,2);
  DataSet next=iter.next();
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.LINE_GRADIENT_DESCENT).iterations(1).seed(123).list().layer(0,new DenseLayer.Builder().nIn(28 * 28 * 1).nOut(10).activation("relu").weightInit(WeightInit.XAVIER).build()).layer(1,new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).weightInit(WeightInit.XAVIER).activation("softmax").nIn(10).nOut(10).build()).backprop(true).pretrain(false).build();
  MultiLayerNetwork network=new MultiLayerNetwork(conf);
  network.init();
  network.fit(next);
  MultiLayerConfiguration conf2=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.LINE_GRADIENT_DESCENT).iterations(1).seed(123).list().layer(0,new DenseLayer.Builder().nIn(28 * 28 * 1).nOut(10).activation("identity").weightInit(WeightInit.XAVIER).build()).layer(1,new org.deeplearning4j.nn.conf.layers.ActivationLayer.Builder().activation("relu").build()).layer(2,new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).weightInit(WeightInit.XAVIER).activation("softmax").nIn(10).nOut(10).build()).backprop(true).pretrain(false).build();
  MultiLayerNetwork network2=new MultiLayerNetwork(conf2);
  network2.init();
  network2.fit(next);
  assertEquals(network.getLayer(0).getParam("W"),network2.getLayer(0).getParam("W"));
  assertEquals(network.getLayer(1).getParam("W"),network2.getLayer(2).getParam("W"));
  assertEquals(network.getLayer(0).getParam("b"),network2.getLayer(0).getParam("b"));
  assertEquals(network.getLayer(1).getParam("b"),network2.getLayer(2).getParam("b"));
}
