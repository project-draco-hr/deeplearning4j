{
  currScore=0.0;
  for (int i=0; i < input.rows(); i++) {
    INDArray combined=currInput == null ? Nd4j.concat(0,input.slice(i),input.slice(i + 1)) : Nd4j.concat(0,input.slice(i),currInput);
    if (i == 0) {
      i++;
    }
    currInput=combined;
    allInput=combined;
    y=encode(true);
    z=decode(y);
    INDArray currVisibleLoss=currInput.sub(z);
    INDArray currHiddenLoss=currVisibleLoss.mmul(getParam(RecursiveParamInitializer.ENCODER_WEIGHT_KEY)).muli(y).muli(y.rsub(1));
    INDArray hiddenGradient=z.transpose().mmul(currHiddenLoss);
    INDArray visibleGradient=y.transpose().mmul(currVisibleLoss);
    if (visibleLoss == null)     visibleLoss=visibleGradient;
 else     visibleLoss.addi(visibleGradient);
    if (hiddenLoss == null)     hiddenLoss=hiddenGradient;
 else     hiddenLoss.addi(hiddenGradient);
    INDArray currVBLoss=currVisibleLoss.isMatrix() ? currVisibleLoss.mean(0) : currVisibleLoss;
    INDArray currBLoss=currHiddenLoss.isMatrix() ? currHiddenLoss.mean(0) : currHiddenLoss;
    if (vbLoss == null)     vbLoss=currVBLoss;
 else     vbLoss.addi(currVBLoss);
    if (bLoss == null)     bLoss=currBLoss;
 else     bLoss.addi(currBLoss);
    currScore+=0.5 * pow(z.sub(allInput),2).mean(Integer.MAX_VALUE).getDouble(0);
  }
  gradient=createGradient(hiddenLoss,visibleLoss,bLoss,vbLoss);
  score=currScore;
}
