{
  SparkConf sparkConf=new SparkConf().setMaster("local[8]").setAppName("sparktest").set("spark.driver.maxResultSize","4g").set("spark.driver.memory","8g").set("spark.executor.memory","8g");
  JavaSparkContext sc=new JavaSparkContext(sparkConf);
  String dataPath="/ext/Temp/SampleRussianCorpus.txt";
  JavaRDD<String> corpus=sc.textFile(dataPath);
  TokenizerFactory t=new DefaultTokenizerFactory();
  t.setTokenPreProcessor(new LowCasePreProcessor());
  Word2Vec word2Vec=new Word2Vec.Builder().setNGrams(1).tokenizerFactory(t).seed(42L).negative(3).useAdaGrad(false).layerSize(100).windowSize(5).learningRate(0.025).minLearningRate(0.0001).iterations(1).batchSize(100).minWordFrequency(5).useUnknown(true).build();
  word2Vec.train(corpus);
  sc.stop();
  WordVectorSerializer.writeWordVectors(word2Vec.getLookupTable(),"/ext/Temp/sparkRuModel.txt");
}
