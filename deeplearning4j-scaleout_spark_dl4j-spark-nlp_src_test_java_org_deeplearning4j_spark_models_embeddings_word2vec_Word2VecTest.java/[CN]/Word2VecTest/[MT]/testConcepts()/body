{
  SparkConf sparkConf=new SparkConf().setMaster("local[4]").setAppName("sparktest").set(VECTOR_LENGTH,String.valueOf(100)).set(ADAGRAD,String.valueOf(false)).set(NEGATIVE,String.valueOf(0)).set(NUM_WORDS,String.valueOf(1)).set(WINDOW,String.valueOf(5)).set(ALPHA,String.valueOf(0.025)).set(MIN_ALPHA,String.valueOf(1e-2)).set(ITERATIONS,String.valueOf(1)).set(N_GRAMS,String.valueOf(1)).set(TOKENIZER,"org.deeplearning4j.text.tokenization.tokenizerfactory.DefaultTokenizerFactory").set(TOKEN_PREPROCESSOR,"org.deeplearning4j.text.tokenization.tokenizer.preprocessor.CommonPreprocessor").set(REMOVE_STOPWORDS,String.valueOf(false));
  JavaSparkContext sc=new JavaSparkContext(sparkConf);
  String dataPath=new ClassPathResource("raw_sentences.txt").getFile().getAbsolutePath();
  JavaRDD<String> corpus=sc.textFile(dataPath);
  Word2Vec word2Vec=new Word2Vec();
  Pair<VocabCache,WeightLookupTable> table=word2Vec.train(corpus);
  WordVectors vectors=WordVectorSerializer.fromPair(new Pair<>((InMemoryLookupTable)table.getSecond(),table.getFirst()));
  Collection<String> words=vectors.wordsNearest("day",10);
  System.out.println(Arrays.toString(words.toArray()));
}
