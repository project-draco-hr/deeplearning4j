{
  System.err.println("running worker..");
  if (args.length < 2) {
    System.err.println("Usage: DL4J_Spark <file> <iters>");
    System.exit(1);
  }
  System.err.println("Setting up Spark Conf");
  SparkConf sparkConf=new SparkConf().setMaster("spark://localhost").setAppName("SparkDebugExample");
  System.out.println("Setting up Spark Context...");
  JavaSparkContext sc=new JavaSparkContext(sparkConf);
  JavaRDD<String> lines=sc.textFile(args[0]);
  long count=lines.count();
  JavaRDD<DataSet> points=lines.map(new RecordReaderFunction(new SVMLightRecordReader(),Integer.parseInt(args[2]),Integer.parseInt(args[3])));
  points.map(new Function<DataSet,Object>(){
    @Override public Object call(    DataSet v1) throws Exception {
      System.out.println(v1.getFeatureMatrix());
      return null;
    }
  }
).cache();
  int ITERATIONS=Integer.parseInt(args[1]);
  long c=lines.count();
  System.out.println("svmLight records: " + c);
  for (int i=1; i <= ITERATIONS; i++) {
    System.out.println("On iteration " + i);
    System.out.println("end iteration " + i);
    sc.stop();
  }
}
