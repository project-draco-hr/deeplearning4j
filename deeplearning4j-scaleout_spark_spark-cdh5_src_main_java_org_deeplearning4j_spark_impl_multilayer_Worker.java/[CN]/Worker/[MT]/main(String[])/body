{
  if (args.length < 2) {
    System.err.println("Usage: JavaHdfsLR <file> <iters>");
    System.exit(1);
  }
  SparkConf sparkConf=new SparkConf().setAppName("DL4J");
  JavaSparkContext sc=new JavaSparkContext(sparkConf);
  JavaRDD<String> lines=sc.textFile(args[0]);
  JavaRDD<DataPoint> points=lines.map(new ParsePoint()).cache();
  int ITERATIONS=Integer.parseInt(args[1]);
  double[] w=new double[D];
  for (int i=1; i <= ITERATIONS; i++) {
    System.out.println("On iteration " + i);
    double[] gradient=points.map(new Worker(w)).reduce(new MasterComputeParameterAverage());
    for (int j=0; j < D; j++) {
      w[j]-=gradient[j];
    }
  }
  sc.stop();
}
