{
  INDArray wGradient=gradient.getwGradient();
  INDArray hBiasGradient=gradient.gethBiasGradient();
  INDArray vBiasGradient=gradient.getvBiasGradient();
  if (iteration != 0 && conf.getResetAdaGradIterations() > 0 && iteration % conf.getResetAdaGradIterations() == 0) {
    wAdaGrad.historicalGradient=null;
    hBiasAdaGrad.historicalGradient=null;
    vBiasAdaGrad.historicalGradient=null;
    if (this.W != null && this.wAdaGrad == null)     this.wAdaGrad=new AdaGrad(this.W.rows(),this.W.columns());
    if (this.vBias != null && this.vBiasAdaGrad == null)     this.vBiasAdaGrad=new AdaGrad(this.vBias.rows(),this.vBias.columns());
    if (this.hBias != null && this.hBiasAdaGrad == null)     this.hBiasAdaGrad=new AdaGrad(this.hBias.rows(),this.hBias.columns());
    log.info("Resetting adagrad");
  }
  INDArray wLearningRates=wAdaGrad.getLearningRates(wGradient);
  float momentum=conf.getMomentum();
  if (conf.getMomentumAfter() != null && !conf.getMomentumAfter().isEmpty()) {
    int key=conf.getMomentumAfter().keySet().iterator().next();
    if (iteration >= key) {
      momentum=conf.getMomentumAfter().get(key);
    }
  }
  if (conf.isUseAdaGrad())   wGradient.muli(wLearningRates);
 else   wGradient.muli(learningRate);
  if (conf.isUseAdaGrad())   hBiasGradient=hBiasGradient.mul(hBiasAdaGrad.getLearningRates(hBiasGradient));
 else   hBiasGradient=hBiasGradient.mul(learningRate);
  if (conf.isUseAdaGrad())   vBiasGradient=vBiasGradient.mul(vBiasAdaGrad.getLearningRates(vBiasGradient));
 else   vBiasGradient=vBiasGradient.mul(learningRate);
  if (this.hBiasGradient != null)   applySparsity(hBiasGradient);
  if (momentum != 0 && this.wGradient != null)   wGradient.addi(this.wGradient.mul(momentum).add(wGradient.mul(1 - momentum)));
  if (momentum != 0 && this.vBiasGradient != null)   vBiasGradient.addi(this.vBiasGradient.mul(momentum).add(vBiasGradient.mul(1 - momentum)));
  if (momentum != 0 && this.hBiasGradient != null)   hBiasGradient.addi(this.hBiasGradient.mul(momentum).add(hBiasGradient.mul(1 - momentum)));
  wGradient.divi(lastMiniBatchSize);
  vBiasGradient.divi(lastMiniBatchSize);
  hBiasGradient.divi(lastMiniBatchSize);
  if (conf.isUseRegularization() && conf.getL2() > 0) {
    if (conf.isUseAdaGrad())     wGradient.subi(W.mul(conf.getL2()).mul(wLearningRates));
 else     wGradient.subi(W.mul(conf.getL2() * learningRate));
  }
  if (conf.isConstrainGradientToUnitNorm()) {
    wGradient.divi(wGradient.norm2(Integer.MAX_VALUE));
    vBiasGradient.divi(vBiasGradient.norm2(Integer.MAX_VALUE));
    hBiasGradient.divi(hBiasGradient.norm2(Integer.MAX_VALUE));
  }
  this.wGradient=wGradient;
  this.vBiasGradient=vBiasGradient;
  this.hBiasGradient=hBiasGradient;
}
