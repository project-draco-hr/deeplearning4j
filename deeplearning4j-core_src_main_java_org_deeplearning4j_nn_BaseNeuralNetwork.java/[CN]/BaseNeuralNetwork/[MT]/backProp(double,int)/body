{
  double currRecon=getReConstructionCrossEntropy();
  boolean train=true;
  NeuralNetwork revert=clone();
  int numEpochs=0;
  while (train) {
    if (numEpochs > epochs)     break;
    DoubleMatrix output=reconstruct(input);
    DoubleMatrix delta=output.sub(input);
    if (isUseAdaGrad())     delta.muli(getAdaGrad().getLearningRates(delta));
 else     delta.muli(lr);
    if (momentum != 0)     delta.muli(momentum).add(delta.mul(1 - momentum));
    if (normalizeByInputRows)     delta.divi(input.rows);
    DoubleMatrix means=delta.columnMeans().transpose();
    getW().subiColumnVector(means);
    double newRecon=getReConstructionCrossEntropy();
    if (newRecon >= currRecon) {
      update((BaseNeuralNetwork)revert);
      log.info("Converged for new recon; breaking...");
      break;
    }
 else {
      currRecon=newRecon;
      revert=clone();
      log.info("Recon went down " + currRecon);
    }
  }
}
