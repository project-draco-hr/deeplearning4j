{
  double currRecon=getReConstructionCrossEntropy();
  boolean train=true;
  NeuralNetwork revert=clone();
  int numEpochs=0;
  while (train) {
    if (numEpochs > epochs)     break;
    NeuralNetworkGradient gradient=getGradient(extraParams);
    DoubleMatrix wLearningRates=getAdaGrad().getLearningRates(gradient.getwGradient());
    Pair<DoubleMatrix,DoubleMatrix> sample=sampleHiddenGivenVisible(input);
    DoubleMatrix hiddenSample=sample.getSecond().transpose();
    DoubleMatrix scaledInput=input.dup();
    MatrixUtil.normalizeZeroMeanAndUnitVariance(scaledInput);
    DoubleMatrix z=reconstruct(input);
    MatrixUtil.normalizeZeroMeanAndUnitVariance(z);
    DoubleMatrix outputDiff=z.sub(scaledInput);
    DoubleMatrix delta=hiddenSample.mmul(outputDiff).transpose();
    DoubleMatrix hBiasMean=sample.getFirst().columnSums().transpose();
    if (isUseAdaGrad())     delta.muli(wLearningRates);
 else     delta.muli(lr);
    if (momentum != 0)     delta.muli(momentum).add(delta.mul(1 - momentum));
    if (normalizeByInputRows)     delta.divi(input.rows);
    getW().subi(delta);
    if (isUseAdaGrad())     hBiasMean.muli(gethBiasAdaGrad().getLearningRates(gradient.gethBiasGradient()));
 else     hBiasMean.muli(lr);
    if (momentum != 0)     hBiasMean.muli(momentum).add(hBiasMean.mul(1 - momentum));
    if (normalizeByInputRows)     hBiasMean.divi(input.rows);
    gethBias().subi(hBiasMean);
    double newRecon=getReConstructionCrossEntropy();
    if (newRecon > currRecon) {
      update((BaseNeuralNetwork)revert);
      log.info("Converged for new recon; breaking...");
      break;
    }
 else     if (newRecon == currRecon)     continue;
 else {
      currRecon=newRecon;
      revert=clone();
      log.info("Recon went down " + currRecon);
    }
    numEpochs++;
    int plotEpochs=getRenderEpochs();
    if (plotEpochs > 0) {
      NeuralNetPlotter plotter=new NeuralNetPlotter();
      if (numEpochs % plotEpochs == 0) {
        plotter.plotNetworkGradient(this,getGradient(extraParams));
      }
    }
  }
}
