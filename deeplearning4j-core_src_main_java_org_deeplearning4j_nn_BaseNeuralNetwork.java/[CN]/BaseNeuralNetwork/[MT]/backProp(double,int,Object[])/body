{
  double currRecon=squaredLoss();
  boolean train=true;
  NeuralNetwork revert=clone();
  int numEpochs=0;
  while (train) {
    if (numEpochs > epochs)     break;
    NeuralNetworkGradient gradient=getGradient(extraParams);
    updateGradientAccordingToParams(gradient,lr);
    double newRecon=this.squaredLoss();
    if (newRecon > currRecon || currRecon < 0 && newRecon < currRecon) {
      update((BaseNeuralNetwork)revert);
      log.info("Converged for new recon; breaking...");
      break;
    }
 else     if (Double.isNaN(newRecon) || Double.isInfinite(newRecon)) {
      update((BaseNeuralNetwork)revert);
      log.info("Converged for new recon; breaking...");
      break;
    }
 else     if (newRecon == currRecon)     break;
 else {
      currRecon=newRecon;
      revert=clone();
      log.info("Recon went down " + currRecon);
    }
    numEpochs++;
    int plotEpochs=getRenderEpochs();
    if (plotEpochs > 0) {
      NeuralNetPlotter plotter=new NeuralNetPlotter();
      if (numEpochs % plotEpochs == 0) {
        plotter.plotNetworkGradient(this,getGradient(extraParams),getInput().rows);
      }
    }
  }
}
