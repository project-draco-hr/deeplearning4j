{
  final MultiDimensionalMap<String,String,FloatMatrix> binaryTD=MultiDimensionalMap.newTreeBackedMap();
  final MultiDimensionalMap<String,String,FloatTensor> binaryFloatTensorTD=MultiDimensionalMap.newTreeBackedMap();
  final MultiDimensionalMap<String,String,FloatMatrix> binaryCD=MultiDimensionalMap.newTreeBackedMap();
  final Map<String,FloatMatrix> unaryCD=new TreeMap<>();
  final Map<String,FloatMatrix> wordVectorD=new TreeMap<>();
  for (  MultiDimensionalMap.Entry<String,String,FloatMatrix> entry : binaryTransform.entrySet()) {
    int numRows=entry.getValue().rows;
    int numCols=entry.getValue().columns;
    binaryTD.put(entry.getFirstKey(),entry.getSecondKey(),new FloatMatrix(numRows,numCols));
  }
  if (!combineClassification) {
    for (    MultiDimensionalMap.Entry<String,String,FloatMatrix> entry : binaryClassification.entrySet()) {
      int numRows=entry.getValue().rows;
      int numCols=entry.getValue().columns;
      binaryCD.put(entry.getFirstKey(),entry.getSecondKey(),new FloatMatrix(numRows,numCols));
    }
  }
  if (useFloatTensors) {
    for (    MultiDimensionalMap.Entry<String,String,FloatTensor> entry : binaryFloatTensors.entrySet()) {
      int numRows=entry.getValue().rows();
      int numCols=entry.getValue().columns;
      int numSlices=entry.getValue().slices();
      binaryFloatTensorTD.put(entry.getFirstKey(),entry.getSecondKey(),new FloatTensor(numRows,numCols,numSlices));
    }
  }
  for (  Map.Entry<String,FloatMatrix> entry : unaryClassification.entrySet()) {
    int numRows=entry.getValue().rows;
    int numCols=entry.getValue().columns;
    unaryCD.put(entry.getKey(),new FloatMatrix(numRows,numCols));
  }
  for (  Map.Entry<String,FloatMatrix> entry : featureVectors.entrySet()) {
    int numRows=entry.getValue().rows;
    int numCols=entry.getValue().columns;
    wordVectorD.put(entry.getKey(),new FloatMatrix(numRows,numCols));
  }
  final List<Tree> forwardPropTrees=new CopyOnWriteArrayList<>();
  Parallelization.iterateInParallel(trainingTrees,new Parallelization.RunnableWithParams<Tree>(){
    public void run(    Tree currentItem,    Object[] args){
      Tree trainingTree=currentItem.clone();
      forwardPropagateTree(trainingTree);
      forwardPropTrees.add(trainingTree);
    }
  }
,rnTnActorSystem);
  final AtomicDouble error=new AtomicDouble(0);
  Parallelization.iterateInParallel(forwardPropTrees,new Parallelization.RunnableWithParams<Tree>(){
    public void run(    Tree currentItem,    Object[] args){
      backpropDerivativesAndError(currentItem,binaryTD,binaryCD,binaryFloatTensorTD,unaryCD,wordVectorD);
      error.addAndGet(currentItem.errorSum());
    }
  }
,new Parallelization.RunnableWithParams<Tree>(){
    public void run(    Tree currentItem,    Object[] args){
    }
  }
,rnTnActorSystem,new Object[]{binaryTD,binaryCD,binaryFloatTensorTD,unaryCD,wordVectorD});
  float scale=(1.0f / trainingTrees.size());
  value=error.floatValue() * scale;
  value+=scaleAndRegularize(binaryTD,binaryTransform,scale,regTransformMatrix);
  value+=scaleAndRegularize(binaryCD,binaryClassification,scale,regClassification);
  value+=scaleAndRegularizeFloatTensor(binaryFloatTensorTD,binaryFloatTensors,scale,regTransformFloatTensor);
  value+=scaleAndRegularize(unaryCD,unaryClassification,scale,regClassification);
  value+=scaleAndRegularize(wordVectorD,featureVectors,scale,regWordVector);
  FloatMatrix derivative=MatrixUtil.toFlattenedFloat(getNumParameters(),binaryTD.values().iterator(),binaryCD.values().iterator(),binaryFloatTensorTD.values().iterator(),unaryCD.values().iterator(),wordVectorD.values().iterator());
  if (paramAdaGrad == null)   paramAdaGrad=new AdaGradFloat(1,derivative.columns);
  derivative.muli(paramAdaGrad.getLearningRates(derivative));
  return derivative;
}
