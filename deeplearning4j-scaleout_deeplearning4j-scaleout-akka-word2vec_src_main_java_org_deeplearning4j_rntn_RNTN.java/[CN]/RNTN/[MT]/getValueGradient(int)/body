{
  final MultiDimensionalMap<String,String,DoubleMatrix> binaryTD=MultiDimensionalMap.newTreeBackedMap();
  final MultiDimensionalMap<String,String,Tensor> binaryTensorTD=MultiDimensionalMap.newTreeBackedMap();
  final MultiDimensionalMap<String,String,DoubleMatrix> binaryCD=MultiDimensionalMap.newTreeBackedMap();
  final Map<String,DoubleMatrix> unaryCD=new TreeMap<>();
  final Map<String,DoubleMatrix> wordVectorD=new TreeMap<>();
  for (  MultiDimensionalMap.Entry<String,String,DoubleMatrix> entry : binaryTransform.entrySet()) {
    int numRows=entry.getValue().rows;
    int numCols=entry.getValue().columns;
    binaryTD.put(entry.getFirstKey(),entry.getSecondKey(),new DoubleMatrix(numRows,numCols));
  }
  if (!combineClassification) {
    for (    MultiDimensionalMap.Entry<String,String,DoubleMatrix> entry : binaryClassification.entrySet()) {
      int numRows=entry.getValue().rows;
      int numCols=entry.getValue().columns;
      binaryCD.put(entry.getFirstKey(),entry.getSecondKey(),new DoubleMatrix(numRows,numCols));
    }
  }
  if (useTensors) {
    for (    MultiDimensionalMap.Entry<String,String,Tensor> entry : binaryTensors.entrySet()) {
      int numRows=entry.getValue().rows();
      int numCols=entry.getValue().columns;
      int numSlices=entry.getValue().slices();
      binaryTensorTD.put(entry.getFirstKey(),entry.getSecondKey(),new Tensor(numRows,numCols,numSlices));
    }
  }
  for (  Map.Entry<String,DoubleMatrix> entry : unaryClassification.entrySet()) {
    int numRows=entry.getValue().rows;
    int numCols=entry.getValue().columns;
    unaryCD.put(entry.getKey(),new DoubleMatrix(numRows,numCols));
  }
  for (  Map.Entry<String,DoubleMatrix> entry : featureVectors.entrySet()) {
    int numRows=entry.getValue().rows;
    int numCols=entry.getValue().columns;
    wordVectorD.put(entry.getKey(),new DoubleMatrix(numRows,numCols));
  }
  final List<Tree> forwardPropTrees=new CopyOnWriteArrayList<>();
  Parallelization.iterateInParallel(trainingTrees,new Parallelization.RunnableWithParams<Tree>(){
    @Override public void run(    Tree currentItem,    Object[] args){
      Tree trainingTree=currentItem.clone();
      forwardPropagateTree(trainingTree);
      forwardPropTrees.add(trainingTree);
    }
  }
,rnTnActorSystem);
  final AtomicDouble error=new AtomicDouble(0);
  Parallelization.iterateInParallel(forwardPropTrees,new Parallelization.RunnableWithParams<Tree>(){
    @Override public void run(    Tree currentItem,    Object[] args){
      backpropDerivativesAndError(currentItem,binaryTD,binaryCD,binaryTensorTD,unaryCD,wordVectorD);
    }
  }
,new Parallelization.RunnableWithParams<Tree>(){
    @Override public void run(    Tree currentItem,    Object[] args){
      error.addAndGet(currentItem.errorSum());
    }
  }
,rnTnActorSystem,new Object[]{binaryTD,binaryCD,binaryTensorTD,unaryCD,wordVectorD});
  double scale=(1.0 / trainingTrees.size());
  value=error.doubleValue() * scale;
  value+=scaleAndRegularize(binaryTD,binaryTransform,scale,regTransformMatrix);
  value+=scaleAndRegularize(binaryCD,binaryClassification,scale,regClassification);
  value+=scaleAndRegularizeTensor(binaryTensorTD,binaryTensors,scale,regTransformTensor);
  value+=scaleAndRegularize(unaryCD,unaryClassification,scale,regClassification);
  value+=scaleAndRegularize(wordVectorD,featureVectors,scale,regWordVector);
  DoubleMatrix derivative=MatrixUtil.toFlattened(getNumParameters(),binaryTD.values().iterator(),binaryCD.values().iterator(),binaryTensorTD.values().iterator(),unaryCD.values().iterator(),wordVectorD.values().iterator());
  if (paramAdaGrad == null)   paramAdaGrad=new AdaGrad(1,derivative.columns);
  derivative.muli(paramAdaGrad.getLearningRates(derivative));
  return derivative;
}
