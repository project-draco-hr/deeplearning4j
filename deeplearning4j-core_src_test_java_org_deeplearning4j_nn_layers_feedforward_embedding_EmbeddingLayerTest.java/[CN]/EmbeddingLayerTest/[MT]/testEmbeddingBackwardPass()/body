{
  int nClassesIn=10;
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().activation("tanh").list().layer(0,new EmbeddingLayer.Builder().nIn(nClassesIn).nOut(5).build()).layer(1,new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).nIn(5).nOut(4).activation("softmax").build()).pretrain(false).backprop(true).build();
  MultiLayerConfiguration conf2=new NeuralNetConfiguration.Builder().activation("tanh").weightInit(WeightInit.XAVIER).list().layer(0,new DenseLayer.Builder().nIn(nClassesIn).nOut(5).build()).layer(1,new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).nIn(5).nOut(4).activation("softmax").build()).pretrain(false).backprop(true).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  MultiLayerNetwork net2=new MultiLayerNetwork(conf2);
  net.init();
  net2.init();
  net2.setParams(net.params().dup());
  int batchSize=3;
  INDArray inEmbedding=Nd4j.create(batchSize,1);
  INDArray inOneHot=Nd4j.create(batchSize,nClassesIn);
  INDArray outLabels=Nd4j.create(batchSize,4);
  Random r=new Random(12345);
  for (int i=0; i < batchSize; i++) {
    int classIdx=r.nextInt(nClassesIn);
    inEmbedding.putScalar(i,classIdx);
    inOneHot.putScalar(new int[]{i,classIdx},1.0);
    int labelIdx=r.nextInt(4);
    outLabels.putScalar(new int[]{i,labelIdx},1.0);
  }
  net.setInput(inEmbedding);
  net2.setInput(inOneHot);
  net.setLabels(outLabels);
  net2.setLabels(outLabels);
  net.computeGradientAndScore();
  net2.computeGradientAndScore();
  System.out.println(net.score() + "\t" + net2.score());
  assertEquals(net2.score(),net.score(),1e-6);
  Map<String,INDArray> gradient=net.gradient().gradientForVariable();
  Map<String,INDArray> gradient2=net2.gradient().gradientForVariable();
  assertEquals(gradient.size(),gradient2.size());
  for (  String s : gradient.keySet()) {
    assertEquals(gradient2.get(s),gradient.get(s));
  }
}
