{
  int nClassesIn=10;
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().activation("tanh").list().layer(0,new EmbeddingLayer.Builder().nIn(nClassesIn).nOut(5).build()).layer(1,new GravesLSTM.Builder().nIn(5).nOut(7).activation("softsign").build()).layer(2,new RnnOutputLayer.Builder(LossFunctions.LossFunction.MCXENT).nIn(7).nOut(4).activation("softmax").build()).inputPreProcessor(0,new RnnToFeedForwardPreProcessor()).inputPreProcessor(1,new FeedForwardToRnnPreProcessor()).pretrain(false).backprop(true).build();
  MultiLayerConfiguration conf2=new NeuralNetConfiguration.Builder().activation("tanh").weightInit(WeightInit.XAVIER).list().layer(0,new DenseLayer.Builder().nIn(nClassesIn).nOut(5).build()).layer(1,new GravesLSTM.Builder().nIn(5).nOut(7).activation("softsign").build()).layer(2,new RnnOutputLayer.Builder(LossFunctions.LossFunction.MCXENT).nIn(7).nOut(4).activation("softmax").build()).inputPreProcessor(0,new RnnToFeedForwardPreProcessor()).inputPreProcessor(1,new FeedForwardToRnnPreProcessor()).pretrain(false).backprop(true).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  MultiLayerNetwork net2=new MultiLayerNetwork(conf2);
  net.init();
  net2.init();
  net2.setParams(net.params().dup());
  int batchSize=3;
  int timeSeriesLength=8;
  INDArray inEmbedding=Nd4j.create(batchSize,1,timeSeriesLength);
  INDArray inOneHot=Nd4j.create(batchSize,nClassesIn,timeSeriesLength);
  INDArray outLabels=Nd4j.create(batchSize,4,timeSeriesLength);
  Random r=new Random(12345);
  for (int i=0; i < batchSize; i++) {
    for (int j=0; j < timeSeriesLength; j++) {
      int classIdx=r.nextInt(nClassesIn);
      inEmbedding.putScalar(new int[]{i,0,j},classIdx);
      inOneHot.putScalar(new int[]{i,classIdx,j},1.0);
      int labelIdx=r.nextInt(4);
      outLabels.putScalar(new int[]{i,labelIdx,j},1.0);
    }
  }
  net.setInput(inEmbedding);
  net2.setInput(inOneHot);
  net.setLabels(outLabels);
  net2.setLabels(outLabels);
  net.computeGradientAndScore();
  net2.computeGradientAndScore();
  System.out.println(net.score() + "\t" + net2.score());
  assertEquals(net2.score(),net.score(),1e-6);
  Map<String,INDArray> gradient=net.gradient().gradientForVariable();
  Map<String,INDArray> gradient2=net2.gradient().gradientForVariable();
  assertEquals(gradient.size(),gradient2.size());
  for (  String s : gradient.keySet()) {
    assertEquals(gradient2.get(s),gradient.get(s));
  }
}
