{
  INDArray inputData=Nd4j.ones(miniBatchSize,nIn,timeSeriesLength);
  NeuralNetConfiguration conf=new NeuralNetConfiguration.Builder().layer(new org.deeplearning4j.nn.conf.layers.GRU.Builder().nIn(nIn).nOut(gruNHiddenUnits).weightInit(WeightInit.DISTRIBUTION).dist(new UniformDistribution(0,1)).activation("tanh").build()).build();
  GRU gru=LayerFactories.getFactory(conf.getLayer()).create(conf);
  gru.activate(inputData);
  assertNotNull(gru.input());
  INDArray epsilon=Nd4j.ones(miniBatchSize,gruNHiddenUnits,timeSeriesLength);
  Pair<Gradient,INDArray> out=gru.backpropGradient(epsilon);
  Gradient outGradient=out.getFirst();
  INDArray nextEpsilon=out.getSecond();
  INDArray biasGradient=outGradient.getGradientFor(GRUParamInitializer.BIAS_KEY);
  INDArray inWeightGradient=outGradient.getGradientFor(GRUParamInitializer.INPUT_WEIGHT_KEY);
  INDArray recurrentWeightGradient=outGradient.getGradientFor(GRUParamInitializer.RECURRENT_WEIGHT_KEY);
  assertNotNull(biasGradient);
  assertNotNull(inWeightGradient);
  assertNotNull(recurrentWeightGradient);
  assertArrayEquals(biasGradient.shape(),new int[]{1,3 * gruNHiddenUnits});
  assertArrayEquals(inWeightGradient.shape(),new int[]{nIn,3 * gruNHiddenUnits});
  assertArrayEquals(recurrentWeightGradient.shape(),new int[]{gruNHiddenUnits,3 * gruNHiddenUnits});
  assertNotNull(nextEpsilon);
  assertArrayEquals(nextEpsilon.shape(),new int[]{miniBatchSize,nIn,timeSeriesLength});
}
