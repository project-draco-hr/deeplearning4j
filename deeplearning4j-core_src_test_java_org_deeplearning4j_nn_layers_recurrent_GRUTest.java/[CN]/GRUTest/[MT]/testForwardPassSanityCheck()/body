{
  Random r=new Random(12345L);
  int timeSeriesLength=20;
  int nIn=5;
  int nOut=4;
  int gruNUnits=7;
  int miniBatchSize=11;
  INDArray inputData=Nd4j.ones(miniBatchSize,nIn,timeSeriesLength);
  for (int i=0; i < miniBatchSize; i++) {
    for (int j=0; j < nIn; j++) {
      for (int k=0; k < timeSeriesLength; k++) {
        inputData.putScalar(new int[]{i,j,k},r.nextDouble() - 0.5);
      }
    }
  }
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().regularization(false).list(2).layer(0,new org.deeplearning4j.nn.conf.layers.GRU.Builder().activation("tanh").weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,0.1)).nIn(nIn).nOut(gruNUnits).build()).layer(1,new RnnOutputLayer.Builder(LossFunction.MCXENT).activation("softmax").weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,0.1)).nIn(gruNUnits).nOut(nOut).build()).build();
  MultiLayerNetwork mln=new MultiLayerNetwork(conf);
  mln.init();
  List<INDArray> activations=mln.feedForward(inputData);
  INDArray gruActiv=activations.get(1);
  INDArray outActiv=activations.get(2);
  assertArrayEquals(gruActiv.shape(),new int[]{miniBatchSize,gruNUnits,timeSeriesLength});
  assertArrayEquals(outActiv.shape(),new int[]{miniBatchSize * timeSeriesLength,nOut});
  for (int i=0; i < gruActiv.length(); i++) {
    double d=gruActiv.getDouble(i);
    assertTrue(!Double.isNaN(d) && !Double.isInfinite(d));
    assertTrue(d >= -1.0 && d <= 1.0);
  }
  for (int i=0; i < outActiv.length(); i++) {
    double d=outActiv.getDouble(i);
    assertTrue(!Double.isNaN(d) && !Double.isInfinite(d));
    assertTrue(d >= 0.0 && d <= 1.0);
  }
}
