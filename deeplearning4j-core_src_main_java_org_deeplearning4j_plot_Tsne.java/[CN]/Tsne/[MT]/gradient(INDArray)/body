{
  INDArray sumY=pow(y,2).sum(1);
  if (yIncs == null)   yIncs=zeros(y.shape());
  if (gains == null)   gains=ones(y.shape());
  INDArray qu=y.mmul(y.transpose()).muli(-2).addiRowVector(sumY).transpose().addiRowVector(sumY).addi(1).rdivi(1);
  int n=y.rows();
  doAlongDiagonal(qu,new Zero());
  INDArray q=qu.div(qu.sum(Integer.MAX_VALUE));
  BooleanIndexing.applyWhere(q,Conditions.lessThan(realMin),new Value(realMin));
  INDArray PQ=p.sub(q);
  INDArray yGrads=getYGradient(n,PQ,qu);
  gains=gains.add(.2).muli(yGrads.cond(Conditions.greaterThan(0)).neqi(yIncs.cond(Conditions.greaterThan(0)))).addi(gains.mul(0.8).muli(yGrads.cond(Conditions.greaterThan(0)).eqi(yIncs.cond(Conditions.greaterThan(0)))));
  BooleanIndexing.applyWhere(gains,Conditions.lessThan(minGain),new Value(minGain));
  INDArray gradChange=gains.mul(yGrads);
  if (useAdaGrad)   gradChange=adaGrad.getGradient(gradChange);
 else   gradChange.muli(learningRate);
  yIncs.muli(momentum).subi(gradChange);
  double cost=p.mul(log(p.div(q),false)).sum(Integer.MAX_VALUE).getDouble(0);
  return new Pair<>(cost,yIncs);
}
