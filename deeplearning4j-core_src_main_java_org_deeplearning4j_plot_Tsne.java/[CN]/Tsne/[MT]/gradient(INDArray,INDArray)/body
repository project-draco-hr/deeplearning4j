{
  INDArray sumY=pow(y,2).sum(1);
  if (yIncs == null)   yIncs=zeros(y.shape());
  if (gains == null)   gains=ones(y.shape());
  INDArray qu=y.mmul(y.transpose()).muli(-2).addiRowVector(sumY).transpose().addiRowVector(sumY).addi(1).rdivi(1);
  int n=y.rows();
  doAlongDiagonal(qu,new Zero());
  INDArray q=max(qu.div(qu.sum(Integer.MAX_VALUE)),realMin);
  qu.data().flush();
  INDArray PQ=p.sub(q);
  INDArray yGrads=Nd4j.create(y.shape());
  for (int i=0; i < n; i++) {
    INDArray sum1=Nd4j.tile(PQ.getRow(i).mul(qu.getRow(i)),new int[]{y.columns(),1}).transpose().mul(y.getRow(i).broadcast(y.shape()).sub(y)).sum(0);
    yGrads.putRow(i,sum1);
  }
  gains=gains.add(.2).muli(yGrads.cond(Conditions.greaterThan(0)).neqi(yIncs.cond(Conditions.greaterThan(0)))).addi(gains.mul(0.8).muli(yGrads.cond(Conditions.greaterThan(0)).eqi(yIncs.cond(Conditions.greaterThan(0)))));
  BooleanIndexing.applyWhere(gains,Conditions.lessThan(minGain),new Value(minGain));
  INDArray gradChange=gains.mul(yGrads);
  if (useAdaGrad)   gradChange.muli(adaGrad.getLearningRates(gradChange));
 else   gradChange.muli(learningRate);
  yIncs.muli(momentum).subi(gradChange);
  double cost=p.mul(log(p.div(q))).sum(Integer.MAX_VALUE).getDouble(0);
  return new Pair<>(cost,yIncs);
}
