{
  int miniBatchSizePerWorker=10;
  int nWorkers=4;
  for (  boolean saveUpdater : new boolean[]{true,false}) {
    JavaSparkContext sc=getContext(nWorkers);
    try {
      int[] seeds={1,2,3};
      ComputationGraph net=new ComputationGraph(getGraphConf(12345,Updater.SGD));
      net.init();
      INDArray initialParams=net.params().dup();
      for (int i=0; i < seeds.length; i++) {
        DataSet ds=getOneDataSet(miniBatchSizePerWorker * nWorkers,seeds[i]);
        if (!saveUpdater)         net.setUpdater(null);
        net.fit(ds);
      }
      INDArray finalParams=net.params().dup();
      TrainingMaster tm=getTrainingMaster(1,miniBatchSizePerWorker,nWorkers,saveUpdater);
      SparkComputationGraph sparkNet=new SparkComputationGraph(sc,getGraphConf(12345,Updater.SGD),tm);
      sparkNet.setCollectTrainingStats(true);
      INDArray initialSparkParams=sparkNet.getNetwork().params().dup();
      for (int i=0; i < seeds.length; i++) {
        List<DataSet> list=getOneDataSetAsIndividalExamples(miniBatchSizePerWorker * nWorkers,seeds[i]);
        JavaRDD<DataSet> rdd=sc.parallelize(list);
        sparkNet.fit(rdd);
      }
      System.out.println(sparkNet.getSparkTrainingStats().statsAsString());
      INDArray finalSparkParams=sparkNet.getNetwork().params().dup();
      assertEquals(initialParams,initialSparkParams);
      assertNotEquals(initialParams,finalParams);
      assertEquals(finalParams,finalSparkParams);
      double sparkScore=sparkNet.getScore();
      assertTrue(sparkScore > 0.0);
      assertEquals(net.score(),sparkScore,1e-3);
    }
  finally {
      sc.stop();
    }
  }
}
