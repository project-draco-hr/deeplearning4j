{
  int miniBatchSize=10;
  int nWorkers=1;
  JavaSparkContext sc=getContext(nWorkers);
  try {
    ParameterAveragingTrainingMaster tm=new ParameterAveragingTrainingMaster.Builder(nWorkers).averagingFrequency(1).batchSizePerWorker(miniBatchSize).saveUpdater(true).workerPrefetchNumBatches(0).build();
    int[] seeds={1,2,3};
    MultiLayerNetwork net=new MultiLayerNetwork(getConf(12345,Updater.RMSPROP));
    net.init();
    INDArray initialParams=net.params().dup();
    for (int i=0; i < seeds.length; i++) {
      DataSet ds=getOneDataSet(miniBatchSize,seeds[i]);
      net.fit(ds);
    }
    INDArray finalParams=net.params().dup();
    SparkDl4jMultiLayer sparkNet=new SparkDl4jMultiLayer(sc,getConf(12345,Updater.RMSPROP),tm);
    sparkNet.setCollectTrainingStats(true);
    INDArray initialSparkParams=sparkNet.getNetwork().params().dup();
    for (int i=0; i < seeds.length; i++) {
      List<DataSet> list=getOneDataSetAsIndividalExamples(miniBatchSize,seeds[i]);
      JavaRDD<DataSet> rdd=sc.parallelize(list);
      sparkNet.fit(rdd);
    }
    INDArray finalSparkParams=sparkNet.getNetwork().params().dup();
    assertEquals(initialParams,initialSparkParams);
    assertNotEquals(initialParams,finalParams);
    assertEquals(finalParams,finalSparkParams);
  }
  finally {
    sc.stop();
  }
}
