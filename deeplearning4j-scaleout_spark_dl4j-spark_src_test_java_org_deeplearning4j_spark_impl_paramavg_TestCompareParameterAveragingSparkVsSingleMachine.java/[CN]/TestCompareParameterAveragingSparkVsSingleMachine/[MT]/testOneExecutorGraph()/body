{
  int miniBatchSize=10;
  int nWorkers=1;
  JavaSparkContext sc=getContext(nWorkers);
  try {
    int[] seeds={1,2,3};
    ComputationGraph net=new ComputationGraph(getGraphConf(12345,Updater.RMSPROP));
    net.init();
    INDArray initialParams=net.params().dup();
    for (int i=0; i < seeds.length; i++) {
      DataSet ds=getOneDataSet(miniBatchSize,seeds[i]);
      net.fit(ds);
    }
    INDArray finalParams=net.params().dup();
    TrainingMaster tm=getTrainingMaster(1,miniBatchSize,nWorkers);
    SparkComputationGraph sparkNet=new SparkComputationGraph(sc,getGraphConf(12345,Updater.RMSPROP),tm);
    sparkNet.setCollectTrainingStats(true);
    INDArray initialSparkParams=sparkNet.getNetwork().params().dup();
    for (int i=0; i < seeds.length; i++) {
      List<DataSet> list=getOneDataSetAsIndividalExamples(miniBatchSize,seeds[i]);
      JavaRDD<DataSet> rdd=sc.parallelize(list);
      sparkNet.fit(rdd);
    }
    INDArray finalSparkParams=sparkNet.getNetwork().params().dup();
    assertEquals(initialParams,initialSparkParams);
    assertNotEquals(initialParams,finalParams);
    assertEquals(finalParams,finalSparkParams);
  }
  finally {
    sc.stop();
  }
}
