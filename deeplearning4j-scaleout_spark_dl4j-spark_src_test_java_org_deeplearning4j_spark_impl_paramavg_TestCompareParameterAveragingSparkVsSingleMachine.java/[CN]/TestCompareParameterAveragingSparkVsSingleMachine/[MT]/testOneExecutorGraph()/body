{
  int miniBatchSize=10;
  int nWorkers=1;
  for (  boolean saveUpdater : new boolean[]{true,false}) {
    JavaSparkContext sc=getContext(nWorkers);
    try {
      int[] seeds={1,2,3};
      ComputationGraph net=new ComputationGraph(getGraphConf(12345,Updater.RMSPROP));
      net.init();
      INDArray initialParams=net.params().dup();
      for (int i=0; i < seeds.length; i++) {
        DataSet ds=getOneDataSet(miniBatchSize,seeds[i]);
        if (!saveUpdater)         net.setUpdater(null);
        net.fit(ds);
      }
      INDArray finalParams=net.params().dup();
      TrainingMaster tm=getTrainingMaster(1,miniBatchSize,nWorkers,saveUpdater);
      SparkComputationGraph sparkNet=new SparkComputationGraph(sc,getGraphConf(12345,Updater.RMSPROP),tm);
      sparkNet.setCollectTrainingStats(true);
      INDArray initialSparkParams=sparkNet.getNetwork().params().dup();
      for (int i=0; i < seeds.length; i++) {
        List<DataSet> list=getOneDataSetAsIndividalExamples(miniBatchSize,seeds[i]);
        JavaRDD<DataSet> rdd=sc.parallelize(list);
        sparkNet.fit(rdd);
      }
      INDArray finalSparkParams=sparkNet.getNetwork().params().dup();
      assertEquals(initialParams,initialSparkParams);
      assertNotEquals(initialParams,finalParams);
      assertEquals(finalParams,finalSparkParams);
    }
  finally {
      sc.stop();
    }
  }
}
