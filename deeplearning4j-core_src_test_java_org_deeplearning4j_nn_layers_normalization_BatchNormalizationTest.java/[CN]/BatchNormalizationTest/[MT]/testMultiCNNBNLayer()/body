{
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).iterations(1).seed(123).list().layer(0,new ConvolutionLayer.Builder().nIn(1).nOut(6).weightInit(WeightInit.XAVIER).activation("relu").build()).layer(1,new BatchNormalization.Builder().build()).layer(2,new DenseLayer.Builder().nOut(2).build()).layer(3,new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).weightInit(WeightInit.XAVIER).activation("softmax").nIn(2).nOut(10).build()).backprop(true).pretrain(false).setInputType(InputType.convolutionalFlat(28,28,1)).build();
  MultiLayerNetwork network=new MultiLayerNetwork(conf);
  network.init();
  DataSetIterator iter=new MnistDataSetIterator(2,2);
  DataSet next=iter.next();
  network.setInput(next.getFeatureMatrix());
  INDArray activationsActual=network.preOutput(next.getFeatureMatrix());
  assertEquals(10,activationsActual.shape()[1],1e-2);
  network.fit(next);
  INDArray actualGammaParam=network.getLayer(1).getParam(BatchNormalizationParamInitializer.GAMMA);
  INDArray actualBetaParam=network.getLayer(1).getParam(BatchNormalizationParamInitializer.BETA);
  assertTrue(actualGammaParam != null);
  assertTrue(actualBetaParam != null);
}
