{
  Nd4j.getRandom().setSeed(12345);
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).iterations(1).updater(Updater.RMSPROP).seed(12345).list().layer(0,new BatchNormalization.Builder().nIn(10).nOut(10).eps(1e-5).decay(0.95).build()).layer(1,new OutputLayer.Builder(LossFunctions.LossFunction.MSE).weightInit(WeightInit.XAVIER).activation("identity").nIn(10).nOut(10).build()).backprop(true).pretrain(false).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  net.init();
  int minibatch=32;
  List<DataSet> list=new ArrayList<>();
  for (int i=0; i < 200; i++) {
    list.add(new DataSet(Nd4j.rand(minibatch,10),Nd4j.rand(minibatch,10)));
  }
  DataSetIterator iter=new ListDataSetIterator(list);
  INDArray expMean=Nd4j.valueArrayOf(new int[]{1,10},0.5);
  INDArray expVar=Nd4j.valueArrayOf(new int[]{1,10},1 / 12.0);
  for (int i=0; i < 10; i++) {
    iter.reset();
    net.fit(iter);
  }
  INDArray estMean=net.getLayer(0).getParam(BatchNormalizationParamInitializer.GLOBAL_MEAN);
  INDArray estVar=net.getLayer(0).getParam(BatchNormalizationParamInitializer.GLOBAL_VAR);
  float[] fMeanExp=expMean.data().asFloat();
  float[] fMeanAct=estMean.data().asFloat();
  float[] fVarExp=expVar.data().asFloat();
  float[] fVarAct=estVar.data().asFloat();
  assertArrayEquals(fMeanExp,fMeanAct,0.01f);
  assertArrayEquals(fVarExp,fVarAct,0.01f);
}
