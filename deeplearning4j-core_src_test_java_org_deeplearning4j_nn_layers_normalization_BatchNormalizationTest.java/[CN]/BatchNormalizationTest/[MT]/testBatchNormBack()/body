{
  org.deeplearning4j.nn.conf.layers.BatchNormalization bN=new org.deeplearning4j.nn.conf.layers.BatchNormalization.Builder().build();
  NeuralNetConfiguration layerConf=new NeuralNetConfiguration.Builder().iterations(1).layer(bN).build();
  Layer layer=LayerFactories.getFactory(layerConf).create(layerConf);
  INDArray data=Nd4j.create(new double[]{4.,4.,4.,4.,8.,8.,8.,8.,4.,4.,4.,4.,8.,8.,8.,8.,4.,4.,4.,4.,8.,8.,8.,8.,4.,4.,4.,4.,8.,8.,8.,8,2.,2.,2.,2.,4.,4.,4.,4.,2.,2.,2.,2.,4.,4.,4.,4.,2.,2.,2.,2.,4.,4.,4.,4.,2.,2.,2.,2.,4.,4.,4.,4.},new int[]{2,2,4,4});
  INDArray epsilon=Nd4j.create(new double[]{1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.,-1.},new int[]{2,2,4,4});
  INDArray actualActivation=layer.preOutput(data);
  Pair<Gradient,INDArray> actualOut=layer.backpropGradient(epsilon);
  INDArray expectedEpsilon=Nd4j.create(new double[]{1.,1.,1.,1.,.5,.5,.5,.5,1.,1.,1.,1.,.5,.5,.5,.5,1.,1.,1.,1.,.5,.5,.5,.5,1.,1.,1.,1.,.5,.5,.5,.5,-1.,-1.,-1.,-1.,-.5,-.5,-.5,-.5,-1.,-1.,-1.,-1.,-.5,-.5,-.5,-.5,-1.,-1.,-1.,-1.,-.5,-.5,-.5,-.5,-1.,-1.,-1.,-1.,-.5,-.5,-.5,-.5},new int[]{2,2,4,4});
  assertEquals(expectedEpsilon,actualOut.getSecond());
  assertEquals(null,actualOut.getFirst().getGradientFor("W"));
}
