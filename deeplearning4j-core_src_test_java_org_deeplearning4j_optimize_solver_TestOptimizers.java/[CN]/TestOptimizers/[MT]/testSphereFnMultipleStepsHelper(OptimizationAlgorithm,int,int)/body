{
  double[] scores=new double[nOptIter + 1];
  for (int i=0; i <= nOptIter; i++) {
    Random rng=new DefaultRandom(12345L);
    org.nd4j.linalg.api.rng.distribution.Distribution dist=new org.nd4j.linalg.api.rng.distribution.impl.UniformDistribution(rng,-10,10);
    NeuralNetConfiguration conf=new NeuralNetConfiguration.Builder().maxNumLineSearchIterations(maxNumLineSearchIter).iterations(i).learningRate(0.1).layer(new RBM.Builder().nIn(1).nOut(1).updater((oa == OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT ? Updater.SGD : Updater.ADAGRAD)).build()).batchSize(1).build();
    conf.addVariable("x");
    Model m=new SphereFunctionModel(100,dist,conf);
    if (i == 0) {
      m.computeGradientAndScore();
      scores[0]=m.score();
    }
 else {
      ConvexOptimizer opt=getOptimizer(oa,conf,m);
      opt.optimize();
      m.computeGradientAndScore();
      scores[i]=m.score();
      assertTrue(!Double.isNaN(scores[i]) && !Double.isInfinite(scores[i]));
    }
  }
  if (PRINT_OPT_RESULTS) {
    System.out.println("Multiple optimization iterations (" + nOptIter + " opt. iter.) score vs iteration, maxNumLineSearchIter="+ maxNumLineSearchIter+ ": "+ oa);
    System.out.println(Arrays.toString(scores));
  }
  for (int i=1; i < scores.length; i++) {
    assertTrue(scores[i] <= scores[i - 1]);
  }
  assertTrue(scores[scores.length - 1] < 1.0);
}
