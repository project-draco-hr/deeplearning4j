{
  double[] scores=new double[nOptIter + 1];
  for (int i=0; i <= nOptIter; i++) {
    NeuralNetConfiguration conf=new NeuralNetConfiguration.Builder().maxNumLineSearchIterations(maxNumLineSearchIter).iterations(i).miniBatch(false).learningRate(1e-2).layer(new DenseLayer.Builder().nIn(1).nOut(1).updater(Updater.ADAGRAD).build()).build();
    conf.addVariable("W");
    Model m=new RastriginFunctionModel(100,conf);
    int nParams=m.numParams();
    if (i == 0) {
      m.computeGradientAndScore();
      scores[0]=m.score();
    }
 else {
      ConvexOptimizer opt=getOptimizer(oa,conf,m);
      opt.getUpdater().setStateViewArray((Layer)m,Nd4j.createUninitialized(new int[]{1,nParams},'c'),true);
      opt.optimize();
      m.computeGradientAndScore();
      scores[i]=m.score();
      assertTrue(!Double.isNaN(scores[i]) && !Double.isInfinite(scores[i]));
    }
  }
  if (PRINT_OPT_RESULTS) {
    System.out.println("Rastrigin: Multiple optimization iterations (" + nOptIter + " opt. iter.) score vs iteration, maxNumLineSearchIter="+ maxNumLineSearchIter+ ": "+ oa);
    System.out.println(Arrays.toString(scores));
  }
  for (int i=1; i < scores.length; i++) {
    if (i == 1) {
      assertTrue(scores[i] <= scores[i - 1]);
    }
 else {
      assertTrue(scores[i] <= scores[i - 1]);
    }
  }
}
