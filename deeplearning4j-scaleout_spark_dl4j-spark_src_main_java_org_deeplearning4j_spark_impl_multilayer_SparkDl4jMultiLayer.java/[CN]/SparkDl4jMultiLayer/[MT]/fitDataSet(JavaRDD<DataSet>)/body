{
  int batchSize=conf.getConf(0).getBatchSize();
  int iterations=conf.getConf(0).getNumIterations();
  JavaRDD<DataSet> miniBatches=new RDDMiniBatches(batchSize,rdd).miniBatchesJava();
  log.info("Running distributed training averaging each iteration " + averageEachIteration + " with a mini batch size of "+ batchSize+ " and number of iterations "+ iterations);
  if (!averageEachIteration) {
    MultiLayerNetwork network=new MultiLayerNetwork(conf);
    network.init();
    final INDArray params=network.params();
    this.params=sc.broadcast(params);
    accum=sc.accumulator(params,"params",new ParamAccumulator());
    int paramsLength=network.numParams();
    if (params.length() != paramsLength)     throw new IllegalStateException("Number of params " + paramsLength + " was not equal to "+ params.length());
    miniBatches.foreach(new DataSetTrain(accum,this.params,conf.toJson()));
    INDArray newParams=accum.value().divi(miniBatches.count());
    network.setParameters(newParams);
    this.network=network;
  }
 else {
    for (    NeuralNetConfiguration conf : this.conf.getConfs())     conf.setNumIterations(1);
    MultiLayerNetwork network=new MultiLayerNetwork(conf);
    network.init();
    final INDArray params=network.params();
    this.params=sc.broadcast(params);
    for (int i=0; i < iterations; i++) {
      accum=sc.accumulator(params,"params",new ParamAccumulator());
      int paramsLength=network.numParams();
      if (params.length() != paramsLength)       throw new IllegalStateException("Number of params " + paramsLength + " was not equal to "+ params.length());
      miniBatches.foreach(new DataSetTrain(accum,this.params,conf.toJson()));
      this.params=sc.broadcast(accum.value().divi(miniBatches.count()));
    }
    network.setParameters(this.params.value());
    this.network=network;
  }
  return network;
}
