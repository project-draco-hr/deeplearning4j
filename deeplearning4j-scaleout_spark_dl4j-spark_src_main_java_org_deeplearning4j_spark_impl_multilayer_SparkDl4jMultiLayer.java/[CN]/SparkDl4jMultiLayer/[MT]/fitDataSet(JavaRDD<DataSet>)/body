{
  int batchSize=conf.getConf(0).getBatchSize();
  int iterations=conf.getConf(0).getNumIterations();
  JavaRDD<DataSet> miniBatches=new RDDMiniBatches(batchSize,rdd).miniBatchesJava();
  log.info("Running distributed training averaging each iteration " + averageEachIteration + " with a mini batch size of "+ batchSize+ " and number of iterations "+ iterations);
  if (!averageEachIteration) {
    MultiLayerNetwork network=new MultiLayerNetwork(conf);
    network.init();
    final INDArray params=network.params();
    this.params=sc.broadcast(params);
    int paramsLength=network.numParams();
    if (params.length() != paramsLength)     throw new IllegalStateException("Number of params " + paramsLength + " was not equal to "+ params.length());
    INDArray newParams=miniBatches.map(new IterativeReduce(this.params,conf.toJson())).reduce(new Reduce()).divi(miniBatches.count());
    network.setParameters(newParams);
    this.network=network;
  }
 else {
    for (    NeuralNetConfiguration conf : this.conf.getConfs())     conf.setNumIterations(1);
    MultiLayerNetwork network=new MultiLayerNetwork(conf);
    network.init();
    final INDArray params=network.params();
    this.params=sc.broadcast(params);
    for (int i=0; i < iterations; i++) {
      accum=params;
      int paramsLength=network.numParams();
      if (params.length() != paramsLength)       throw new IllegalStateException("Number of params " + paramsLength + " was not equal to "+ params.length());
      INDArray add=miniBatches.map(new IterativeReduce(this.params,conf.toJson())).reduce(new Reduce());
      this.params=sc.broadcast(add.divi(miniBatches.count()));
    }
    network.setParameters(this.params.value());
    this.network=network;
  }
  return network;
}
