{
  log.info("Broadcasting initial parameters of length " + network.numParams());
  this.params=sc.broadcast(network.params());
  int paramsLength=network.numParams();
  boolean accumGrad=sc.getConf().getBoolean(ACCUM_GRADIENT,false);
  if (accumGrad) {
    JavaRDD<Gradient> results=rdd.mapPartitions(new GradientAccumFlatMap(conf.toJson(),this.params),true).cache();
    log.info("Ran iterative reduce...averaging results now.");
    GradientAdder a=new GradientAdder(paramsLength);
    results.foreach(a);
    INDArray accumulatedGradient=a.getAccumulator().value();
    boolean divideGrad=sc.getConf().getBoolean(DIVIDE_ACCUM_GRADIENT,false);
    if (divideGrad)     accumulatedGradient.divi(results.partitions().size());
    log.info("Accumulated parameters");
    log.info("Summed gradients.");
    network.setParameters(network.params().addi(accumulatedGradient));
    log.info("Set parameters");
  }
 else {
    JavaRDD<INDArray> results=rdd.mapPartitions(new IterativeReduceFlatMap(conf.toJson(),this.params,this.best_score_acc),true).cache();
    log.info("Ran iterative reduce... averaging parameters now.");
    Adder a=new Adder(paramsLength);
    results.foreach(a);
    INDArray newParams=a.getAccumulator().value();
    log.info("Accumulated parameters");
    newParams.divi(rdd.partitions().size());
    log.info("Divided by partitions");
    network.setParameters(newParams);
    log.info("Set parameters");
  }
}
