{
  log.info("Broadcasting initial parameters of length " + network.numParams(false));
  int paramsLength=network.numParams(false);
  boolean accumGrad=sc.getConf().getBoolean(ACCUM_GRADIENT,false);
  if (accumGrad) {
    JavaRDD<Tuple2<Gradient,Updater>> results=rdd.mapPartitions(new GradientAccumFlatMap(conf.toJson(),this.params,this.updater),true).cache();
    log.info("Ran iterative reduce...averaging results now.");
    JavaRDD<Gradient> resultsGradient=results.map(new GradientFromTupleFunction());
    GradientAdder a=new GradientAdder(paramsLength);
    resultsGradient.foreach(a);
    INDArray accumulatedGradient=a.getAccumulator().value();
    boolean divideGrad=sc.getConf().getBoolean(DIVIDE_ACCUM_GRADIENT,false);
    if (divideGrad)     accumulatedGradient.divi(results.partitions().size());
    log.info("Accumulated parameters");
    log.info("Summed gradients.");
    network.setParameters(network.params(false).addi(accumulatedGradient));
    log.info("Set parameters");
    log.info("Processing updaters");
    JavaRDD<Updater> resultsUpdater=results.map(new UpdaterFromGradientTupleFunction());
    UpdaterAggregator aggregator=resultsUpdater.aggregate(resultsUpdater.first().getAggregator(false),new UpdaterElementCombiner(),new UpdaterAggregatorCombiner());
    Updater combinedUpdater=aggregator.getUpdater();
    network.setUpdater(combinedUpdater);
    log.info("Set updater");
  }
 else {
    JavaRDD<Tuple2<MultiLayerNetwork,Double>> results=rdd.map(new IterativeReduceFlatMap(network,bestScoreAcc)).persist(org.apache.spark.storage.StorageLevel.MEMORY_AND_DISK_SER());
    JavaRDD<INDArray> resultsParams=results.map(new INDArrayFromTupleFunction());
    log.info("Ran iterative reduce... averaging parameters now.");
    Adder a=new Adder(paramsLength);
    resultsParams.foreach(a);
    INDArray newParams=a.getAccumulator().value();
    log.info("Accumulated parameters");
    newParams.divi(rdd.partitions().size());
    log.info("Divided by partitions");
    network.setParameters(newParams);
    log.info("Set parameters");
    log.info("Processing updaters");
    JavaRDD<Updater> resultsUpdater=results.map(new UpdaterFromTupleFunction());
    JavaDoubleRDD scores=results.mapToDouble(new DoubleFunction<Tuple2<MultiLayerNetwork,Double>>(){
      @Override public double call(      Tuple2<MultiLayerNetwork,Double> t2) throws Exception {
        return t2._2();
      }
    }
);
    lastScore=scores.mean();
    UpdaterAggregator aggregator=resultsUpdater.aggregate(null,new UpdaterElementCombiner(),new UpdaterAggregatorCombiner());
    Updater combinedUpdater=aggregator.getUpdater();
    network.setUpdater(combinedUpdater);
    log.info("Set updater");
  }
}
