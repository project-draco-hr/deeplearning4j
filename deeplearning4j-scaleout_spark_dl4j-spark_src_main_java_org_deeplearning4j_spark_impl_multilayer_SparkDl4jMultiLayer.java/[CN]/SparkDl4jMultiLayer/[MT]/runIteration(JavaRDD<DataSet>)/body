{
  int maxRep=0;
  int paramsLength=network.numParams(false);
  log.info("Broadcasting initial parameters of length " + paramsLength);
  INDArray valToBroadcast=network.params(false);
  this.params=sc.broadcast(valToBroadcast);
  Updater updater=network.getUpdater();
  if (updater == null) {
    network.setUpdater(UpdaterCreator.getUpdater(network));
    log.warn("Unable to propagate null updater");
    updater=network.getUpdater();
  }
  this.updater=sc.broadcast(updater);
  boolean accumGrad=sc.getConf().getBoolean(ACCUM_GRADIENT,false);
  if (accumGrad) {
    JavaRDD<Tuple2<Gradient,Updater>> results=rdd.mapPartitions(new GradientAccumFlatMap(conf.toJson(),this.params,this.updater),true).cache();
    JavaRDD<Gradient> resultsGradient=results.map(new GradientFromTupleFunction());
    log.info("Ran iterative reduce... averaging results now.");
    GradientAdder a=new GradientAdder(paramsLength);
    resultsGradient.foreach(a);
    INDArray accumulatedGradient=a.getAccumulator().value();
    boolean divideGrad=sc.getConf().getBoolean(DIVIDE_ACCUM_GRADIENT,false);
    if (divideGrad) {
      maxRep=results.partitions().size();
      accumulatedGradient.divi(maxRep);
    }
    log.info("Accumulated parameters");
    log.info("Summed gradients.");
    network.setParameters(network.params(false).addi(accumulatedGradient));
    log.info("Set parameters");
    log.info("Processing updaters");
    JavaRDD<Updater> resultsUpdater=results.map(new UpdaterFromGradientTupleFunction());
    UpdaterAggregator aggregator=resultsUpdater.aggregate(resultsUpdater.first().getAggregator(false),new UpdaterElementCombiner(),new UpdaterAggregatorCombiner());
    Updater combinedUpdater=aggregator.getUpdater();
    network.setUpdater(combinedUpdater);
    log.info("Set updater");
  }
 else {
    JavaRDD<Tuple3<INDArray,Updater,Double>> results=rdd.mapPartitions(new IterativeReduceFlatMap(conf.toJson(),this.params,this.updater,this.bestScoreAcc),true).cache();
    JavaRDD<INDArray> resultsParams=results.map(new INDArrayFromTupleFunction());
    log.info("Running iterative reduce and averaging parameters");
    Adder a=new Adder(paramsLength,sc.accumulator(0));
    resultsParams.foreach(a);
    INDArray newParams=a.getAccumulator().value();
    maxRep=a.getCounter().value();
    newParams.divi(maxRep);
    network.setParameters(newParams);
    log.info("Accumulated and set parameters");
    JavaDoubleRDD scores=results.mapToDouble(new DoubleFunction<Tuple3<INDArray,Updater,Double>>(){
      @Override public double call(      Tuple3<INDArray,Updater,Double> t3) throws Exception {
        return t3._3();
      }
    }
);
    lastScore=scores.mean();
    JavaRDD<Updater> resultsUpdater=results.map(new UpdaterFromTupleFunction());
    UpdaterAggregator aggregator=resultsUpdater.aggregate(null,new UpdaterElementCombiner(),new UpdaterAggregatorCombiner());
    Updater combinedUpdater=aggregator.getUpdater();
    network.setUpdater(combinedUpdater);
    log.info("Processed and set updater");
  }
  if (listeners.size() > 0)   invokeListeners(network,iterationsCount.incrementAndGet());
  if (!initDone) {
    initDone=true;
    update(maxRep,0);
  }
}
