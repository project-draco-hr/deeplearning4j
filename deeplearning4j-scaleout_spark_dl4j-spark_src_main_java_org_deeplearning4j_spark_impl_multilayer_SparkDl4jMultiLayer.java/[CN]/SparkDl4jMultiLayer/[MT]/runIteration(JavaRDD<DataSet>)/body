{
  MultiLayerNetwork network=new MultiLayerNetwork(conf);
  network.init();
  final INDArray params=network.params();
  this.params=sc.broadcast(params);
  log.info("Broadcasting initial parameters of length " + params.length());
  int paramsLength=network.numParams();
  if (params.length() != paramsLength)   throw new IllegalStateException("Number of params " + paramsLength + " was not equal to "+ params.length());
  boolean accumGrad=sc.getConf().getBoolean(ACCUM_GRADIENT,false);
  if (accumGrad) {
    JavaRDD<Gradient> results=rdd.mapPartitions(new GradientAccumFlatMap(conf.toJson(),this.params),true).cache();
    log.info("Ran iterative reduce...averaging results now.");
    GradientAdder a=new GradientAdder(params.length());
    results.foreach(a);
    INDArray accumulatedGradient=a.getAccumulator().value();
    log.info("Accumulated parameters");
    log.info("Summed gradients.");
    network.setParameters(network.params().addi(accumulatedGradient));
    log.info("Set parameters");
    this.network=network;
  }
 else {
    JavaRDD<INDArray> results=rdd.mapPartitions(new IterativeReduceFlatMap(conf.toJson(),this.params),true).cache();
    log.info("Ran iterative reduce...averaging results now.");
    Adder a=new Adder(params.length());
    results.foreach(a);
    INDArray newParams=a.getAccumulator().value();
    log.info("Accumulated parameters");
    newParams.divi(rdd.partitions().size());
    log.info("Divided by partitions");
    network.setParameters(newParams);
    log.info("Set parameters");
    this.network=network;
  }
}
