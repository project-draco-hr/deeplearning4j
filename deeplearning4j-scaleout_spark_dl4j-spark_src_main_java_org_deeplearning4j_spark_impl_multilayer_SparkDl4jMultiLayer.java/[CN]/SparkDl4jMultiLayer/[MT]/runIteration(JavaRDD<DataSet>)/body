{
  log.info("Broadcasting initial parameters of length " + network.numParams(false));
  log.info("Broadcasting initial parameters of length " + network.numParams(false));
  INDArray valToBroadcast=network.params(false);
  this.params=sc.broadcast(valToBroadcast);
  Updater updater=network.getUpdater();
  if (updater == null) {
    network.setUpdater(UpdaterCreator.getUpdater(network));
    log.warn("Unable to propagate null updater");
    updater=network.getUpdater();
  }
  this.updater=sc.broadcast(updater);
  int paramsLength=network.numParams(true);
  boolean accumGrad=sc.getConf().getBoolean(ACCUM_GRADIENT,false);
  if (accumGrad) {
    JavaRDD<Tuple2<Gradient,Updater>> results=rdd.mapPartitions(new GradientAccumFlatMap(conf.toJson(),this.params,this.updater),true).cache();
    JavaRDD<Gradient> resultsGradient=results.map(new GradientFromTupleFunction());
    log.info("Ran iterative reduce... averaging results now.");
    GradientAdder a=new GradientAdder(paramsLength);
    resultsGradient.foreach(a);
    INDArray accumulatedGradient=a.getAccumulator().value();
    boolean divideGrad=sc.getConf().getBoolean(DIVIDE_ACCUM_GRADIENT,false);
    if (divideGrad)     accumulatedGradient.divi(results.partitions().size());
    log.info("Accumulated parameters");
    log.info("Summed gradients.");
    network.setParameters(network.params(false).addi(accumulatedGradient));
    log.info("Set parameters");
    log.info("Processing updaters");
    JavaRDD<Updater> resultsUpdater=results.map(new UpdaterFromGradientTupleFunction());
    UpdaterAggregator aggregator=resultsUpdater.aggregate(resultsUpdater.first().getAggregator(false),new UpdaterElementCombiner(),new UpdaterAggregatorCombiner());
    Updater combinedUpdater=aggregator.getUpdater();
    network.setUpdater(combinedUpdater);
    log.info("Set updater");
  }
 else {
    JavaRDD<Tuple3<INDArray,Updater,Double>> results=rdd.mapPartitions(new IterativeReduceFlatMap(network,this.bestScoreAcc),true).cache();
    JavaRDD<INDArray> resultsParams=results.map(new INDArrayFromTupleFunction());
    log.info("Running iterative reduce and averaging parameters");
    Adder a=new Adder(paramsLength);
    resultsParams.foreach(a);
    INDArray newParams=a.getAccumulator().value();
    log.info("Accumulated parameters");
    newParams.divi(rdd.partitions().size());
    log.info("Divided by partitions");
    network.setParameters(newParams);
    log.info("Set parameters");
    log.info("Processing updaters");
    JavaRDD<Updater> resultsUpdater=results.map(new UpdaterFromTupleFunction());
    JavaDoubleRDD scores=results.mapToDouble(new DoubleFunction<Tuple3<INDArray,Updater,Double>>(){
      @Override public double call(      Tuple3<INDArray,Updater,Double> t3) throws Exception {
        return t3._3();
      }
    }
);
    List<Double> s=scores.collect();
    lastScore=scores.mean();
    UpdaterAggregator aggregator=resultsUpdater.aggregate(null,new UpdaterElementCombiner(),new UpdaterAggregatorCombiner());
    Updater combinedUpdater=aggregator.getUpdater();
    network.setUpdater(combinedUpdater);
    log.info("Set updater");
  }
}
