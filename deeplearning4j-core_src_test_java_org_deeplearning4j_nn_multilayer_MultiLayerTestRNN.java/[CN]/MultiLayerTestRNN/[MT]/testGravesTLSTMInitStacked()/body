{
  int nIn=8;
  int nOut=25;
  int[] nHiddenUnits={17,19,23};
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().list().layer(0,new org.deeplearning4j.nn.conf.layers.GravesLSTM.Builder().nIn(nIn).nOut(17).weightInit(WeightInit.DISTRIBUTION).activation("tanh").build()).layer(1,new org.deeplearning4j.nn.conf.layers.GravesLSTM.Builder().nIn(17).nOut(19).weightInit(WeightInit.DISTRIBUTION).activation("tanh").build()).layer(2,new org.deeplearning4j.nn.conf.layers.GravesLSTM.Builder().nIn(19).nOut(23).weightInit(WeightInit.DISTRIBUTION).activation("tanh").build()).layer(3,new RnnOutputLayer.Builder(LossFunctions.LossFunction.SQUARED_LOSS).nIn(23).nOut(nOut).weightInit(WeightInit.DISTRIBUTION).activation("tanh").build()).build();
  MultiLayerNetwork network=new MultiLayerNetwork(conf);
  network.init();
  for (int i=0; i < nHiddenUnits.length; i++) {
    Layer layer=network.getLayer(i);
    assertTrue(layer instanceof GravesLSTM);
    Map<String,INDArray> paramTable=layer.paramTable();
    assertTrue(paramTable.size() == 3);
    int layerNIn=(i == 0 ? nIn : nHiddenUnits[i - 1]);
    INDArray recurrentWeights=paramTable.get(GravesLSTMParamInitializer.RECURRENT_WEIGHT_KEY);
    assertArrayEquals(recurrentWeights.shape(),new int[]{nHiddenUnits[i],4 * nHiddenUnits[i] + 3});
    INDArray inputWeights=paramTable.get(GravesLSTMParamInitializer.INPUT_WEIGHT_KEY);
    assertArrayEquals(inputWeights.shape(),new int[]{layerNIn,4 * nHiddenUnits[i]});
    INDArray biases=paramTable.get(GravesLSTMParamInitializer.BIAS_KEY);
    assertArrayEquals(biases.shape(),new int[]{1,4 * nHiddenUnits[i]});
    INDArray forgetGateBiases=biases.get(NDArrayIndex.point(0),NDArrayIndex.interval(nHiddenUnits[i],2 * nHiddenUnits[i]));
    assertEquals(nHiddenUnits[i],(int)forgetGateBiases.gt(0).sum(Integer.MAX_VALUE).getDouble(0));
    int nParams=recurrentWeights.length() + inputWeights.length() + biases.length();
    assertTrue(nParams == layer.numParams());
  }
}
