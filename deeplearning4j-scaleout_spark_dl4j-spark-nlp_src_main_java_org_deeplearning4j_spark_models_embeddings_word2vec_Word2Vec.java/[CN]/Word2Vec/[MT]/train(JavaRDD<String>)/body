{
  log.info("Start training ...");
  final JavaSparkContext sc=new JavaSparkContext(corpusRDD.context());
  Map<String,Object> tokenizerVarMap=getTokenizerVarMap();
  Map<String,Object> word2vecVarMap=getWord2vecVarMap();
  final JavaRDD<AtomicLong> sentenceWordsCountRDD;
  final JavaRDD<List<VocabWord>> vocabWordListRDD;
  final JavaPairRDD<List<VocabWord>,Long> vocabWordListSentenceCumSumRDD;
  final VocabCache<VocabWord> vocabCache;
  final JavaRDD<Long> sentenceCumSumCountRDD;
  int maxRep=1;
  log.info("Tokenization and building VocabCache ...");
  Broadcast<Map<String,Object>> broadcastTokenizerVarMap=sc.broadcast(tokenizerVarMap);
  TextPipeline pipeline=new TextPipeline(corpusRDD,broadcastTokenizerVarMap);
  pipeline.buildVocabCache();
  pipeline.buildVocabWordListRDD();
  word2vecVarMap.put("totalWordCount",pipeline.getTotalWordCount());
  sentenceWordsCountRDD=pipeline.getSentenceCountRDD();
  vocabWordListRDD=pipeline.getVocabWordListRDD();
  Broadcast<VocabCache<VocabWord>> vocabCacheBroadcast=pipeline.getBroadCastVocabCache();
  vocabCache=vocabCacheBroadcast.getValue();
  log.info("Vocab size: {}",vocabCache.numWords());
  log.info("Building Huffman Tree ...");
  log.info("Calculating cumulative sum of sentence counts ...");
  sentenceCumSumCountRDD=new CountCumSum(sentenceWordsCountRDD).buildCumSum();
  log.info("Mapping to RDD(vocabWordList, cumulative sentence count) ...");
  vocabWordListSentenceCumSumRDD=vocabWordListRDD.zip(sentenceCumSumCountRDD).setName("vocabWordListSentenceCumSumRDD").cache();
  log.info("Broadcasting word2vec variables to workers ...");
  Broadcast<Map<String,Object>> word2vecVarMapBroadcast=sc.broadcast(word2vecVarMap);
  Broadcast<double[]> expTableBroadcast=sc.broadcast(expTable);
  log.info("Training word2vec sentences ...");
  FlatMapFunction firstIterFunc=new FirstIterationFunction(word2vecVarMapBroadcast,expTableBroadcast,vocabCacheBroadcast);
  @SuppressWarnings("unchecked") JavaRDD<Pair<VocabWord,INDArray>> indexSyn0UpdateEntryRDD=vocabWordListSentenceCumSumRDD.mapPartitions(firstIterFunc).map(new MapToPairFunction());
  List<Pair<VocabWord,INDArray>> syn0UpdateEntries=indexSyn0UpdateEntryRDD.collect();
  INDArray syn0=Nd4j.zeros(vocabCache.numWords(),layerSize);
  Map<VocabWord,AtomicInteger> updates=new HashMap<>();
  Map<Long,Long> updaters=new HashMap<>();
  for (  Pair<VocabWord,INDArray> syn0UpdateEntry : syn0UpdateEntries) {
    syn0.getRow(syn0UpdateEntry.getFirst().getIndex()).addi(syn0UpdateEntry.getSecond());
    if (updates.containsKey(syn0UpdateEntry.getFirst())) {
      updates.get(syn0UpdateEntry.getFirst()).incrementAndGet();
    }
 else     updates.put(syn0UpdateEntry.getFirst(),new AtomicInteger(1));
    if (!updaters.containsKey(syn0UpdateEntry.getFirst().getVocabId())) {
      updaters.put(syn0UpdateEntry.getFirst().getVocabId(),syn0UpdateEntry.getFirst().getAffinityId());
    }
  }
  for (  Map.Entry<VocabWord,AtomicInteger> entry : updates.entrySet()) {
    if (entry.getValue().get() > 1) {
      if (entry.getValue().get() > maxRep)       maxRep=entry.getValue().get();
      syn0.getRow(entry.getKey().getIndex()).divi(entry.getValue().get());
    }
  }
  long totals=0;
  vocab=vocabCache;
  InMemoryLookupTable<VocabWord> inMemoryLookupTable=new InMemoryLookupTable<VocabWord>();
  Environment env=EnvironmentUtils.buildEnvironment();
  env.setNumCores(maxRep);
  env.setAvailableMemory(totals);
  update(env,Event.SPARK);
  inMemoryLookupTable.setVocab(vocabCache);
  inMemoryLookupTable.setVectorLength(layerSize);
  inMemoryLookupTable.setSyn0(syn0);
  lookupTable=inMemoryLookupTable;
  modelUtils.init(lookupTable);
}
