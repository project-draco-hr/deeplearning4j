{
  log.info("Start training ...");
  final JavaSparkContext sc=new JavaSparkContext(corpusRDD.context());
  Map<String,Object> tokenizerVarMap=getTokenizerVarMap();
  Map<String,Object> word2vecVarMap=getWord2vecVarMap();
  final JavaRDD<AtomicLong> sentenceWordsCountRDD;
  final JavaRDD<List<VocabWord>> vocabWordListRDD;
  final JavaPairRDD<List<VocabWord>,Long> vocabWordListSentenceCumSumRDD;
  final VocabCache vocabCache;
  final JavaRDD<Long> sentenceCumSumCountRDD;
  log.info("Tokenization and building VocabCache ...");
  Broadcast<Map<String,Object>> broadcastTokenizerVarMap=sc.broadcast(tokenizerVarMap);
  TextPipeline pipeline=new TextPipeline(corpusRDD,broadcastTokenizerVarMap);
  pipeline.buildVocabCache();
  pipeline.buildVocabWordListRDD();
  word2vecVarMap.put("totalWordCount",pipeline.getTotalWordCount());
  sentenceWordsCountRDD=pipeline.getSentenceCountRDD();
  vocabWordListRDD=pipeline.getVocabWordListRDD();
  Broadcast<VocabCache<VocabWord>> vocabCacheBroadcast=pipeline.getBroadCastVocabCache();
  vocabCache=vocabCacheBroadcast.getValue();
  log.info("Building Huffman Tree ...");
  Huffman huffman=new Huffman(vocabCache.vocabWords());
  huffman.build();
  log.info("Calculating cumulative sum of sentence counts ...");
  sentenceCumSumCountRDD=new CountCumSum(sentenceWordsCountRDD).buildCumSum();
  log.info("Mapping to RDD(vocabWordList, cumulative sentence count) ...");
  vocabWordListSentenceCumSumRDD=vocabWordListRDD.zip(sentenceCumSumCountRDD).setName("vocabWordListSentenceCumSumRDD").cache();
  log.info("Broadcasting word2vec variables to workers ...");
  Broadcast<Map<String,Object>> word2vecVarMapBroadcast=sc.broadcast(word2vecVarMap);
  Broadcast<double[]> expTableBroadcast=sc.broadcast(expTable);
  log.info("Training word2vec sentences ...");
  FlatMapFunction firstIterFunc=new FirstIterationFunction(word2vecVarMapBroadcast,expTableBroadcast);
  @SuppressWarnings("unchecked") JavaRDD<Pair<Integer,INDArray>> indexSyn0UpdateEntryRDD=vocabWordListSentenceCumSumRDD.mapPartitions(firstIterFunc).map(new MapToPairFunction());
  List<Pair<Integer,INDArray>> syn0UpdateEntries=indexSyn0UpdateEntryRDD.collect();
  INDArray syn0=Nd4j.create(vocabCache.numWords(),vectorLength);
  for (  Pair<Integer,INDArray> syn0UpdateEntry : syn0UpdateEntries) {
    syn0.getRow(syn0UpdateEntry.getFirst()).addi(syn0UpdateEntry.getSecond());
  }
  vocab=vocabCache;
  InMemoryLookupTable inMemoryLookupTable=new InMemoryLookupTable();
  inMemoryLookupTable.setVocab(vocabCache);
  inMemoryLookupTable.setVectorLength(vectorLength);
  inMemoryLookupTable.setSyn0(syn0);
  lookupTable=inMemoryLookupTable;
}
