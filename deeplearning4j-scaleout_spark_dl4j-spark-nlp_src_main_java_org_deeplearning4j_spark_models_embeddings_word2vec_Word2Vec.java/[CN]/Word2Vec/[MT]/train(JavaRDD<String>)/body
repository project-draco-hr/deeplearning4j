{
  TextPipeline pipeline=new TextPipeline(rdd);
  Pair<VocabCache,Long> vocabAndNumWords=pipeline.process(tokenizerFactoryClazz);
  SparkConf conf=rdd.context().getConf();
  final JavaSparkContext sc=new JavaSparkContext(rdd.context());
  vocabCacheBroadcast=sc.broadcast(vocabAndNumWords.getFirst());
  final InMemoryLookupTable lookupTable=this.table != null ? table : (InMemoryLookupTable)new InMemoryLookupTable.Builder().cache(vocabAndNumWords.getFirst()).lr(conf.getDouble(Word2VecPerformerVoid.ALPHA,0.025)).vectorLength(conf.getInt(Word2VecPerformerVoid.VECTOR_LENGTH,100)).negative(conf.getDouble(Word2VecPerformerVoid.NEGATIVE,5)).useAdaGrad(conf.getBoolean(Word2VecPerformerVoid.ADAGRAD,false)).build();
  if (this.table == null)   lookupTable.resetWeights();
  Huffman huffman=new Huffman(vocabAndNumWords.getFirst().vocabWords());
  huffman.build();
  log.info("Built huffman tree");
  JavaRDD<Pair<List<VocabWord>,AtomicLong>> r=rdd.map(new TokenizerFunction(tokenizerFactoryClazz)).map(new TokentoVocabWord(vocabCacheBroadcast));
  log.info("Built vocab..");
  final Word2VecParam param=new Word2VecParam.Builder().negative(lookupTable.getNegative()).window(conf.getInt(Word2VecPerformer.WINDOW,5)).expTable(sc.broadcast(lookupTable.getExpTable())).setAlpha(lookupTable.getLr().get()).setMinAlpha(1e-2).setVectorLength(lookupTable.getVectorLength()).useAdaGrad(lookupTable.isUseAdaGrad()).weights(lookupTable).build();
  param.setTotalWords(vocabAndNumWords.getSecond().intValue());
  final List<Long> wordsSeen=new ArrayList<>();
  final JavaRDD<AtomicLong> frequencies=r.map(new Function<Pair<List<VocabWord>,AtomicLong>,AtomicLong>(){
    @Override public AtomicLong call(    Pair<List<VocabWord>,AtomicLong> listAtomicLongPair) throws Exception {
      return listAtomicLongPair.getSecond();
    }
  }
).cache();
  final Accumulator<Double> acc=sc.accumulator(0L);
  frequencies.foreach(new VoidFunction<AtomicLong>(){
    @Override public void call(    AtomicLong atomicLong) throws Exception {
      acc.add((double)atomicLong.get());
      wordsSeen.add((long)(acc.value() + atomicLong.get()));
    }
  }
);
  JavaRDD<List<VocabWord>> words=r.map(new Function<Pair<List<VocabWord>,AtomicLong>,List<VocabWord>>(){
    @Override public List<VocabWord> call(    Pair<List<VocabWord>,AtomicLong> listAtomicLongPair) throws Exception {
      return listAtomicLongPair.getFirst();
    }
  }
);
  JavaPairRDD<List<VocabWord>,Long> wordsAndWordsSeen=words.zipWithIndex().mapToPair(new PairFunction<Tuple2<List<VocabWord>,Long>,List<VocabWord>,Long>(){
    @Override public Tuple2<List<VocabWord>,Long> call(    Tuple2<List<VocabWord>,Long> listLongTuple2) throws Exception {
      return new Tuple2<>(listLongTuple2._1(),wordsSeen.get(listLongTuple2._2().intValue()));
    }
  }
).cache();
  log.info("Calculated word frequencies");
  log.info("Training word 2vec");
  for (int i=0; i < conf.getInt(Word2VecPerformerVoid.ITERATIONS,5); i++) {
    final Broadcast<Word2VecParam> finalParamBroadcast=sc.broadcast(param);
    if (finalParamBroadcast.value() == null)     throw new IllegalStateException("Value not found for param broadcast");
    JavaRDD<Word2VecFuncCall> call=wordsAndWordsSeen.map(new Word2VecSetup(finalParamBroadcast));
    JavaRDD<Word2VecChange> change2=call.map(new SentenceBatch());
    change2.foreach(new VoidFunction<Word2VecChange>(){
      @Override public void call(      Word2VecChange change) throws Exception {
        change.apply(lookupTable);
      }
    }
);
    change2.unpersist();
    change2=null;
    log.info("Iteration " + i);
  }
  return new Pair<VocabCache,WeightLookupTable>(vocabCacheBroadcast.getValue(),lookupTable);
}
