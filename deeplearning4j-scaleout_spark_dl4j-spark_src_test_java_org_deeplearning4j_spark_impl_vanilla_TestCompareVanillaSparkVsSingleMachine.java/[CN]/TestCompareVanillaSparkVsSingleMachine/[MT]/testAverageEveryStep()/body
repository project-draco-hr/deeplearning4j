{
  int miniBatchSizePerWorker=10;
  int nWorkers=4;
  JavaSparkContext sc=getContext(nWorkers);
  try {
    VanillaTrainingMaster tm=new VanillaTrainingMaster.Builder(nWorkers).averagingFrequency(1).batchSizePerWorker(miniBatchSizePerWorker).saveUpdater(true).workerPrefetchNumBatches(0).build();
    int[] seeds={1,2,3};
    MultiLayerNetwork net=new MultiLayerNetwork(getConf(12345,Updater.SGD));
    net.init();
    INDArray initialParams=net.params().dup();
    for (int i=0; i < seeds.length; i++) {
      DataSet ds=getOneDataSet(miniBatchSizePerWorker * nWorkers,seeds[i]);
      net.fit(ds);
    }
    INDArray finalParams=net.params().dup();
    SparkDl4jMultiLayer sparkNet=new SparkDl4jMultiLayer(sc,getConf(12345,Updater.SGD),tm);
    sparkNet.setCollectTrainingStats(true);
    INDArray initialSparkParams=sparkNet.getNetwork().params().dup();
    for (int i=0; i < seeds.length; i++) {
      List<DataSet> list=getOneDataSetAsIndividalExamples(miniBatchSizePerWorker * nWorkers,seeds[i]);
      JavaRDD<DataSet> rdd=sc.parallelize(list);
      sparkNet.fit(rdd);
    }
    System.out.println(sparkNet.getSparkTrainingStats().statsAsString());
    INDArray finalSparkParams=sparkNet.getNetwork().params().dup();
    assertEquals(initialParams,initialSparkParams);
    assertNotEquals(initialParams,finalParams);
    assertEquals(finalParams,finalSparkParams);
    double sparkScore=sparkNet.getScore();
    assertTrue(sparkScore > 0.0);
    assertEquals(net.score(),sparkScore,1e-3);
  }
  finally {
    sc.stop();
  }
}
