{
  if (args.length < 2) {
    System.err.println("Usage: JavaHdfsLR <file> <iters>");
    System.exit(1);
  }
  showWarning();
  SparkConf sparkConf=new SparkConf().setAppName("JavaHdfsLR");
  JavaSparkContext sc=new JavaSparkContext(sparkConf);
  JavaRDD<String> lines=sc.textFile(args[0]);
  JavaRDD<DataPoint> points=lines.map(new ParsePoint()).cache();
  int ITERATIONS=Integer.parseInt(args[1]);
  double[] w=new double[D];
  for (int i=0; i < D; i++) {
    w[i]=2 * rand.nextDouble() - 1;
  }
  System.out.print("Initial w: ");
  printWeights(w);
  for (int i=1; i <= ITERATIONS; i++) {
    System.out.println("On iteration " + i);
    double[] gradient=points.map(new ComputeGradient(w)).reduce(new VectorSum());
    for (int j=0; j < D; j++) {
      w[j]-=gradient[j];
    }
  }
  System.out.print("Final w: ");
  printWeights(w);
  sc.stop();
}
