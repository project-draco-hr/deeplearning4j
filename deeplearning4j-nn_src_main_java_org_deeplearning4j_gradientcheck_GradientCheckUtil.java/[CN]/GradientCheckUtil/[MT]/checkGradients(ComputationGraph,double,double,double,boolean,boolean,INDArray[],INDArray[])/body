{
  if (epsilon <= 0.0 || epsilon > 0.1)   throw new IllegalArgumentException("Invalid epsilon: expect epsilon in range (0,0.1], usually 1e-4 or so");
  if (maxRelError <= 0.0 || maxRelError > 0.25)   throw new IllegalArgumentException("Invalid maxRelativeError: " + maxRelError);
  if (graph.getNumInputArrays() != inputs.length)   throw new IllegalArgumentException("Invalid input arrays: expect " + graph.getNumInputArrays() + " inputs");
  if (graph.getNumOutputArrays() != labels.length)   throw new IllegalArgumentException("Invalid labels arrays: expect " + graph.getNumOutputArrays() + " outputs");
  int layerCount=0;
  for (  String vertexName : graph.getConfiguration().getVertices().keySet()) {
    GraphVertex gv=graph.getConfiguration().getVertices().get(vertexName);
    if (!(gv instanceof LayerVertex))     continue;
    LayerVertex lv=(LayerVertex)gv;
    org.deeplearning4j.nn.conf.Updater u=lv.getLayerConf().getLayer().getUpdater();
    if (u == org.deeplearning4j.nn.conf.Updater.SGD) {
      double lr=lv.getLayerConf().getLayer().getLearningRate();
      if (lr != 1.0) {
        throw new IllegalStateException("When using SGD updater, must also use lr=1.0 for layer \"" + vertexName + "\"; got "+ u);
      }
    }
 else     if (u != org.deeplearning4j.nn.conf.Updater.NONE) {
      throw new IllegalStateException("Must have Updater.NONE (or SGD + lr=1.0) for layer \"" + vertexName + "\"; got "+ u);
    }
  }
  for (int i=0; i < inputs.length; i++)   graph.setInput(i,inputs[i]);
  for (int i=0; i < labels.length; i++)   graph.setLabel(i,labels[i]);
  graph.computeGradientAndScore();
  Pair<Gradient,Double> gradAndScore=graph.gradientAndScore();
  ComputationGraphUpdater updater=new ComputationGraphUpdater(graph);
  updater.update(graph,gradAndScore.getFirst(),0,graph.batchSize());
  INDArray gradientToCheck=gradAndScore.getFirst().gradient().dup();
  INDArray originalParams=graph.params().dup();
  int nParams=originalParams.length();
  int totalNFailures=0;
  double maxError=0.0;
  for (int i=0; i < nParams; i++) {
    INDArray params=originalParams.dup();
    params.putScalar(i,params.getDouble(i) + epsilon);
    graph.setParams(params);
    graph.computeGradientAndScore();
    double scorePlus=graph.score();
    params.putScalar(i,params.getDouble(i) - 2 * epsilon);
    graph.setParams(params);
    graph.computeGradientAndScore();
    double scoreMinus=graph.score();
    double scoreDelta=scorePlus - scoreMinus;
    double numericalGradient=scoreDelta / (2 * epsilon);
    if (Double.isNaN(numericalGradient))     throw new IllegalStateException("Numerical gradient was NaN for parameter " + i + " of "+ nParams);
    double backpropGradient=gradientToCheck.getDouble(i);
    double relError=Math.abs(backpropGradient - numericalGradient) / (Math.abs(numericalGradient) + Math.abs(backpropGradient));
    if (backpropGradient == 0.0 && numericalGradient == 0.0)     relError=0.0;
    if (relError > maxError)     maxError=relError;
    if (relError > maxRelError || Double.isNaN(relError)) {
      double absError=Math.abs(backpropGradient - numericalGradient);
      if (absError < minAbsoluteError) {
        log.info("Param " + i + " passed: grad= "+ backpropGradient+ ", numericalGrad= "+ numericalGradient+ ", relError= "+ relError+ "; absolute error = "+ absError+ " < minAbsoluteError = "+ minAbsoluteError);
      }
 else {
        if (print)         log.info("Param " + i + " FAILED: grad= "+ backpropGradient+ ", numericalGrad= "+ numericalGradient+ ", relError= "+ relError+ ", scorePlus="+ scorePlus+ ", scoreMinus= "+ scoreMinus);
        if (exitOnFirstError)         return false;
        totalNFailures++;
      }
    }
 else     if (print) {
      log.info("Param " + i + " passed: grad= "+ backpropGradient+ ", numericalGrad= "+ numericalGradient+ ", relError= "+ relError);
    }
  }
  if (print) {
    int nPass=nParams - totalNFailures;
    log.info("GradientCheckUtil.checkGradients(): " + nParams + " params checked, "+ nPass+ " passed, "+ totalNFailures+ " failed. Largest relative error = "+ maxError);
  }
  return totalNFailures == 0;
}
