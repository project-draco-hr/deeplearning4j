{
  if (epsilon <= 0.0 || epsilon > 0.1)   throw new IllegalArgumentException("Invalid epsilon: expect epsilon in range (0,0.1], usually 1e-4 or so");
  if (maxRelError <= 0.0 || maxRelError > 0.25)   throw new IllegalArgumentException("Invalid maxRelativeError: " + maxRelError);
  if (!(mln.getOutputLayer() instanceof BaseOutputLayer))   throw new IllegalArgumentException("Cannot check backprop gradients without OutputLayer");
  int layerCount=0;
  for (  NeuralNetConfiguration n : mln.getLayerWiseConfigurations().getConfs()) {
    org.deeplearning4j.nn.conf.Updater u=n.getLayer().getUpdater();
    if (u == org.deeplearning4j.nn.conf.Updater.SGD) {
      double lr=n.getLayer().getLearningRate();
      if (lr != 1.0) {
        throw new IllegalStateException("When using SGD updater, must also use lr=1.0 for layer " + layerCount + "; got "+ u);
      }
    }
 else     if (u != org.deeplearning4j.nn.conf.Updater.NONE) {
      throw new IllegalStateException("Must have Updater.NONE (or SGD + lr=1.0) for layer " + layerCount + "; got "+ u);
    }
  }
  mln.setInput(input);
  mln.setLabels(labels);
  mln.computeGradientAndScore();
  Pair<Gradient,Double> gradAndScore=mln.gradientAndScore();
  Updater updater=UpdaterCreator.getUpdater(mln);
  updater.update(mln,gradAndScore.getFirst(),0,mln.batchSize());
  INDArray gradientToCheck=gradAndScore.getFirst().gradient().dup();
  INDArray originalParams=mln.params().dup();
  int nParams=originalParams.length();
  int totalNFailures=0;
  double maxError=0.0;
  for (int i=0; i < nParams; i++) {
    INDArray params=originalParams.dup();
    params.putScalar(i,params.getDouble(i) + epsilon);
    mln.setParameters(params);
    mln.computeGradientAndScore();
    double scorePlus=mln.score();
    params.putScalar(i,params.getDouble(i) - 2 * epsilon);
    mln.setParameters(params);
    mln.computeGradientAndScore();
    double scoreMinus=mln.score();
    double scoreDelta=scorePlus - scoreMinus;
    double numericalGradient=scoreDelta / (2 * epsilon);
    if (Double.isNaN(numericalGradient))     throw new IllegalStateException("Numerical gradient was NaN for parameter " + i + " of "+ nParams);
    double backpropGradient=gradientToCheck.getDouble(i);
    double relError=Math.abs(backpropGradient - numericalGradient) / (Math.abs(numericalGradient) + Math.abs(backpropGradient));
    if (backpropGradient == 0.0 && numericalGradient == 0.0)     relError=0.0;
    if (relError > maxError)     maxError=relError;
    if (relError > maxRelError || Double.isNaN(relError)) {
      double absError=Math.abs(backpropGradient - numericalGradient);
      if (absError < minAbsoluteError) {
        log.info("Param " + i + " passed: grad= "+ backpropGradient+ ", numericalGrad= "+ numericalGradient+ ", relError= "+ relError+ "; absolute error = "+ absError+ " < minAbsoluteError = "+ minAbsoluteError);
      }
 else {
        if (print)         log.info("Param " + i + " FAILED: grad= "+ backpropGradient+ ", numericalGrad= "+ numericalGradient+ ", relError= "+ relError+ ", scorePlus="+ scorePlus+ ", scoreMinus= "+ scoreMinus);
        if (exitOnFirstError)         return false;
        totalNFailures++;
      }
    }
 else     if (print) {
      log.info("Param " + i + " passed: grad= "+ backpropGradient+ ", numericalGrad= "+ numericalGradient+ ", relError= "+ relError);
    }
  }
  if (print) {
    int nPass=nParams - totalNFailures;
    log.info("GradientCheckUtil.checkGradients(): " + nParams + " params checked, "+ nPass+ " passed, "+ totalNFailures+ " failed. Largest relative error = "+ maxError);
  }
  return totalNFailures == 0;
}
