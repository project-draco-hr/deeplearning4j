{
  Nd4j.getRandom().setSeed(12345);
  int nIn=5;
  int nOut=6;
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).regularization(true).l1(0.01).l2(0.01).learningRate(0.1).activation("tanh").weightInit(WeightInit.XAVIER).list().layer(0,new DenseLayer.Builder().nIn(nIn).nOut(20).build()).layer(1,new DenseLayer.Builder().nIn(20).nOut(30).build()).layer(2,new OutputLayer.Builder().lossFunction(LossFunctions.LossFunction.MSE).nIn(30).nOut(nOut).build()).build();
  MultiLayerConfiguration confNoReg=new NeuralNetConfiguration.Builder().seed(12345).regularization(false).learningRate(0.1).activation("tanh").weightInit(WeightInit.XAVIER).list().layer(0,new DenseLayer.Builder().nIn(nIn).nOut(20).build()).layer(1,new DenseLayer.Builder().nIn(20).nOut(30).build()).layer(2,new OutputLayer.Builder().lossFunction(LossFunctions.LossFunction.MSE).nIn(30).nOut(nOut).build()).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  net.init();
  MultiLayerNetwork netNoReg=new MultiLayerNetwork(confNoReg);
  netNoReg.init();
  netNoReg.setParameters(net.params().dup());
  INDArray input=Nd4j.rand(3,nIn);
  INDArray output=Nd4j.rand(3,nOut);
  DataSet ds=new DataSet(input,output);
  INDArray scoresWithRegularization=net.scoreExamples(ds,true);
  INDArray scoresNoRegularization=net.scoreExamples(ds,false);
  assertArrayEquals(new int[]{3,1},scoresWithRegularization.shape());
  assertArrayEquals(new int[]{3,1},scoresNoRegularization.shape());
  for (int i=0; i < 3; i++) {
    DataSet singleEx=new DataSet(input.getRow(i),output.getRow(i));
    double score=net.score(singleEx);
    double scoreNoReg=netNoReg.score(singleEx);
    double scoreUsingScoreExamples=scoresWithRegularization.getDouble(i);
    double scoreUsingScoreExamplesNoReg=scoresNoRegularization.getDouble(i);
    assertEquals(score,scoreUsingScoreExamples,1e-4);
    assertEquals(scoreNoReg,scoreUsingScoreExamplesNoReg,1e-4);
    assertTrue(scoreUsingScoreExamples > scoreUsingScoreExamplesNoReg);
  }
}
