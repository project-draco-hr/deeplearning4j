{
  Nd4j.getRandom().setSeed(123);
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.LINE_GRADIENT_DESCENT).iterations(5).weightInit(WeightInit.XAVIER).seed(123).activationFunction("tanh").nIn(4).nOut(3).layer(new org.deeplearning4j.nn.conf.layers.OutputLayer()).list(3).backprop(true).pretrain(false).hiddenLayerSizes(3,2).override(2,new ConfOverride(){
    @Override public void overrideLayer(    int i,    NeuralNetConfiguration.Builder builder){
      builder.activationFunction("softmax");
      builder.layer(new org.deeplearning4j.nn.conf.layers.OutputLayer());
      builder.lossFunction(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD);
    }
  }
).build();
  MultiLayerNetwork network=new MultiLayerNetwork(conf);
  network.init();
  network.setListeners(Lists.<IterationListener>newArrayList(new ScoreIterationListener(1)));
  DataSetIterator iter=new IrisDataSetIterator(150,150);
  DataSet next=iter.next();
  next.normalizeZeroMeanZeroUnitVariance();
  SplitTestAndTrain trainTest=next.splitTestAndTrain(110);
  network.setInput(trainTest.getTrain().getFeatureMatrix());
  network.setLabels(trainTest.getTrain().getLabels());
  network.init();
  network.fit(trainTest.getTrain());
  DataSet test=trainTest.getTest();
  Evaluation eval=new Evaluation();
  INDArray output=network.output(test.getFeatureMatrix());
  eval.eval(test.getLabels(),output);
  log.info("Score " + eval.stats());
}
