{
  Nd4j.getRandom().setSeed(123);
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.LINE_GRADIENT_DESCENT).iterations(5).seed(123).list(4).layer(0,new DenseLayer.Builder().nIn(4).nOut(3).weightInit(WeightInit.XAVIER).activation("tanh").build()).layer(1,new DenseLayer.Builder().nIn(3).nOut(2).weightInit(WeightInit.XAVIER).activation("tanh").build()).layer(2,BatchNormalization.builder().shape(new int[]{1,2}).build()).layer(3,new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).weightInit(WeightInit.XAVIER).activation("softmax").nIn(2).nOut(3).build()).backprop(true).pretrain(false).inputPreProcessor(2,new FeedForwardToCnnPreProcessor(1,2,1)).inputPreProcessor(3,new CnnToFeedForwardPreProcessor(1,2,1)).build();
  MultiLayerNetwork network=new MultiLayerNetwork(conf);
  network.init();
  network.setListeners(new ScoreIterationListener(1));
  DataSetIterator iter=new IrisDataSetIterator(150,150);
  DataSet next=iter.next();
  next.normalizeZeroMeanZeroUnitVariance();
  SplitTestAndTrain trainTest=next.splitTestAndTrain(110);
  network.setLabels(trainTest.getTrain().getLabels());
  network.init();
  network.setInput(trainTest.getTrain().getFeatureMatrix());
  network.setLabels(trainTest.getTrain().getLabels());
  network.computeGradientAndScore();
  INDArray grad=network.gradient().gradient();
  assertTrue("Gradient contains invalid numbers",!Nd4j.hasInvalidNumber(grad));
  int numParams=network.numParams();
  network.fit(trainTest.getTrain());
}
