{
  DataSetIterator iter=new IrisDataSetIterator(1,1);
  Gradient expectedGradient=new DefaultGradient();
  expectedGradient.setGradientFor("0_W",Nd4j.ones(4,5));
  expectedGradient.setGradientFor("0_b",Nd4j.ones(1,5));
  expectedGradient.setGradientFor("1_W",Nd4j.ones(5,3));
  expectedGradient.setGradientFor("1_b",Nd4j.ones(1,3));
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().updater(org.deeplearning4j.nn.conf.Updater.SGD).learningRate(1).activation("relu").weightInit(WeightInit.XAVIER).list().layer(0,new DenseLayer.Builder().name("dnn1").nIn(4).nOut(5).build()).layer(1,new OutputLayer.Builder().name("output").nIn(5).nOut(3).activation("softmax").weightInit(WeightInit.XAVIER).build()).backprop(true).pretrain(false).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  net.init();
  net.fit(iter.next());
  Gradient actualGradient=net.gradient();
  assertNotEquals(expectedGradient.gradient(),actualGradient.gradient());
  net.update(expectedGradient);
  actualGradient=net.gradient();
  assertEquals(expectedGradient.gradient(),actualGradient.gradient());
  net.setParam("0_W",Nd4j.ones(4,5));
  net.setParam("0_b",Nd4j.ones(1,5));
  net.setParam("1_W",Nd4j.ones(5,3));
  net.setParam("1_b",Nd4j.ones(1,3));
  INDArray actualParams=net.params();
  assertEquals(expectedGradient.gradient(),actualParams);
  net.update(expectedGradient);
  actualParams=net.params();
  assertEquals(Nd4j.ones(1,43).addi(1),actualParams);
}
