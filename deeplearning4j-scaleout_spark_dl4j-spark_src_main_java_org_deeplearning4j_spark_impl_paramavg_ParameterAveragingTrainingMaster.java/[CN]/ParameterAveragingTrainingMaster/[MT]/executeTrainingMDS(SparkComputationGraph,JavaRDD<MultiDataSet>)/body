{
  if (collectTrainingStats)   stats.logFitStart();
  trainingData.persist(StorageLevel.MEMORY_ONLY());
  long totalCount=trainingData.count();
  int examplesPerSplit=numWorkers * batchSizePerWorker * averagingFrequency;
  JavaRDD<MultiDataSet>[] splits=randomSplit((int)totalCount,examplesPerSplit,trainingData);
  int splitNum=1;
  for (  JavaRDD<MultiDataSet> split : splits) {
    log.info("Starting graph training of split {} of {}. workerMiniBatchSize={}, averagingFreq={}, dataSetTotalExamples={}. Configured for {} executors",splitNum,splits.length,batchSizePerWorker,averagingFrequency,totalCount,numWorkers);
    FlatMapFunction<Iterator<MultiDataSet>,ParameterAveragingTrainingResult> function=new ExecuteWorkerMultiDataSetFlatMap<>(getWorkerInstance(graph));
    JavaRDD<ParameterAveragingTrainingResult> result=split.mapPartitions(function);
    processResults(null,graph,result,splitNum,splits.length);
    splitNum++;
  }
  if (collectTrainingStats)   stats.logFitEnd();
}
