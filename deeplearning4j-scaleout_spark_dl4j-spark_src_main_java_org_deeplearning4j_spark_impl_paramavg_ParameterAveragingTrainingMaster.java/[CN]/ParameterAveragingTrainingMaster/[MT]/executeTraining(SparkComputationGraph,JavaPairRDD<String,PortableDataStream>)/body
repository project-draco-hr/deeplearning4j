{
  if (numWorkers == null)   numWorkers=graph.getSparkContext().defaultParallelism();
  if (collectTrainingStats)   stats.logFitStart();
  int origNumPartitions=trainingData.partitions().size();
  if (origNumPartitions >= COALESCE_THRESHOLD * numWorkers) {
    log.info("Coalesing streams from {} to {} partitions",origNumPartitions,numWorkers);
    trainingData=trainingData.coalesce(numWorkers);
  }
  if (collectTrainingStats)   stats.logCountStart();
  long totalDataSetObjectCount=trainingData.count();
  if (collectTrainingStats)   stats.logCountEnd();
  int dataSetObjectsPerSplit=getNumDataSetObjectsPerSplit();
  if (collectTrainingStats)   stats.logSplitStart();
  JavaPairRDD<String,PortableDataStream>[] splits=SparkUtils.balancedRandomSplit((int)totalDataSetObjectCount,dataSetObjectsPerSplit,trainingData,new Random().nextLong());
  if (collectTrainingStats)   stats.logSplitEnd();
  int splitNum=1;
  for (  JavaPairRDD<String,PortableDataStream> split : splits) {
    JavaRDD<PortableDataStream> streams=split.values();
    doIterationPDS(null,graph,streams,splitNum++,splits.length);
  }
  if (collectTrainingStats)   stats.logFitEnd((int)totalDataSetObjectCount);
}
