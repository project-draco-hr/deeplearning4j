{
  if (collectTrainingStats)   stats.logFitStart();
  trainingData.persist(StorageLevel.MEMORY_ONLY());
  long totalDataSetObjectCount=trainingData.count();
  int dataSetObjectsPerSplit=getNumDataSetObjectsPerSplit();
  if (collectTrainingStats)   stats.logSplitStart();
  JavaRDD<DataSet>[] splits=randomSplit((int)totalDataSetObjectCount,dataSetObjectsPerSplit,trainingData);
  if (collectTrainingStats)   stats.logSplitEnd();
  int splitNum=1;
  for (  JavaRDD<DataSet> split : splits) {
    log.info("Starting training of split {} of {}. workerMiniBatchSize={}, averagingFreq={}, dataSetTotalExamples={}. Configured for {} executors",splitNum,splits.length,batchSizePerWorker,averagingFrequency,totalDataSetObjectCount,numWorkers);
    if (collectTrainingStats)     stats.logMapPartitionsStart();
    JavaRDD<DataSet> splitData=split;
    splitData=repartitionIfRequired(splitData);
    int nPartitions=split.partitions().size();
    FlatMapFunction<Iterator<DataSet>,ParameterAveragingTrainingResult> function=new ExecuteWorkerFlatMap<>(getWorkerInstance(network));
    JavaRDD<ParameterAveragingTrainingResult> result=splitData.mapPartitions(function);
    processResults(network,null,result,splitNum,splits.length);
    splitNum++;
    if (collectTrainingStats)     stats.logMapPartitionsEnd(nPartitions);
  }
  if (collectTrainingStats)   stats.logFitEnd((int)totalDataSetObjectCount);
}
