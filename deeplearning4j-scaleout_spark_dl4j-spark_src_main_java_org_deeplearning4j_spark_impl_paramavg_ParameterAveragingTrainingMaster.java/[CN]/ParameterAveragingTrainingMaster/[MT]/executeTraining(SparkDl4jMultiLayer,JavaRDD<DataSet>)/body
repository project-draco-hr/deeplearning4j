{
  if (collectTrainingStats)   stats.logFitStart();
  trainingData.persist(StorageLevel.MEMORY_ONLY());
  long totalCount=trainingData.count();
  int dataSetObjectsPerSplit;
  if (rddDataSetNumExamples == 1) {
    dataSetObjectsPerSplit=numWorkers * batchSizePerWorker * averagingFrequency;
  }
 else {
    int reqNumExamplesEachWorker=batchSizePerWorker * averagingFrequency;
    int numDataSetsReqEachWorker=reqNumExamplesEachWorker / batchSizePerWorker;
    if (numDataSetsReqEachWorker < 1) {
      numDataSetsReqEachWorker=1;
    }
    dataSetObjectsPerSplit=(int)(totalCount / numDataSetsReqEachWorker);
    if (numDataSetsReqEachWorker >= totalCount)     dataSetObjectsPerSplit=(int)totalCount;
  }
  JavaRDD<DataSet>[] splits;
  if (collectTrainingStats)   stats.logSplitStart();
  if (totalCount <= dataSetObjectsPerSplit) {
    splits=(JavaRDD<DataSet>[])Array.newInstance(JavaRDD.class,1);
    splits[0]=trainingData;
  }
 else {
    int numSplits=(int)(totalCount / dataSetObjectsPerSplit);
    double[] weights=new double[numSplits];
    for (int i=0; i < weights.length; i++)     weights[i]=1.0 / numSplits;
    splits=trainingData.randomSplit(weights);
  }
  if (collectTrainingStats)   stats.logSplitEnd();
  int splitNum=1;
  for (  JavaRDD<DataSet> split : splits) {
    log.info("Starting training of split {} of {}. workerMiniBatchSize={}, averagingFreq={}, dataSetTotalExamples={}. Configured for {} executors",splitNum,splits.length,batchSizePerWorker,averagingFrequency,totalCount,numWorkers);
    if (collectTrainingStats)     stats.logMapPartitionsStart();
    JavaRDD<DataSet> splitData=split;
    int nPartitions=split.partitions().size();
switch (repartition) {
case Never:
      break;
case NumPartitionsExecutorsDiffers:
    if (nPartitions == numWorkers)     break;
case Always:
  if (collectTrainingStats)   stats.logRepartitionStart();
splitData=split.repartition(numWorkers);
if (collectTrainingStats) stats.logRepartitionEnd();
break;
default :
throw new RuntimeException("Unknown setting for repartition: " + repartition);
}
FlatMapFunction<Iterator<DataSet>,ParameterAveragingTrainingResult> function=new ExecuteWorkerFlatMap<>(getWorkerInstance(network));
JavaRDD<ParameterAveragingTrainingResult> result=splitData.mapPartitions(function);
processResults(network,null,result,splitNum,splits.length);
splitNum++;
if (collectTrainingStats) stats.logMapPartitionsEnd(nPartitions);
}
if (collectTrainingStats) stats.logFitEnd((int)totalCount);
}
