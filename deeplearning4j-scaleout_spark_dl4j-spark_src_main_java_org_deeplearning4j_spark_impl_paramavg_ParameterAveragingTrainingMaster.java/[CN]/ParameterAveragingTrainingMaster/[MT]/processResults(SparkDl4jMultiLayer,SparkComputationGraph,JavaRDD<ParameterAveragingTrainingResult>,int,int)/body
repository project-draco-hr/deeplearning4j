{
  if (collectTrainingStats)   stats.logAggregateStartTime();
  ParameterAveragingAggregationTuple tuple=results.aggregate(null,new ParameterAveragingElementAddFunction(),new ParameterAveragingElementCombineFunction());
  INDArray params=tuple.getParametersSum();
  int aggCount=tuple.getAggregationsCount();
  SparkTrainingStats aggregatedStats=tuple.getSparkTrainingStats();
  if (collectTrainingStats)   stats.logAggregationEndTime();
  if (collectTrainingStats)   stats.logProcessParamsUpdaterStart();
  params.divi(aggCount);
  INDArray updaterState=tuple.getUpdaterStateSum();
  if (updaterState != null)   updaterState.divi(aggCount);
  if (network != null) {
    MultiLayerNetwork net=network.getNetwork();
    net.setParameters(params);
    if (updaterState != null)     net.getUpdater().setStateViewArray(null,updaterState,false);
    network.setScore(tuple.getScoreSum() / tuple.getAggregationsCount());
  }
 else {
    ComputationGraph g=graph.getNetwork();
    g.setParams(params);
    if (updaterState != null)     g.getUpdater().setStateViewArray(updaterState);
    graph.setScore(tuple.getScoreSum() / tuple.getAggregationsCount());
  }
  if (collectTrainingStats) {
    stats.logProcessParamsUpdaterEnd();
    stats.addWorkerStats(aggregatedStats);
  }
  log.info("Completed training of split {} of {}",splitNum,totalSplits);
  if (listeners != null) {
    if (network != null) {
      MultiLayerNetwork net=network.getNetwork();
      net.setScore(network.getScore());
      for (      IterationListener il : listeners) {
        il.iterationDone(net,iterationCount);
      }
    }
 else {
      ComputationGraph g=graph.getNetwork();
      g.setScore(graph.getScore());
      for (      IterationListener il : listeners) {
        il.iterationDone(g,iterationCount);
      }
    }
  }
  iterationCount++;
}
