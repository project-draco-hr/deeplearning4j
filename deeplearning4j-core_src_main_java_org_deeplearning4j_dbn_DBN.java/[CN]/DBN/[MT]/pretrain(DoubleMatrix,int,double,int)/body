{
  if (isUseGaussNewtonVectorProductBackProp()) {
    log.warn("WARNING; Gauss newton back vector back propagation is primarily used for hessian free which does not involve pretrain; just finetune. Use this at your own risk");
  }
  if (this.getInput() == null || this.getLayers() == null || this.getLayers()[0] == null || this.getSigmoidLayers() == null || this.getSigmoidLayers()[0] == null) {
    setInput(input);
    initializeLayers(input);
  }
 else   setInput(input);
  DoubleMatrix layerInput=null;
  for (int i=0; i < getnLayers(); i++) {
    if (i == 0)     layerInput=getInput();
 else {
      boolean activateOnly=getSampleOrActivate() != null && getSampleOrActivate().get(i) != null ? getSampleOrActivate().get(i) : !sampleFromHiddenActivations;
      if (useRBMPropUpAsActivations) {
        RBM r=(RBM)layers[i - 1];
        layerInput=r.propUp(layerInput);
        if (sampleFromHiddenActivations)         layerInput=MatrixUtil.binomial(layerInput,1,getRng());
      }
 else       if (activateOnly)       layerInput=getSigmoidLayers()[i - 1].activate(layerInput);
 else       if (isSampleFromHiddenActivations())       layerInput=getLayers()[i - 1].sampleHiddenGivenVisible(getSigmoidLayers()[i - 1].getActivationFunction().apply(layerInput)).getSecond();
 else       layerInput=getLayers()[i - 1].sampleHiddenGivenVisible(layerInput).getSecond();
    }
    log.info("Training on layer " + (i + 1));
    double realLearningRate=layerLearningRates.get(i) != null ? layerLearningRates.get(i) : learningRate;
    if (isForceNumEpochs()) {
      for (int epoch=0; epoch < epochs; epoch++) {
        log.info("Error on epoch " + epoch + " for layer "+ (i + 1)+ " is "+ getLayers()[i].getReConstructionCrossEntropy());
        getLayers()[i].train(layerInput,realLearningRate,new Object[]{k,learningRate});
        getLayers()[i].iterationDone(epoch);
      }
    }
 else     getLayers()[i].trainTillConvergence(layerInput,realLearningRate,new Object[]{k,realLearningRate,epochs});
  }
}
