{
  String[] activFns={"sigmoid","tanh","relu"};
  boolean[] characteristic={false,true};
  LossFunctions.LossFunction[] lossFunctions={LossFunctions.LossFunction.MSE,LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD};
  String[] outputActivations={"softmax"};
  DataSet ds=new IrisDataSetIterator(150,150).next();
  ds.normalizeZeroMeanZeroUnitVariance();
  INDArray input=ds.getFeatureMatrix();
  INDArray labels=ds.getLabels();
  for (  String afn : activFns) {
    for (    boolean doLearningFirst : characteristic) {
      for (int i=0; i < lossFunctions.length; i++) {
        LossFunctions.LossFunction lf=lossFunctions[i];
        String outputActivation=outputActivations[i];
        MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().regularization(false).optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT).learningRate(1.0).seed(12345L).list(2).layer(0,new ConvolutionLayer.Builder(new int[]{1,1}).nIn(1).nOut(6).weightInit(WeightInit.XAVIER).dist(new NormalDistribution(0,1)).activation(afn).updater(Updater.SGD).build()).layer(1,new OutputLayer.Builder(lf).activation(outputActivation).nIn(6).nOut(3).weightInit(WeightInit.XAVIER).dist(new NormalDistribution(0,1)).updater(Updater.SGD).build()).pretrain(false).backprop(true).inputPreProcessor(0,new FeedForwardToCnnPreProcessor(2,2,1)).inputPreProcessor(1,new CnnToFeedForwardPreProcessor()).build();
        MultiLayerNetwork mln=new MultiLayerNetwork(conf);
        mln.init();
        if (doLearningFirst) {
          mln.setInput(ds.getFeatures());
          mln.setLabels(ds.getLabels());
          mln.computeGradientAndScore();
          double scoreBefore=mln.score();
          for (int j=0; j < 10; j++)           mln.fit(ds);
          mln.computeGradientAndScore();
          double scoreAfter=mln.score();
          String msg="testGradMLP2LayerIrisSimple() - score did not (sufficiently) decrease during learning - activationFn=" + afn + ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", doLearningFirst="+ doLearningFirst+ " (before="+ scoreBefore+ ", scoreAfter="+ scoreAfter+ ")";
          assertTrue(msg,scoreAfter < 0.8 * scoreBefore);
        }
        if (PRINT_RESULTS) {
          System.out.println("testGradientMLP2LayerIrisSimpleRandom() - activationFn=" + afn + ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", doLearningFirst="+ doLearningFirst);
          for (int j=0; j < mln.getnLayers(); j++)           System.out.println("Layer " + j + " # params: "+ mln.getLayer(j).numParams());
        }
        boolean gradOK=GradientCheckUtil.checkGradients(mln,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels,true);
        String msg="testGradMLP2LayerIrisSimple() - activationFn=" + afn + ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", doLearningFirst="+ doLearningFirst;
        assertTrue(msg,gradOK);
      }
    }
  }
}
