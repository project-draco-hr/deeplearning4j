{
  if (graph == null) {
  }
  if (network == null) {
    final int numRows=40;
    final int numColumns=40;
    int nChannels=3;
    int outputNum=LFWLoader.NUM_LABELS;
    int numSamples=LFWLoader.NUM_IMAGES;
    boolean useSubset=false;
    int batchSize=200;
    int iterations=5;
    int splitTrainNum=(int)(batchSize * .8);
    int seed=123;
    int listenerFreq=iterations / 5;
    DataSet lfwNext;
    SplitTestAndTrain trainTest;
    DataSet trainInput;
    List<INDArray> testInput=new ArrayList<>();
    List<INDArray> testLabels=new ArrayList<>();
    MultiLayerConfiguration.Builder builder=new NeuralNetConfiguration.Builder().seed(seed).iterations(iterations).activation("relu").weightInit(WeightInit.XAVIER).gradientNormalization(GradientNormalization.RenormalizeL2PerLayer).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).learningRate(0.01).momentum(0.9).regularization(true).updater(Updater.ADAGRAD).useDropConnect(true).list(6).layer(0,new ConvolutionLayer.Builder(4,4).name("cnn1").nIn(nChannels).stride(1,1).nOut(20).build()).layer(1,new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX,new int[]{2,2}).name("pool1").build()).layer(2,new ConvolutionLayer.Builder(3,3).name("cnn2").stride(1,1).nOut(40).build()).layer(3,new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX,new int[]{2,2}).name("pool2").build()).layer(2,new ConvolutionLayer.Builder(3,3).name("cnn3").stride(1,1).nOut(60).build()).layer(3,new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX,new int[]{2,2}).name("pool3").build()).layer(2,new ConvolutionLayer.Builder(2,2).name("cnn3").stride(1,1).nOut(80).build()).layer(4,new DenseLayer.Builder().name("ffn1").nOut(160).dropOut(0.5).build()).layer(5,new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD).nOut(outputNum).activation("softmax").build()).backprop(true).pretrain(false);
    new ConvolutionLayerSetup(builder,numRows,numColumns,nChannels);
    network=new MultiLayerNetwork(builder.build());
    network.init();
  }
}
