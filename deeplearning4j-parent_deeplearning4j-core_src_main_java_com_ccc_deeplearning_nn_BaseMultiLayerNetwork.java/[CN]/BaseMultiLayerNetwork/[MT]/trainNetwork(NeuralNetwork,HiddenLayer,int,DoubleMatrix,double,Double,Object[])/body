{
  DoubleMatrix w=network.getW();
  DoubleMatrix hBias=network.gethBias();
  network.trainTillConvergence(input,lr,params);
  if (network.getRenderEpochs() > 0) {
    if (epoch % network.getRenderEpochs() == 0) {
      NeuralNetPlotter plotter=new NeuralNetPlotter();
      plotter.plot(network);
    }
  }
  h.W=network.getW();
  h.b=network.gethBias();
  double entropy=network.getReConstructionCrossEntropy();
  if (Double.isNaN(entropy) || Double.isInfinite(entropy)) {
    network.setW(w.dup());
    network.sethBias(hBias.dup());
    h.W=network.getW();
    h.b=hBias;
    log.info("Went over too many times....reverting to last known good state");
  }
 else   if (entropy > bestLoss || entropy == bestLoss) {
    network.setW(w.dup());
    network.sethBias(hBias.dup());
    h.W=network.getW();
    h.b=hBias;
    log.info("Went over too many times....reverting to last known good state");
    return false;
  }
 else   if (entropy < bestLoss) {
    w=network.getW().dup();
    hBias=network.gethBias().dup();
    bestLoss=entropy;
  }
  log.info("Cross entropy for layer " + (epoch + 1) + " on epoch "+ epoch+ " is "+ entropy);
  double curr=network.getReConstructionCrossEntropy();
  if (curr > bestLoss) {
    log.info("Converged past global minimum; reverting");
    network.setW(w.dup());
    network.sethBias(hBias.dup());
    return false;
  }
  return true;
}
