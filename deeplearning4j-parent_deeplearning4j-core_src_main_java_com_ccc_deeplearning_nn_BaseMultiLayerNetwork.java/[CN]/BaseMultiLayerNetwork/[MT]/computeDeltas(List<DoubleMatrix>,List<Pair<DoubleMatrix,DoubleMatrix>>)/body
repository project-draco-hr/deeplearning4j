{
  DoubleMatrix[] gradients=new DoubleMatrix[nLayers + 2];
  DoubleMatrix[] deltas=new DoubleMatrix[nLayers + 2];
  ActivationFunction derivative=sigmoidLayers[0].activationFunction;
  DoubleMatrix delta=null;
  List<DoubleMatrix> weights=new ArrayList<>();
  for (int j=0; j < layers.length; j++)   weights.add(layers[j].getW());
  weights.add(logLayer.W);
  List<DoubleMatrix> zs=new ArrayList<>();
  zs.add(input);
  for (int i=0; i < layers.length; i++) {
    zs.add(layers[i].getInput().mmul(weights.get(i)).addRowVector(layers[i].gethBias()));
  }
  zs.add(logLayer.input.mmul(logLayer.W).addRowVector(logLayer.b));
  for (int i=nLayers + 1; i >= 0; i--) {
    if (i >= nLayers + 1) {
      DoubleMatrix z=zs.get(i);
      delta=labels.sub(activations.get(i)).neg();
      DoubleMatrix initialDelta=delta.mul(derivative.applyDerivative(z));
      deltas[i]=initialDelta;
    }
 else {
      delta=deltas[i + 1];
      DoubleMatrix w=weights.get(i).transpose();
      DoubleMatrix z=zs.get(i);
      DoubleMatrix a=activations.get(i + 1);
      DoubleMatrix error=delta.mmul(w);
      deltas[i]=error;
      error=error.mul(derivative.applyDerivative(z));
      deltas[i]=error;
      gradients[i]=a.transpose().mmul(error).transpose();
    }
  }
  if (deltaRet.isEmpty()) {
    for (int i=0; i < gradients.length; i++) {
      deltaRet.add(new Pair<>(gradients[i],deltas[i]));
    }
  }
 else {
    for (int i=0; i < gradients.length; i++) {
      if (deltaRet.get(i).getFirst() != null)       deltaRet.get(i).getFirst().addi(gradients[i]);
      if (deltaRet.get(i).getSecond() != null)       deltaRet.get(i).getSecond().addi(deltas[i]);
    }
  }
}
