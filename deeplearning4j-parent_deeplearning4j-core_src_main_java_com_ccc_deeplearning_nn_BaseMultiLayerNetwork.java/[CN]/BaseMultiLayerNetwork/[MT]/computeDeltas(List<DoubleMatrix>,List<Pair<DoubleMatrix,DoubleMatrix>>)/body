{
  DoubleMatrix[] gradients=new DoubleMatrix[nLayers + 2];
  DoubleMatrix[] deltas=new DoubleMatrix[nLayers + 2];
  ActivationFunction derivative=sigmoidLayers[0].activationFunction;
  DoubleMatrix delta=null;
  List<DoubleMatrix> weights=new ArrayList<>();
  for (int j=0; j < layers.length; j++)   weights.add(layers[j].getW());
  weights.add(logLayer.W);
  List<DoubleMatrix> zs=new ArrayList<>();
  zs.add(input);
  for (int i=0; i < layers.length; i++) {
    if (layers[i].getInput() == null && i == 0) {
      layers[i].setInput(input);
    }
 else     if (layers[i].getInput() == null) {
      this.feedForward();
    }
    zs.add(MatrixUtil.sigmoid(layers[i].getInput().mmul(weights.get(i)).addRowVector(layers[i].gethBias())));
  }
  zs.add(logLayer.input.mmul(logLayer.W).addRowVector(logLayer.b));
  for (int i=nLayers + 1; i >= 0; i--) {
    if (i >= nLayers + 1) {
      DoubleMatrix z=zs.get(i);
      delta=labels.sub(activations.get(i)).neg();
      DoubleMatrix initialDelta=delta.mul(derivative.applyDerivative(z));
      deltas[i]=initialDelta;
    }
 else {
      delta=deltas[i + 1];
      DoubleMatrix w=weights.get(i).transpose();
      DoubleMatrix z=zs.get(i);
      DoubleMatrix a=activations.get(i);
      DoubleMatrix error=delta.mmul(w);
      deltas[i]=error;
      error=error.mul(derivative.applyDerivative(z));
      deltas[i]=error;
      DoubleMatrix lastLayerDelta=deltas[i + 1].transpose();
      DoubleMatrix newGradient=lastLayerDelta.mmul(a);
      gradients[i]=newGradient.div(input.rows);
    }
  }
  for (int i=0; i < gradients.length; i++)   deltaRet.add(new Pair<>(gradients[i],deltas[i]));
}
