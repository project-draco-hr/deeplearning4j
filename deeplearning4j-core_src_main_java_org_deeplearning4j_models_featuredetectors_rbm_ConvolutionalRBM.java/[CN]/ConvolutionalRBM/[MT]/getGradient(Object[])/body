{
  int k=(int)params[0];
  float learningRate=(float)params[1];
  if (wAdaGrad != null)   wAdaGrad.setMasterStepSize(learningRate);
  if (hBiasAdaGrad != null)   hBiasAdaGrad.setMasterStepSize(learningRate);
  if (vBiasAdaGrad != null)   vBiasAdaGrad.setMasterStepSize(learningRate);
  INDArray chainStart=propUp(input);
  this.chainStart=chainStart;
  INDArray nvSamples=null;
  INDArray hiddenMeans=chainStart;
  for (int i=0; i < k; i++) {
    nvSamples=propDown(Sampling.binomial(eHid,1,conf().getRng()));
    hiddenMeans=propUp(nvSamples);
  }
  INDArray wGradient=Nd4j.create(W.shape());
  for (int i=0; i < numFilters[0]; i++)   for (int j=0; j < numFilters[1]; j++) {
    wGradient.putSlice(j,Convolution.convn(input,Nd4j.reverse(eHid.slice(j)),Convolution.Type.VALID).subi(Convolution.convn(eVis,Nd4j.reverse(eHid.slice(j)),Convolution.Type.VALID)));
  }
  INDArray vBiasGradient=Nd4j.scalar((float)chainStart.sub(hiddenMeans).sum(1).sum(Integer.MAX_VALUE).element());
  INDArray hBiasGradient=Nd4j.scalar((float)(input.sub(nvSamples)).sum(1).sum(Integer.MAX_VALUE).element());
  NeuralNetworkGradient ret=new NeuralNetworkGradient(wGradient,vBiasGradient,hBiasGradient);
  updateGradientAccordingToParams(ret,learningRate);
  return ret;
}
