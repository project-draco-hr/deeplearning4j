{
  boolean train=false;
  float currRecon=LossFunctions.reconEntropy(input,hBias,vBias,W,conf.getActivationFunction());
  ;
  NeuralNetwork revert=clone();
  int numEpochs=0;
  while (train) {
    if (numEpochs > iterations)     break;
    NeuralNetworkGradient gradient=getGradient(extraParams);
    INDArray wLearningRates=getAdaGrad().getLearningRates(gradient.getwGradient());
    INDArray z=transform(input);
    INDArray scaledInput=input.dup();
    Transforms.normalizeZeroMeanAndUnitVariance(scaledInput);
    Transforms.normalizeZeroMeanAndUnitVariance(z);
    INDArray outputDiff=z.sub(scaledInput);
    INDArray delta=W.mmul(outputDiff);
    INDArray hBiasMean=z.sum(1).transpose();
    if (conf.isUseAdaGrad()) {
      delta.muli(wLearningRates);
    }
 else     delta.muli(lr);
    if (conf.getMomentum() != 0)     delta.muli(conf.getMomentum()).add(delta.mul(1 - conf.getMomentum()));
    delta.divi(input.rows());
    getW().addi(W.sub(delta));
    float newRecon=LossFunctions.reconEntropy(input,hBias,vBias,W,conf.getActivationFunction());
    if (newRecon > currRecon || currRecon < 0 && newRecon < currRecon) {
      update((BaseNeuralNetwork)revert);
      log.info("Converged for new recon; breaking...");
      break;
    }
 else     if (newRecon == currRecon)     break;
 else {
      currRecon=newRecon;
      revert=clone();
      log.info("Recon went down " + currRecon);
    }
    numEpochs++;
    int plotEpochs=conf.getRenderWeightsEveryNumEpochs();
    if (plotEpochs > 0) {
      NeuralNetPlotter plotter=new NeuralNetPlotter();
      if (numEpochs % plotEpochs == 0) {
        plotter.plotNetworkGradient(this,getGradient(extraParams),getW().slices());
      }
    }
  }
}
