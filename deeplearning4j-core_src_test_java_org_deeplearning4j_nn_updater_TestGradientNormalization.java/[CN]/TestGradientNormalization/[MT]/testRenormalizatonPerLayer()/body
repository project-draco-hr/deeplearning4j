{
  Nd4j.getRandom().setSeed(12345);
  NeuralNetConfiguration conf=new NeuralNetConfiguration.Builder().layer(new DenseLayer.Builder().nIn(10).nOut(20).updater(org.deeplearning4j.nn.conf.Updater.NONE).gradientNormalization(GradientNormalization.RenormalizeL2PerLayer).build()).build();
  Layer layer=LayerFactories.getFactory(conf.getLayer()).create(conf);
  Updater updater=UpdaterCreator.getUpdater(layer);
  INDArray weightGrad=Nd4j.rand(10,20);
  INDArray biasGrad=Nd4j.rand(1,10);
  INDArray weightGradCopy=weightGrad.dup();
  INDArray biasGradCopy=biasGrad.dup();
  Gradient gradient=new DefaultGradient();
  gradient.setGradientFor(DefaultParamInitializer.WEIGHT_KEY,weightGrad);
  gradient.setGradientFor(DefaultParamInitializer.BIAS_KEY,biasGrad);
  updater.update(layer,gradient,0);
  assertNotEquals(weightGradCopy,weightGrad);
  assertNotEquals(biasGradCopy,biasGrad);
  double sumSquaresWeight=weightGradCopy.mul(weightGradCopy).sumNumber().doubleValue();
  double sumSquaresBias=biasGradCopy.mul(biasGradCopy).sumNumber().doubleValue();
  double sumSquares=sumSquaresWeight + sumSquaresBias;
  double l2Layer=Math.sqrt(sumSquares);
  INDArray normWeightsExpected=weightGradCopy.div(l2Layer);
  INDArray normBiasExpected=biasGradCopy.div(l2Layer);
  double l2Weight=gradient.getGradientFor(DefaultParamInitializer.WEIGHT_KEY).norm2Number().doubleValue();
  double l2Bias=gradient.getGradientFor(DefaultParamInitializer.BIAS_KEY).norm2Number().doubleValue();
  assertTrue(!Double.isNaN(l2Weight) && l2Weight > 0.0);
  assertTrue(!Double.isNaN(l2Bias) && l2Bias > 0.0);
  assertEquals(normWeightsExpected,gradient.getGradientFor(DefaultParamInitializer.WEIGHT_KEY));
  assertEquals(normBiasExpected,gradient.getGradientFor(DefaultParamInitializer.BIAS_KEY));
}
