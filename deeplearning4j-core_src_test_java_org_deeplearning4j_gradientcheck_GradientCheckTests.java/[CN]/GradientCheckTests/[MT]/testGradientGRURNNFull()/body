{
  String[] activFns={"tanh","relu"};
  LossFunction[] lossFunctions={LossFunction.MCXENT,LossFunction.MSE};
  String[] outputActivations={"softmax","tanh"};
  int timeSeriesLength=10;
  int nIn=7;
  int layerSize=9;
  int nOut=4;
  int miniBatchSize=8;
  Random r=new Random(12345L);
  INDArray input=Nd4j.zeros(miniBatchSize,nIn,timeSeriesLength);
  for (int i=0; i < miniBatchSize; i++) {
    for (int j=0; j < nIn; j++) {
      for (int k=0; k < timeSeriesLength; k++) {
        input.putScalar(new int[]{i,j,k},r.nextDouble() - 0.5);
      }
    }
  }
  INDArray labels=Nd4j.zeros(miniBatchSize * timeSeriesLength,nOut);
  for (int i=0; i < labels.size(0); i++) {
    int idx=r.nextInt(nOut);
    labels.putScalar(new int[]{i,idx},1.0f);
  }
  double[] l2vals={0.0,0.4,0.0};
  double[] l1vals={0.0,0.0,0.5};
  for (  String afn : activFns) {
    for (int i=0; i < lossFunctions.length; i++) {
      for (int k=0; k < l2vals.length; k++) {
        LossFunction lf=lossFunctions[i];
        String outputActivation=outputActivations[i];
        double l2=l2vals[k];
        double l1=l1vals[k];
        MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().activationFunction(afn).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).regularization(l1 > 0.0 && l2 > 0.0).dropOut(0.0).l2(l2).l1(l1).updater(Updater.NONE).seed(12345L).list(2).layer(0,new GRU.Builder().nIn(nIn).nOut(layerSize).build()).layer(1,new OutputLayer.Builder(lf).activation(outputActivation).nIn(layerSize).nOut(nOut).build()).inputPreProcessor(1,new RnnToFeedForwardPreProcessor(timeSeriesLength)).pretrain(false).backprop(true).build();
        MultiLayerNetwork mln=new MultiLayerNetwork(conf);
        mln.init();
        if (PRINT_RESULTS) {
          System.out.println("testGradientGRURNNFull() - activationFn=" + afn + ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", l2="+ l2+ ", l1="+ l1);
          for (int j=0; j < mln.getnLayers(); j++)           System.out.println("Layer " + j + " # params: "+ mln.getLayer(j).numParams());
        }
        boolean gradOK=GradientCheckUtil.checkGradients(mln,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels,true);
        String msg="testGradientGRURNNFull() - activationFn=" + afn + ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", l2="+ l2+ ", l1="+ l1;
        assertTrue(msg,gradOK);
      }
    }
  }
}
