{
  Nd4j.getRandom().setSeed(12345L);
  int timeSeriesLength=1;
  int nIn=2;
  int layerSize=2;
  int nOut=3;
  int miniBatchSize=11;
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,5.0)).regularization(false).updater(Updater.NONE).seed(12345L).list(2).layer(0,new GravesLSTM.Builder().nIn(nIn).nOut(layerSize).activation("tanh").build()).layer(1,new OutputLayer.Builder(LossFunction.MCXENT).activation("softmax").nIn(layerSize).nOut(nOut).build()).inputPreProcessor(1,new RnnToFeedForwardPreProcessor(timeSeriesLength)).pretrain(false).backprop(true).build();
  MultiLayerNetwork mln=new MultiLayerNetwork(conf);
  mln.init();
  Random r=new Random(12345L);
  INDArray input=Nd4j.zeros(miniBatchSize,nIn,timeSeriesLength);
  int x=0;
  for (int i=0; i < miniBatchSize; i++) {
    for (int j=0; j < nIn; j++) {
      for (int k=0; k < timeSeriesLength; k++) {
        input.putScalar(x++,r.nextDouble() - 0.5);
      }
    }
  }
  INDArray labels=Nd4j.zeros(miniBatchSize * timeSeriesLength,nOut);
  for (int i=0; i < labels.size(0); i++) {
    int idx=r.nextInt(nOut);
    labels.putScalar(new int[]{i,idx},1.0f);
  }
  if (PRINT_RESULTS) {
    System.out.println("testGravesLSTMBasic()");
    for (int j=0; j < mln.getnLayers(); j++)     System.out.println("Layer " + j + " # params: "+ mln.getLayer(j).numParams());
  }
  System.out.println("INPUT");
  System.out.println(Arrays.toString(flatten(input)));
  System.out.println("Activations: ");
  List<INDArray> activations=mln.feedForward(input);
  INDArray in=activations.get(0);
  System.out.println(Arrays.toString(flatten(in)));
  INDArray l0=activations.get(1);
  System.out.println(Arrays.toString(flatten(l0)));
  INDArray l1=activations.get(2);
  System.out.println(Arrays.toString(flatten(l1)));
  boolean gradOK=GradientCheckUtil.checkGradients(mln,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels,true);
  assertTrue(gradOK);
}
