{
  int nWorkers=4;
  SparkConf sparkConf=new SparkConf();
  sparkConf.setMaster("local[" + nWorkers + "]");
  sparkConf.setAppName("Test");
  JavaSparkContext sc=new JavaSparkContext(sparkConf);
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).iterations(1).list().layer(0,new DenseLayer.Builder().nIn(10).nOut(10).build()).layer(1,new OutputLayer.Builder().nIn(10).nOut(10).build()).pretrain(false).backprop(true).build();
  int miniBatchSizePerWorker=10;
  int averagingFrequency=5;
  int numberOfAveragings=3;
  int totalExamples=nWorkers * miniBatchSizePerWorker * averagingFrequency* numberOfAveragings;
  Nd4j.getRandom().setSeed(12345);
  List<DataSet> list=new ArrayList<>();
  for (int i=0; i < totalExamples; i++) {
    INDArray f=Nd4j.rand(1,10);
    INDArray l=Nd4j.rand(1,10);
    DataSet ds=new DataSet(f,l);
    list.add(ds);
  }
  JavaRDD<DataSet> rdd=sc.parallelize(list);
  rdd.repartition(4);
  VanillaTrainingMaster tm=new VanillaTrainingMaster.Builder(nWorkers).averagingFrequency(averagingFrequency).batchSizePerWorker(miniBatchSizePerWorker).saveUpdater(true).workerPrefetchNumBatches(0).build();
  SparkDl4jMultiLayer sparkNet=new SparkDl4jMultiLayer(sc,conf,tm);
  sparkNet.setCollectTrainingStats(true);
  sparkNet.fit(rdd);
  List<String> expectedStatNames=new ArrayList<>();
  Class<?>[] classes=new Class[]{CommonSparkTrainingStats.class,VanillaTrainingMasterStats.class,VanillaTrainingWorkerStats.class};
  String[] fieldNames=new String[]{"columnNames","columnNames","columnNames"};
  for (int i=0; i < classes.length; i++) {
    Field field=classes[i].getDeclaredField(fieldNames[i]);
    field.setAccessible(true);
    Object f=field.get(null);
    Collection<String> c=(Collection<String>)f;
    expectedStatNames.addAll(c);
  }
  System.out.println(expectedStatNames);
  SparkTrainingStats stats=sparkNet.getSparkTrainingStats();
  Set<String> actualKeySet=stats.getKeySet();
  assertEquals(expectedStatNames.size(),actualKeySet.size());
  for (  String s : stats.getKeySet()) {
    assertTrue(expectedStatNames.contains(s));
    Object o=stats.getValue(s);
  }
  String statsAsString=stats.statsAsString();
  System.out.println(statsAsString);
  assertEquals(actualKeySet.size(),statsAsString.split("\n").length);
  assertTrue(stats instanceof VanillaTrainingMasterStats);
  VanillaTrainingMasterStats masterStats=(VanillaTrainingMasterStats)stats;
  int[] broadcastCreateTime=masterStats.getVanillaMasterBroadcastCreateTimesMs();
  assertEquals(numberOfAveragings,broadcastCreateTime.length);
  assertGreaterEqZero(broadcastCreateTime);
  int[] fitTimes=masterStats.getVanillaMasterFitTimesMs();
  assertEquals(1,fitTimes.length);
  assertGreaterZero(fitTimes);
  int[] splitTimes=masterStats.getVanillaMasterSplitTimesMs();
  assertEquals(1,splitTimes.length);
  assertGreaterEqZero(splitTimes);
  int[] aggregateTimesMs=masterStats.getVanillaMasterAggregateTimesMs();
  assertEquals(numberOfAveragings,aggregateTimesMs.length);
  assertGreaterEqZero(aggregateTimesMs);
  int[] processParamsTimesMs=masterStats.getVanillaMasterProcessParamsUpdaterTimesMs();
  assertEquals(numberOfAveragings,processParamsTimesMs.length);
  assertGreaterEqZero(processParamsTimesMs);
  SparkTrainingStats commonStats=masterStats.getNestedTrainingStats();
  assertNotNull(commonStats);
  assertTrue(commonStats instanceof CommonSparkTrainingStats);
  CommonSparkTrainingStats cStats=(CommonSparkTrainingStats)commonStats;
  int[] workerFlatMapTotalTimeMs=cStats.getWorkerFlatMapTotalTimeMs();
  assertEquals(numberOfAveragings * nWorkers,workerFlatMapTotalTimeMs.length);
  assertGreaterZero(workerFlatMapTotalTimeMs);
  int[] workerFlatMapTotalExampleCount=cStats.getWorkerFlatMapTotalExampleCount();
  assertEquals(numberOfAveragings * nWorkers,workerFlatMapTotalExampleCount.length);
  assertGreaterZero(workerFlatMapTotalExampleCount);
  int[] workerFlatMapGetInitialModelTimeMs=cStats.getWorkerFlatMapGetInitialModelTimeMs();
  assertEquals(numberOfAveragings * nWorkers,workerFlatMapGetInitialModelTimeMs.length);
  assertGreaterEqZero(workerFlatMapGetInitialModelTimeMs);
  int[] workerFlatMapDataSetGetTimesMs=cStats.getWorkerFlatMapDataSetGetTimesMs();
  assertEquals(numberOfAveragings * nWorkers * averagingFrequency,workerFlatMapDataSetGetTimesMs.length);
  assertGreaterEqZero(workerFlatMapDataSetGetTimesMs);
  int[] workerFlatMapProcessMiniBatchTimesMs=cStats.getWorkerFlatMapProcessMiniBatchTimesMs();
  assertEquals(numberOfAveragings * nWorkers * averagingFrequency,workerFlatMapProcessMiniBatchTimesMs.length);
  assertGreaterEqZero(workerFlatMapProcessMiniBatchTimesMs);
  int workerFlatMapCountNoDataInstances=cStats.getWorkerFlatMapCountNoDataInstances();
  assertEquals(0,workerFlatMapCountNoDataInstances);
  SparkTrainingStats vanillaStats=cStats.getNestedTrainingStats();
  assertNotNull(vanillaStats);
  assertTrue(vanillaStats instanceof VanillaTrainingWorkerStats);
  VanillaTrainingWorkerStats vStats=(VanillaTrainingWorkerStats)vanillaStats;
  int[] vanillaWorkerBroadcastGetValueTimeMs=vStats.getVanillaWorkerBroadcastGetValueTimeMs();
  assertEquals(numberOfAveragings * nWorkers,vanillaWorkerBroadcastGetValueTimeMs.length);
  assertGreaterEqZero(vanillaWorkerBroadcastGetValueTimeMs);
  int[] vanillaWorkerInitTimeMs=vStats.getVanillaWorkerInitTimeMs();
  assertEquals(numberOfAveragings * nWorkers,vanillaWorkerInitTimeMs.length);
  assertGreaterEqZero(vanillaWorkerInitTimeMs);
  int[] vanillaWorkerFitTimesMs=vStats.getVanillaWorkerFitTimesMs();
  assertEquals(numberOfAveragings * nWorkers * averagingFrequency,vanillaWorkerFitTimesMs.length);
  assertGreaterEqZero(vanillaWorkerFitTimesMs);
  assertNull(vStats.getNestedTrainingStats());
}
