{
  JavaSparkContext sc=getContext();
  ClassPathResource cpr=new ClassPathResource("/csvsequence/csvsequence_0.txt");
  String path=cpr.getFile().getAbsolutePath();
  String folder=path.substring(0,path.length() - 17);
  path=folder + "*";
  PathToKeyConverter pathConverter=new PathToKeyConverterFilename();
  JavaPairRDD<Text,BytesPairWritable> toWrite=CanovaSparkUtil.combineFilesForSequenceFile(sc,path,path,pathConverter);
  Path p=Files.createTempDirectory("dl4j_testSeqPairFn");
  p.toFile().deleteOnExit();
  String outPath=p.toString() + "/out";
  new File(outPath).deleteOnExit();
  toWrite.saveAsNewAPIHadoopFile(outPath,Text.class,BytesPairWritable.class,SequenceFileOutputFormat.class);
  JavaPairRDD<Text,BytesPairWritable> fromSeq=sc.sequenceFile(outPath,Text.class,BytesPairWritable.class);
  SequenceRecordReader srr1=new CSVSequenceRecordReader(1,",");
  SequenceRecordReader srr2=new CSVSequenceRecordReader(1,",");
  PairSequenceRecordReaderBytesFunction psrbf=new PairSequenceRecordReaderBytesFunction(srr1,srr2);
  JavaRDD<Tuple2<Collection<Collection<Writable>>,Collection<Collection<Writable>>>> writables=fromSeq.map(psrbf);
  CanovaSequencePairDataSetFunction pairFn=new CanovaSequencePairDataSetFunction();
  JavaRDD<DataSet> data=writables.map(pairFn);
  List<DataSet> sparkData=data.collect();
  ClassPathResource resource=new ClassPathResource("csvsequence_0.txt");
  String featuresPath=resource.getFile().getAbsolutePath().replaceAll("0","%d");
  SequenceRecordReader featureReader=new CSVSequenceRecordReader(1,",");
  SequenceRecordReader labelReader=new CSVSequenceRecordReader(1,",");
  featureReader.initialize(new NumberedFileInputSplit(featuresPath,0,2));
  labelReader.initialize(new NumberedFileInputSplit(featuresPath,0,2));
  SequenceRecordReaderDataSetIterator iter=new SequenceRecordReaderDataSetIterator(featureReader,labelReader,1,-1,true);
  List<DataSet> localData=new ArrayList<>(3);
  while (iter.hasNext())   localData.add(iter.next());
  assertEquals(3,sparkData.size());
  assertEquals(3,localData.size());
  for (int i=0; i < 3; i++) {
    DataSet dsSpark=sparkData.get(i);
    DataSet dsLocal=localData.get(i);
    assertNull(dsSpark.getFeaturesMaskArray());
    assertNull(dsSpark.getLabelsMaskArray());
    INDArray fSpark=dsSpark.getFeatureMatrix();
    INDArray fLocal=dsLocal.getFeatureMatrix();
    INDArray lSpark=dsSpark.getLabels();
    INDArray lLocal=dsLocal.getLabels();
    int[] s=new int[]{1,3,4};
    assertArrayEquals(s,fSpark.shape());
    assertArrayEquals(s,fLocal.shape());
    assertArrayEquals(s,lSpark.shape());
    assertArrayEquals(s,lLocal.shape());
  }
  boolean[] found=new boolean[3];
  for (int i=0; i < 3; i++) {
    int foundIndex=-1;
    DataSet ds=sparkData.get(i);
    for (int j=0; j < 3; j++) {
      if (ds.equals(localData.get(j))) {
        if (foundIndex != -1)         fail();
        foundIndex=j;
        if (found[foundIndex])         fail();
        found[foundIndex]=true;
      }
    }
  }
  int count=0;
  for (  boolean b : found)   if (b)   count++;
  assertEquals(3,count);
}
