{
  JavaSparkContext sc=getContext();
  File f=new File("src/test/resources/csvsequence/csvsequence_0.txt");
  String path=f.getPath();
  String folder=path.substring(0,path.length() - 17);
  path=folder + "*";
  JavaPairRDD<String,PortableDataStream> origData=sc.binaryFiles(path);
  assertEquals(3,origData.count());
  SequenceRecordReader seqRR=new CSVSequenceRecordReader(1,",");
  SequenceRecordReaderFunction rrf=new SequenceRecordReaderFunction(seqRR);
  JavaRDD<Collection<Collection<Writable>>> rdd=origData.map(rrf);
  JavaRDD<DataSet> data=rdd.map(new CanovaSequenceDataSetFunction(2,-1,true,null,null));
  List<DataSet> collected=data.collect();
  InputSplit is=new FileSplit(new File(folder),new String[]{"txt"},true);
  SequenceRecordReader seqRR2=new CSVSequenceRecordReader(1,",");
  seqRR2.initialize(is);
  SequenceRecordReaderDataSetIterator iter=new SequenceRecordReaderDataSetIterator(seqRR2,1,-1,2,true);
  List<DataSet> listLocal=new ArrayList<>(3);
  while (iter.hasNext()) {
    listLocal.add(iter.next());
  }
  assertEquals(3,collected.size());
  assertEquals(3,listLocal.size());
  boolean[] found=new boolean[3];
  for (int i=0; i < 3; i++) {
    int foundIndex=-1;
    DataSet ds=collected.get(i);
    for (int j=0; j < 3; j++) {
      if (ds.equals(listLocal.get(j))) {
        if (foundIndex != -1)         fail();
        foundIndex=j;
        if (found[foundIndex])         fail();
        found[foundIndex]=true;
      }
    }
  }
  int count=0;
  for (  boolean b : found)   if (b)   count++;
  assertEquals(3,count);
}
