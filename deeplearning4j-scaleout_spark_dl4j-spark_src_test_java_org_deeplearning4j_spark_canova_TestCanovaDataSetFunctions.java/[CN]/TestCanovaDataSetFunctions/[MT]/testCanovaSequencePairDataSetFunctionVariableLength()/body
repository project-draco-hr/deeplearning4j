{
  File f=new File("src/test/resources/csvsequence/csvsequence_0.txt");
  String pathFeatures=f.getAbsolutePath();
  String folderFeatures=pathFeatures.substring(0,pathFeatures.length() - 17);
  pathFeatures=folderFeatures + "*";
  File f2=new File("src/test/resources/csvsequencelabels/csvsequencelabelsShort_0.txt");
  String pathLabels=f2.getPath();
  String folderLabels=pathLabels.substring(0,pathLabels.length() - 28);
  pathLabels=folderLabels + "*";
  PathToKeyConverter pathConverter=new PathToKeyConverterNumber();
  JavaPairRDD<Text,BytesPairWritable> toWrite=CanovaSparkUtil.combineFilesForSequenceFile(sc,pathFeatures,pathLabels,pathConverter);
  Path p=Files.createTempDirectory("dl4j_testSeqPairFnVarLength");
  p.toFile().deleteOnExit();
  String outPath=p.toString() + "/out";
  new File(outPath).deleteOnExit();
  toWrite.saveAsNewAPIHadoopFile(outPath,Text.class,BytesPairWritable.class,SequenceFileOutputFormat.class);
  JavaPairRDD<Text,BytesPairWritable> fromSeq=sc.sequenceFile(outPath,Text.class,BytesPairWritable.class);
  SequenceRecordReader srr1=new CSVSequenceRecordReader(1,",");
  SequenceRecordReader srr2=new CSVSequenceRecordReader(1,",");
  PairSequenceRecordReaderBytesFunction psrbf=new PairSequenceRecordReaderBytesFunction(srr1,srr2);
  JavaRDD<Tuple2<Collection<Collection<Writable>>,Collection<Collection<Writable>>>> writables=fromSeq.map(psrbf);
  CanovaSequencePairDataSetFunction pairFn=new CanovaSequencePairDataSetFunction(4,false,CanovaSequencePairDataSetFunction.AlignmentMode.ALIGN_END);
  JavaRDD<DataSet> data=writables.map(pairFn);
  List<DataSet> sparkData=data.collect();
  String featuresPath=f.getPath().replaceAll("0","%d");
  String labelsPath=f2.getPath().replaceAll("0","%d");
  SequenceRecordReader featureReader=new CSVSequenceRecordReader(1,",");
  SequenceRecordReader labelReader=new CSVSequenceRecordReader(1,",");
  featureReader.initialize(new NumberedFileInputSplit(featuresPath,0,2));
  labelReader.initialize(new NumberedFileInputSplit(labelsPath,0,2));
  SequenceRecordReaderDataSetIterator iter=new SequenceRecordReaderDataSetIterator(featureReader,labelReader,1,4,false,SequenceRecordReaderDataSetIterator.AlignmentMode.ALIGN_END);
  List<DataSet> localData=new ArrayList<>(3);
  while (iter.hasNext())   localData.add(iter.next());
  assertEquals(3,sparkData.size());
  assertEquals(3,localData.size());
  int[] fShapeExp=new int[]{1,3,4};
  int[] lShapeExp=new int[]{1,4,4};
  for (int i=0; i < 3; i++) {
    DataSet dsSpark=sparkData.get(i);
    DataSet dsLocal=localData.get(i);
    assertNull(dsSpark.getFeaturesMaskArray());
    assertNotNull(dsSpark.getLabelsMaskArray());
    INDArray fSpark=dsSpark.getFeatureMatrix();
    INDArray fLocal=dsLocal.getFeatureMatrix();
    INDArray lSpark=dsSpark.getLabels();
    INDArray lLocal=dsLocal.getLabels();
    assertArrayEquals(fShapeExp,fSpark.shape());
    assertArrayEquals(fShapeExp,fLocal.shape());
    assertArrayEquals(lShapeExp,lSpark.shape());
    assertArrayEquals(lShapeExp,lLocal.shape());
  }
  boolean[] found=new boolean[3];
  for (int i=0; i < 3; i++) {
    int foundIndex=-1;
    DataSet ds=sparkData.get(i);
    for (int j=0; j < 3; j++) {
      if (ds.equals(localData.get(j))) {
        if (foundIndex != -1)         fail();
        foundIndex=j;
        if (found[foundIndex])         fail();
        found[foundIndex]=true;
      }
    }
  }
  int count=0;
  for (  boolean b : found)   if (b)   count++;
  assertEquals(3,count);
  CanovaSequencePairDataSetFunction pairFnAlignStart=new CanovaSequencePairDataSetFunction(4,false,CanovaSequencePairDataSetFunction.AlignmentMode.ALIGN_START);
  JavaRDD<DataSet> rddDataAlignStart=writables.map(pairFnAlignStart);
  List<DataSet> sparkDataAlignStart=rddDataAlignStart.collect();
  featureReader.initialize(new NumberedFileInputSplit(featuresPath,0,2));
  labelReader.initialize(new NumberedFileInputSplit(labelsPath,0,2));
  SequenceRecordReaderDataSetIterator iterAlignStart=new SequenceRecordReaderDataSetIterator(featureReader,labelReader,1,4,false,SequenceRecordReaderDataSetIterator.AlignmentMode.ALIGN_START);
  List<DataSet> localDataAlignStart=new ArrayList<>(3);
  while (iterAlignStart.hasNext())   localDataAlignStart.add(iterAlignStart.next());
  assertEquals(3,sparkDataAlignStart.size());
  assertEquals(3,localDataAlignStart.size());
  for (int i=0; i < 3; i++) {
    DataSet dsSpark=sparkDataAlignStart.get(i);
    DataSet dsLocal=localDataAlignStart.get(i);
    assertNull(dsSpark.getFeaturesMaskArray());
    assertNotNull(dsSpark.getLabelsMaskArray());
    INDArray fSpark=dsSpark.getFeatureMatrix();
    INDArray fLocal=dsLocal.getFeatureMatrix();
    INDArray lSpark=dsSpark.getLabels();
    INDArray lLocal=dsLocal.getLabels();
    assertArrayEquals(fShapeExp,fSpark.shape());
    assertArrayEquals(fShapeExp,fLocal.shape());
    assertArrayEquals(lShapeExp,lSpark.shape());
    assertArrayEquals(lShapeExp,lLocal.shape());
  }
  found=new boolean[3];
  for (int i=0; i < 3; i++) {
    int foundIndex=-1;
    DataSet ds=sparkData.get(i);
    for (int j=0; j < 3; j++) {
      if (ds.equals(localData.get(j))) {
        if (foundIndex != -1)         fail();
        foundIndex=j;
        if (found[foundIndex])         fail();
        found[foundIndex]=true;
      }
    }
  }
  count=0;
  for (  boolean b : found)   if (b)   count++;
  assertEquals(3,count);
}
