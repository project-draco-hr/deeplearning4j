{
  int[] ns={320,321,25600,25601,25615};
  for (  int n : ns) {
    List<String> list=new ArrayList<>();
    for (int i=0; i < n; i++) {
      list.add(String.valueOf(i));
    }
    JavaRDD<String> rdd=sc.parallelize(list);
    rdd.repartition(65);
    int totalDataSetObjectCount=n;
    int dataSetObjectsPerSplit=8 * 4 * 10;
    int valuesPerPartition=10;
    int nPartitions=32;
    JavaRDD<String>[] splits=org.deeplearning4j.spark.util.SparkUtils.balancedRandomSplit(totalDataSetObjectCount,dataSetObjectsPerSplit,rdd,new Random().nextLong());
    List<Integer> counts=new ArrayList<>();
    List<List<Tuple2<Integer,Integer>>> partitionCountList=new ArrayList<>();
    for (    JavaRDD<String> split : splits) {
      JavaRDD<String> repartitioned=SparkUtils.repartition(split,Repartition.Always,RepartitionStrategy.Balanced,valuesPerPartition,nPartitions);
      List<Tuple2<Integer,Integer>> partitionCounts=repartitioned.mapPartitionsWithIndex(new CountPartitionsFunction<String>(),true).collect();
      partitionCountList.add(partitionCounts);
      counts.add((int)split.count());
    }
    int expNumPartitionsWithMore=totalDataSetObjectCount % nPartitions;
    int actNumPartitionsWithMore=0;
    for (    List<Tuple2<Integer,Integer>> l : partitionCountList) {
      assertEquals(nPartitions,l.size());
      for (      Tuple2<Integer,Integer> t2 : l) {
        int partitionSize=t2._2();
        if (partitionSize > valuesPerPartition)         actNumPartitionsWithMore++;
      }
    }
    assertEquals(expNumPartitionsWithMore,actNumPartitionsWithMore);
  }
}
