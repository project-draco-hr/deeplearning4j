{
  int lastLayerNumber=numLayers - 1;
  inLayerName=(inputLayer.getLayerName() != null) ? inputLayer.getLayerName() : Integer.toString(i);
  outLayerName=(outputLayer.getLayerName() != null) ? outputLayer.getLayerName() : Integer.toString(i + 1);
  if (i < lastLayerNumber) {
switch (inputLayer.getClass().getSimpleName()) {
case CONVOLUTION_LAYER:
      ConvolutionLayer convolutionLayer=(ConvolutionLayer)inputLayer;
    if (i == 0) {
      conf.inputPreProcessor(i,new FeedForwardToCnnPreProcessor(lastHeight,lastWidth,lastOutChannels));
      lastnOut=convolutionLayer.getNOut();
      convolutionLayer.setNIn(lastOutChannels);
    }
  getConvolutionOutputSize(new int[]{lastHeight,lastWidth},convolutionLayer.getKernelSize(),convolutionLayer.getPadding(),convolutionLayer.getStride());
lastOutChannels=convolutionLayer.getNOut();
switch (outputLayer.getClass().getSimpleName()) {
case CONVOLUTION_LAYER:
ConvolutionLayer nextConv=(ConvolutionLayer)outputLayer;
lastOutChannels=lastnOut=convolutionLayer.getNOut();
storeNInAndNOut(inLayerName,lastnOut);
nextConv.setNIn(lastnOut);
break;
case LOCAL_RESPONSE_NORMALIZATION:
case SUBSAMPLING_LAYER:
lastOutChannels=lastnOut=convolutionLayer.getNOut();
storeNInAndNOut(inLayerName,lastnOut);
break;
case RECURSIVE_AUTO_ENCODER:
case RBM:
case DENSE_LAYER:
case OUTPUT_LAYER:
FeedForwardLayer feedForwardLayer=(FeedForwardLayer)outputLayer;
lastOutChannels=convolutionLayer.getNOut();
lastnOut=lastHeight * lastWidth * lastOutChannels;
storeNInAndNOut(inLayerName,lastnOut);
feedForwardLayer.setNIn(lastnOut);
conf.inputPreProcessor(i + 1,new CnnToFeedForwardPreProcessor(lastHeight,lastWidth,lastOutChannels));
break;
case GRAVES_LSTM:
case GRAVES_BIDIRECTIONAL_LSTM:
case RNN_OUTPUT_LAYER:
feedForwardLayer=(FeedForwardLayer)outputLayer;
lastnOut=lastHeight * lastWidth * lastOutChannels;
storeNInAndNOut(inLayerName,lastnOut);
feedForwardLayer.setNIn(lastnOut);
conf.inputPreProcessor(i + 1,new CnnToRnnPreProcessor(lastHeight,lastWidth,lastOutChannels));
break;
case ACTIVATION_LAYER:
feedForwardLayer=(ActivationLayer)outputLayer;
lastOutChannels=lastnOut=convolutionLayer.getNOut();
storeNInAndNOut(inLayerName,lastnOut);
feedForwardLayer.setNOut(lastnOut);
useCNN=true;
break;
case BATCH_NORMALIZATION:
feedForwardLayer=(BatchNormalization)outputLayer;
lastOutChannels=lastnOut=convolutionLayer.getNOut();
storeNInAndNOut(inLayerName,lastnOut);
feedForwardLayer.setNOut(lastnOut);
useCNN=true;
break;
}
break;
case SUBSAMPLING_LAYER:
SubsamplingLayer subsamplingLayer=(SubsamplingLayer)inputLayer;
getConvolutionOutputSize(new int[]{lastHeight,lastWidth},subsamplingLayer.getKernelSize(),subsamplingLayer.getPadding(),subsamplingLayer.getStride());
if (i == 0) throw new UnsupportedOperationException("Unsupported path: first layer shouldn't be " + inLayerName);
switch (outputLayer.getClass().getSimpleName()) {
case CONVOLUTION_LAYER:
ConvolutionLayer nextConv=(ConvolutionLayer)outputLayer;
storeNInAndNOut(outLayerName,lastOutChannels);
nextConv.setNIn(lastOutChannels);
break;
case SUBSAMPLING_LAYER:
storeNInAndNOut(inLayerName,lastnOut);
break;
case RECURSIVE_AUTO_ENCODER:
case RBM:
case DENSE_LAYER:
case OUTPUT_LAYER:
FeedForwardLayer feedForwardLayer=(FeedForwardLayer)outputLayer;
lastnOut=lastHeight * lastWidth * lastOutChannels;
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer.setNIn(lastnOut);
conf.inputPreProcessor(i + 1,new CnnToFeedForwardPreProcessor(lastHeight,lastWidth,lastOutChannels));
break;
case GRAVES_LSTM:
case GRAVES_BIDIRECTIONAL_LSTM:
case RNN_OUTPUT_LAYER:
feedForwardLayer=(FeedForwardLayer)outputLayer;
lastnOut=lastHeight * lastWidth * lastOutChannels;
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer.setNIn(lastnOut);
conf.inputPreProcessor(i + 1,new CnnToRnnPreProcessor(lastHeight,lastWidth,lastOutChannels));
break;
case ACTIVATION_LAYER:
case BATCH_NORMALIZATION:
feedForwardLayer=(FeedForwardLayer)outputLayer;
storeNInAndNOut(inLayerName,lastnOut);
feedForwardLayer.setNOut(lastnOut);
useCNN=true;
break;
}
break;
case GRAVES_LSTM:
case GRAVES_BIDIRECTIONAL_LSTM:
if (i == 0) throw new UnsupportedOperationException("Apply nIn attribute to the layer configuration for " + inLayerName);
FeedForwardLayer feedForwardLayer=(FeedForwardLayer)inputLayer;
switch (outputLayer.getClass().getSimpleName()) {
case CONVOLUTION_LAYER:
convolutionLayer=(ConvolutionLayer)outputLayer;
conf.inputPreProcessor(i,new RnnToCnnPreProcessor(lastHeight,lastWidth,lastOutChannels));
lastnOut=convolutionLayer.getNOut();
convolutionLayer.setNIn(lastnOut);
break;
case SUBSAMPLING_LAYER:
throw new UnsupportedOperationException("Subsampling Layer should be connected to Convolution, LocalResponseNormalization or BatchNormalization Layer");
case GRAVES_LSTM:
case GRAVES_BIDIRECTIONAL_LSTM:
case RNN_OUTPUT_LAYER:
FeedForwardLayer feedForwardLayer2=(FeedForwardLayer)outputLayer;
lastnOut=feedForwardLayer.getNOut();
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer2.setNIn(lastnOut);
break;
case RECURSIVE_AUTO_ENCODER:
case RBM:
case DENSE_LAYER:
case OUTPUT_LAYER:
feedForwardLayer2=(FeedForwardLayer)outputLayer;
lastnOut=feedForwardLayer.getNOut();
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer2.setNIn(lastnOut);
conf.inputPreProcessor(i + 1,new RnnToFeedForwardPreProcessor());
break;
case BATCH_NORMALIZATION:
throw new UnsupportedOperationException("Currently not implemented for " + inLayerName);
case ACTIVATION_LAYER:
feedForwardLayer2=(FeedForwardLayer)outputLayer;
lastnOut=feedForwardLayer.getNOut();
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer2.setNOut(lastnOut);
conf.inputPreProcessor(i + 1,new RnnToFeedForwardPreProcessor());
useCNN=false;
break;
}
break;
case RECURSIVE_AUTO_ENCODER:
case RBM:
case DENSE_LAYER:
if (i == 0) throw new UnsupportedOperationException("Apply nIn attribute to the layer configuration for " + inLayerName);
feedForwardLayer=(FeedForwardLayer)inputLayer;
switch (outputLayer.getClass().getSimpleName()) {
case CONVOLUTION_LAYER:
convolutionLayer=(ConvolutionLayer)outputLayer;
conf.inputPreProcessor(i + 1,new FeedForwardToCnnPreProcessor(lastHeight,lastWidth,lastOutChannels));
lastnOut=lastOutChannels;
convolutionLayer.setNIn(lastnOut);
break;
case SUBSAMPLING_LAYER:
conf.inputPreProcessor(i + 1,new FeedForwardToCnnPreProcessor(lastHeight,lastWidth,lastOutChannels));
lastnOut=lastOutChannels;
storeNInAndNOut(inLayerName,lastnOut);
break;
case RECURSIVE_AUTO_ENCODER:
case RBM:
case DENSE_LAYER:
case OUTPUT_LAYER:
FeedForwardLayer feedForwardLayer2=(FeedForwardLayer)outputLayer;
lastnOut=feedForwardLayer.getNOut();
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer2.setNIn(lastnOut);
break;
case GRAVES_LSTM:
case GRAVES_BIDIRECTIONAL_LSTM:
case RNN_OUTPUT_LAYER:
feedForwardLayer2=(FeedForwardLayer)outputLayer;
lastnOut=feedForwardLayer.getNOut();
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer2.setNIn(lastnOut);
conf.inputPreProcessor(i + 1,new FeedForwardToRnnPreProcessor());
break;
case BATCH_NORMALIZATION:
throw new UnsupportedOperationException("Currently not implemented for " + inLayerName);
case ACTIVATION_LAYER:
feedForwardLayer2=(FeedForwardLayer)outputLayer;
lastnOut=feedForwardLayer.getNOut();
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer2.setNOut(lastnOut);
useCNN=false;
break;
}
break;
case ACTIVATION_LAYER:
case BATCH_NORMALIZATION:
if (i == 0) throw new UnsupportedOperationException("Unsupported path: first layer shouldn't be " + inLayerName);
switch (outputLayer.getClass().getSimpleName()) {
case CONVOLUTION_LAYER:
convolutionLayer=(ConvolutionLayer)outputLayer;
if (useCNN) {
storeNInAndNOut(outLayerName,lastOutChannels);
convolutionLayer.setNIn(lastnOut);
}
 else {
conf.inputPreProcessor(i + 1,new FeedForwardToCnnPreProcessor(lastHeight,lastWidth,lastOutChannels));
lastnOut=lastOutChannels;
convolutionLayer.setNIn(lastnOut);
}
break;
case SUBSAMPLING_LAYER:
storeNInAndNOut(inLayerName,lastnOut);
break;
case RECURSIVE_AUTO_ENCODER:
case RBM:
case DENSE_LAYER:
case OUTPUT_LAYER:
if (useCNN) {
feedForwardLayer=(FeedForwardLayer)outputLayer;
lastnOut=lastHeight * lastWidth * lastOutChannels;
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer.setNIn(lastnOut);
conf.inputPreProcessor(i + 1,new CnnToFeedForwardPreProcessor(lastHeight,lastWidth,lastOutChannels));
}
 else {
feedForwardLayer=(FeedForwardLayer)outputLayer;
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer.setNIn(lastnOut);
}
break;
case GRAVES_LSTM:
case GRAVES_BIDIRECTIONAL_LSTM:
case RNN_OUTPUT_LAYER:
if (useCNN) {
feedForwardLayer=(FeedForwardLayer)outputLayer;
lastnOut=lastHeight * lastWidth * lastOutChannels;
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer.setNIn(lastnOut);
conf.inputPreProcessor(i + 1,new CnnToRnnPreProcessor(lastHeight,lastWidth,lastOutChannels));
}
 else {
feedForwardLayer=(FeedForwardLayer)outputLayer;
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer.setNIn(lastnOut);
conf.inputPreProcessor(i + 1,new FeedForwardToRnnPreProcessor());
}
break;
case BATCH_NORMALIZATION:
case ACTIVATION_LAYER:
feedForwardLayer=(FeedForwardLayer)outputLayer;
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer.setNOut(lastnOut);
break;
case LOCAL_RESPONSE_NORMALIZATION:
throw new UnsupportedOperationException("LocalResponse should not follow " + inLayerName);
}
break;
case LOCAL_RESPONSE_NORMALIZATION:
if (i == 0) throw new UnsupportedOperationException("Unsupported path: first layer shouldn't be " + inLayerName);
switch (outputLayer.getClass().getSimpleName()) {
case CONVOLUTION_LAYER:
ConvolutionLayer nextConv=(ConvolutionLayer)outputLayer;
storeNInAndNOut(outLayerName,lastOutChannels);
nextConv.setNIn(lastnOut);
break;
case RECURSIVE_AUTO_ENCODER:
case RBM:
case DENSE_LAYER:
case OUTPUT_LAYER:
feedForwardLayer=(FeedForwardLayer)outputLayer;
lastnOut=lastHeight * lastWidth * lastOutChannels;
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer.setNIn(lastnOut);
conf.inputPreProcessor(i + 1,new CnnToFeedForwardPreProcessor(lastHeight,lastWidth,lastOutChannels));
break;
case GRAVES_LSTM:
case GRAVES_BIDIRECTIONAL_LSTM:
case RNN_OUTPUT_LAYER:
feedForwardLayer=(FeedForwardLayer)outputLayer;
lastnOut=lastHeight * lastWidth * lastOutChannels;
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer.setNIn(lastnOut);
conf.inputPreProcessor(i + 1,new CnnToRnnPreProcessor(lastHeight,lastWidth,lastOutChannels));
break;
case BATCH_NORMALIZATION:
throw new UnsupportedOperationException("BaseNormalization should not follow a LocalResponse layer.");
case ACTIVATION_LAYER:
feedForwardLayer=(FeedForwardLayer)outputLayer;
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer.setNOut(lastnOut);
useCNN=true;
break;
}
break;
case RNN_OUTPUT_LAYER:
case OUTPUT_LAYER:
throw new UnsupportedOperationException("OutputLayer should be the last layer");
}
}
 else throw new UnsupportedOperationException("Unsupported path: final " + inputLayer.getClass().getSimpleName() + " layer");
}
