{
  int lastLayerNumber=numLayers - 1;
  inLayerName=(inputLayer.getLayerName() != null) ? inputLayer.getLayerName() : Integer.toString(i);
  outLayerName=(outputLayer.getLayerName() != null) ? outputLayer.getLayerName() : Integer.toString(i + 1);
  if (i < lastLayerNumber) {
switch (inputLayer.getClass().getSimpleName()) {
case "ConvolutionLayer":
      ConvolutionLayer convolutionLayer=(ConvolutionLayer)inputLayer;
    if (i == 0) {
      conf.inputPreProcessor(i,new FeedForwardToCnnPreProcessor(lastHeight,lastWidth,lastOutChannels));
      lastnOut=convolutionLayer.getNOut();
      convolutionLayer.setNIn(lastOutChannels);
    }
  getConvolutionOutputSize(new int[]{lastHeight,lastWidth},convolutionLayer.getKernelSize(),convolutionLayer.getPadding(),convolutionLayer.getStride());
lastOutChannels=convolutionLayer.getNOut();
switch (outputLayer.getClass().getSimpleName()) {
case "ConvolutionLayer":
ConvolutionLayer nextConv=(ConvolutionLayer)outputLayer;
lastOutChannels=lastnOut=convolutionLayer.getNOut();
storeNInAndNOut(inLayerName,lastnOut);
nextConv.setNIn(lastnOut);
break;
case "LocalResponseNormalization":
case "SubsamplingLayer":
lastOutChannels=lastnOut=convolutionLayer.getNOut();
storeNInAndNOut(inLayerName,lastnOut);
break;
case "RecursiveAutoEncoder":
case "RBM":
case "DenseLayer":
case "OutputLayer":
FeedForwardLayer feedForwardLayer=(FeedForwardLayer)outputLayer;
lastOutChannels=convolutionLayer.getNOut();
lastnOut=lastHeight * lastWidth * lastOutChannels;
storeNInAndNOut(inLayerName,lastnOut);
feedForwardLayer.setNIn(lastnOut);
conf.inputPreProcessor(i + 1,new CnnToFeedForwardPreProcessor(lastHeight,lastWidth,lastOutChannels));
break;
case "GravesLSTM":
case "GravesBidirectionalLSTM":
case "RnnOutputLayer":
feedForwardLayer=(FeedForwardLayer)outputLayer;
lastnOut=lastHeight * lastWidth * lastOutChannels;
storeNInAndNOut(inLayerName,lastnOut);
feedForwardLayer.setNIn(lastnOut);
conf.inputPreProcessor(i + 1,new CnnToRnnPreProcessor(lastHeight,lastWidth,lastOutChannels));
break;
case "ActivationLayer":
feedForwardLayer=(ActivationLayer)outputLayer;
lastOutChannels=lastnOut=convolutionLayer.getNOut();
storeNInAndNOut(inLayerName,lastnOut);
feedForwardLayer.setNOut(lastnOut);
useCNN=true;
break;
case "BatchNormalization":
feedForwardLayer=(BatchNormalization)outputLayer;
lastOutChannels=lastnOut=convolutionLayer.getNOut();
storeNInAndNOut(inLayerName,lastnOut);
feedForwardLayer.setNOut(lastnOut);
useCNN=true;
break;
}
break;
case "SubsamplingLayer":
SubsamplingLayer subsamplingLayer=(SubsamplingLayer)inputLayer;
getConvolutionOutputSize(new int[]{lastHeight,lastWidth},subsamplingLayer.getKernelSize(),subsamplingLayer.getPadding(),subsamplingLayer.getStride());
if (i == 0) throw new UnsupportedOperationException("Unsupported path: first layer shouldn't be " + inLayerName);
switch (outputLayer.getClass().getSimpleName()) {
case "ConvolutionLayer":
ConvolutionLayer nextConv=(ConvolutionLayer)outputLayer;
storeNInAndNOut(outLayerName,lastOutChannels);
nextConv.setNIn(lastOutChannels);
break;
case "SubsamplingLayer":
storeNInAndNOut(inLayerName,lastnOut);
break;
case "RecursiveAutoEncoder":
case "RBM":
case "DenseLayer":
case "OutputLayer":
FeedForwardLayer feedForwardLayer=(FeedForwardLayer)outputLayer;
lastnOut=lastHeight * lastWidth * lastOutChannels;
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer.setNIn(lastnOut);
conf.inputPreProcessor(i + 1,new CnnToFeedForwardPreProcessor(lastHeight,lastWidth,lastOutChannels));
break;
case "GravesLSTM":
case "GravesBidirectionalLSTM":
case "RnnOutputLayer":
feedForwardLayer=(FeedForwardLayer)outputLayer;
lastnOut=lastHeight * lastWidth * lastOutChannels;
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer.setNIn(lastnOut);
conf.inputPreProcessor(i + 1,new CnnToRnnPreProcessor(lastHeight,lastWidth,lastOutChannels));
break;
case "ActivationLayer":
case "BatchNormalization":
feedForwardLayer=(FeedForwardLayer)outputLayer;
storeNInAndNOut(inLayerName,lastnOut);
feedForwardLayer.setNOut(lastnOut);
useCNN=true;
break;
}
break;
case "GravesLSTM":
case "GravesBidirectionalLSTM":
if (i == 0) throw new UnsupportedOperationException("Apply nIn attribute to the layer configuration for " + inLayerName);
FeedForwardLayer feedForwardLayer=(FeedForwardLayer)inputLayer;
switch (outputLayer.getClass().getSimpleName()) {
case "ConvolutionLayer":
convolutionLayer=(ConvolutionLayer)outputLayer;
conf.inputPreProcessor(i,new RnnToCnnPreProcessor(lastHeight,lastWidth,lastOutChannels));
lastnOut=convolutionLayer.getNOut();
convolutionLayer.setNIn(lastnOut);
break;
case "SubsamplingLayer":
throw new UnsupportedOperationException("Subsampling Layer should be connected to Convolution, LocalResponseNormalization or BatchNormalization Layer");
case "GravesLSTM":
case "GravesBidirectionalLSTM":
case "RnnOutputLayer":
FeedForwardLayer feedForwardLayer2=(FeedForwardLayer)outputLayer;
lastnOut=feedForwardLayer.getNOut();
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer2.setNIn(lastnOut);
break;
case "RecursiveAutoEncoder":
case "RBM":
case "DenseLayer":
case "OutputLayer":
feedForwardLayer2=(FeedForwardLayer)outputLayer;
lastnOut=feedForwardLayer.getNOut();
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer2.setNIn(lastnOut);
conf.inputPreProcessor(i + 1,new RnnToFeedForwardPreProcessor());
break;
case "BatchNormalization":
throw new UnsupportedOperationException("Currently not implemented for " + inLayerName);
case "ActivationLayer":
feedForwardLayer2=(FeedForwardLayer)outputLayer;
lastnOut=feedForwardLayer.getNOut();
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer2.setNOut(lastnOut);
conf.inputPreProcessor(i + 1,new RnnToFeedForwardPreProcessor());
useCNN=false;
break;
}
break;
case "RecursiveAutoEncoder":
case "RBM":
case "DenseLayer":
if (i == 0) throw new UnsupportedOperationException("Apply nIn attribute to the layer configuration for " + inLayerName);
feedForwardLayer=(FeedForwardLayer)inputLayer;
switch (outputLayer.getClass().getSimpleName()) {
case "ConvolutionLayer":
convolutionLayer=(ConvolutionLayer)outputLayer;
conf.inputPreProcessor(i + 1,new FeedForwardToCnnPreProcessor(lastHeight,lastWidth,lastOutChannels));
lastnOut=lastOutChannels;
convolutionLayer.setNIn(lastnOut);
break;
case "SubsamplingLayer":
conf.inputPreProcessor(i + 1,new FeedForwardToCnnPreProcessor(lastHeight,lastWidth,lastOutChannels));
lastnOut=lastOutChannels;
storeNInAndNOut(inLayerName,lastnOut);
break;
case "RecursiveAutoEncoder":
case "RBM":
case "DenseLayer":
case "OutputLayer":
FeedForwardLayer feedForwardLayer2=(FeedForwardLayer)outputLayer;
lastnOut=feedForwardLayer.getNOut();
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer2.setNIn(lastnOut);
break;
case "GravesLSTM":
case "GravesBidirectionalLSTM":
case "RnnOutputLayer":
feedForwardLayer2=(FeedForwardLayer)outputLayer;
lastnOut=feedForwardLayer.getNOut();
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer2.setNIn(lastnOut);
conf.inputPreProcessor(i + 1,new FeedForwardToRnnPreProcessor());
break;
case "BatchNormalization":
throw new UnsupportedOperationException("Currently not implemented for " + inLayerName);
case "ActivationLayer":
feedForwardLayer2=(FeedForwardLayer)outputLayer;
lastnOut=feedForwardLayer.getNOut();
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer2.setNOut(lastnOut);
useCNN=false;
break;
}
break;
case "ActivationLayer":
case "BatchNormalization":
if (i == 0) throw new UnsupportedOperationException("Unsupported path: first layer shouldn't be " + inLayerName);
switch (outputLayer.getClass().getSimpleName()) {
case "ConvolutionLayer":
convolutionLayer=(ConvolutionLayer)outputLayer;
if (useCNN) {
storeNInAndNOut(outLayerName,lastOutChannels);
convolutionLayer.setNIn(lastnOut);
}
 else {
conf.inputPreProcessor(i + 1,new FeedForwardToCnnPreProcessor(lastHeight,lastWidth,lastOutChannels));
lastnOut=lastOutChannels;
convolutionLayer.setNIn(lastnOut);
}
break;
case "SubsamplingLayer":
storeNInAndNOut(inLayerName,lastnOut);
break;
case "RecursiveAutoEncoder":
case "RBM":
case "DenseLayer":
case "OutputLayer":
if (useCNN) {
feedForwardLayer=(FeedForwardLayer)outputLayer;
lastnOut=lastHeight * lastWidth * lastOutChannels;
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer.setNIn(lastnOut);
conf.inputPreProcessor(i + 1,new CnnToFeedForwardPreProcessor(lastHeight,lastWidth,lastOutChannels));
}
 else {
feedForwardLayer=(FeedForwardLayer)outputLayer;
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer.setNIn(lastnOut);
}
break;
case "GravesLSTM":
case "GravesBidirectionalLSTM":
case "RnnOutputLayer":
if (useCNN) {
feedForwardLayer=(FeedForwardLayer)outputLayer;
lastnOut=lastHeight * lastWidth * lastOutChannels;
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer.setNIn(lastnOut);
conf.inputPreProcessor(i + 1,new CnnToRnnPreProcessor(lastHeight,lastWidth,lastOutChannels));
}
 else {
feedForwardLayer=(FeedForwardLayer)outputLayer;
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer.setNIn(lastnOut);
conf.inputPreProcessor(i + 1,new FeedForwardToRnnPreProcessor());
}
break;
case "BatchNormalization":
case "ActivationLayer":
feedForwardLayer=(FeedForwardLayer)outputLayer;
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer.setNOut(lastnOut);
break;
case "LocalResponseNormalization":
throw new UnsupportedOperationException("LocalResponse should not follow " + inLayerName);
}
break;
case "LocalResponseNormalization":
if (i == 0) throw new UnsupportedOperationException("Unsupported path: first layer shouldn't be " + inLayerName);
switch (outputLayer.getClass().getSimpleName()) {
case "ConvolutionLayer":
ConvolutionLayer nextConv=(ConvolutionLayer)outputLayer;
storeNInAndNOut(outLayerName,lastOutChannels);
nextConv.setNIn(lastnOut);
break;
case "RecursiveAutoEncoder":
case "RBM":
case "DenseLayer":
case "OutputLayer":
feedForwardLayer=(FeedForwardLayer)outputLayer;
lastnOut=lastHeight * lastWidth * lastOutChannels;
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer.setNIn(lastnOut);
conf.inputPreProcessor(i + 1,new CnnToFeedForwardPreProcessor(lastHeight,lastWidth,lastOutChannels));
break;
case "GravesLSTM":
case "GravesBidirectionalLSTM":
case "RnnOutputLayer":
feedForwardLayer=(FeedForwardLayer)outputLayer;
lastnOut=lastHeight * lastWidth * lastOutChannels;
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer.setNIn(lastnOut);
conf.inputPreProcessor(i + 1,new CnnToRnnPreProcessor(lastHeight,lastWidth,lastOutChannels));
break;
case "BatchNormalization":
throw new UnsupportedOperationException("BaseNormalization should not follow a LocalResponse layer.");
case "ActivationLayer":
feedForwardLayer=(FeedForwardLayer)outputLayer;
storeNInAndNOut(outLayerName,lastnOut);
feedForwardLayer.setNOut(lastnOut);
useCNN=true;
break;
}
break;
case "RnnOutputLayer":
case "OutputLayer":
throw new UnsupportedOperationException("OutputLayer should be the last layer");
}
}
 else throw new UnsupportedOperationException("Unsupported path: final " + inputLayer.getClass().getSimpleName() + " layer");
}
