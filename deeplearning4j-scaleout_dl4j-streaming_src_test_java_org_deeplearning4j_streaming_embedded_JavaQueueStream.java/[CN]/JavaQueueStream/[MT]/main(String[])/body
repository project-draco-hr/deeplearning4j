{
  SparkConf sparkConf=new SparkConf().setMaster("local[*]");
  JavaStreamingContext ssc=new JavaStreamingContext(sparkConf,new Duration(1000));
  Queue<JavaRDD<Integer>> rddQueue=new LinkedList<>();
  List<Integer> list=Lists.newArrayList();
  for (int i=0; i < 1000; i++) {
    list.add(i);
  }
  for (int i=0; i < 30; i++) {
    rddQueue.add(ssc.sparkContext().parallelize(list));
  }
  JavaDStream<Integer> inputStream=ssc.queueStream(rddQueue);
  JavaPairDStream<Integer,Integer> mappedStream=inputStream.mapToPair(new PairFunction<Integer,Integer,Integer>(){
    @Override public Tuple2<Integer,Integer> call(    Integer i){
      return new Tuple2<>(i % 10,1);
    }
  }
);
  JavaPairDStream<Integer,Integer> reducedStream=mappedStream.reduceByKey(new Function2<Integer,Integer,Integer>(){
    @Override public Integer call(    Integer i1,    Integer i2){
      return i1 + i2;
    }
  }
);
  reducedStream.print();
  ssc.start();
  ssc.awaitTermination();
}
