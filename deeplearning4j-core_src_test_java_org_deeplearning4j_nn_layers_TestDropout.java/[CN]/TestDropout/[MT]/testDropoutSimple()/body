{
  int nIn=8;
  int nOut=4;
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).updater(Updater.SGD).iterations(1).regularization(true).dropOut(0.5).list().layer(0,new OutputLayer.Builder().lossFunction(LossFunctions.LossFunction.MSE).nIn(nIn).nOut(nOut).weightInit(WeightInit.XAVIER).build()).backprop(true).pretrain(false).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  net.init();
  int nTests=15;
  Nd4j.getRandom().setSeed(12345);
  int noDropoutCount=0;
  for (int i=0; i < nTests; i++) {
    INDArray in=Nd4j.rand(1,nIn);
    INDArray out=Nd4j.rand(1,nOut);
    INDArray inCopy=in.dup();
    net.fit(new DataSet(in,out));
    INDArray l0Input=net.getLayer(0).input();
    for (int j=0; j < inCopy.length(); j++) {
      double origValue=inCopy.getDouble(j);
      double doValue=l0Input.getDouble(j);
      if (doValue > 0.0) {
        assertEquals(origValue * 2.0,doValue,0.0001);
      }
    }
    INDArray in2=Nd4j.rand(1,nIn);
    INDArray out2=Nd4j.rand(1,nOut);
    INDArray outTest1=net.output(in2,false);
    INDArray outTest2=net.output(in2,false);
    INDArray outTest3=net.output(in2,false);
    assertEquals(outTest1,outTest2);
    assertEquals(outTest1,outTest3);
    double score1=net.score(new DataSet(in2,out2),false);
    double score2=net.score(new DataSet(in2,out2),false);
    double score3=net.score(new DataSet(in2,out2),false);
    assertEquals(score1,score2,0.0);
    assertEquals(score1,score3,0.0);
  }
  if (noDropoutCount >= nTests / 3) {
    fail("Too many instances of dropout not being applied");
  }
}
