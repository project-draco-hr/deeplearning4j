{
  int dataSetObjSize=5;
  int batchSizePerExecutor=10;
  String path=FilenameUtils.concat(System.getProperty("java.io.tmpdir"),"dl4j_testpreprocdata");
  File f=new File(path);
  if (f.exists())   f.delete();
  f.mkdir();
  DataSetIterator iter=new IrisDataSetIterator(5,150);
  int i=0;
  while (iter.hasNext()) {
    File f2=new File(FilenameUtils.concat(path,"data" + (i++) + ".bin"));
    iter.next().save(f2);
  }
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().updater(Updater.RMSPROP).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).iterations(1).list().layer(0,new org.deeplearning4j.nn.conf.layers.DenseLayer.Builder().nIn(4).nOut(3).activation("tanh").build()).layer(1,new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).nIn(3).nOut(3).activation("softmax").build()).pretrain(false).backprop(true).build();
  SparkDl4jMultiLayer sparkNet=new SparkDl4jMultiLayer(sc,conf,new ParameterAveragingTrainingMaster.Builder(numExecutors(),dataSetObjSize).batchSizePerWorker(batchSizePerExecutor).averagingFrequency(1).repartionData(Repartition.Always).build());
  sparkNet.setCollectTrainingStats(true);
  JavaPairRDD<String,PortableDataStream> pds=sc.binaryFiles(path);
  assertEquals(150 / dataSetObjSize,pds.count());
  sparkNet.fit(path);
  SparkTrainingStats sts=sparkNet.getSparkTrainingStats();
  int expNumFits=12;
  assertTrue(Math.abs(expNumFits - sts.getValue("ParameterAveragingWorkerFitTimesMs").size()) < 3);
  assertEquals(3,sts.getValue("ParameterAveragingMasterMapPartitionsTimesMs").size());
}
