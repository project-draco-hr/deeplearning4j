{
  Map<Integer,Double> learningRateAfter=new HashMap<>();
  learningRateAfter.put(1,0.2);
  int iterations=2;
  int[] nIns={4,2};
  int[] nOuts={2,3};
  for (  org.deeplearning4j.nn.conf.Updater updaterFunc : updaters) {
    double lr=1e-2;
    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().learningRate(lr).learningRateDecayPolicy(LearningRatePolicy.Schedule).learningRateSchedule(learningRateAfter).iterations(iterations).updater(updaterFunc).list().layer(0,new DenseLayer.Builder().nIn(nIns[0]).nOut(nOuts[0]).build()).layer(1,new OutputLayer.Builder().nIn(nIns[1]).nOut(nOuts[1]).build()).backprop(true).pretrain(false).build();
    MultiLayerNetwork net=new MultiLayerNetwork(conf);
    net.init();
    Updater updater=UpdaterCreator.getUpdater(net);
    String wKey, bKey;
    Gradient gradientActual=new DefaultGradient();
    Gradient gradientExpected=new DefaultGradient();
    for (int k=0; k < net.getnLayers(); k++) {
      wKey=String.valueOf(k) + "_" + DefaultParamInitializer.WEIGHT_KEY;
      gradientActual.setGradientFor(wKey,weightGradient);
      gradientExpected.setGradientFor(wKey,weightGradient);
      bKey=String.valueOf(k) + "_" + DefaultParamInitializer.BIAS_KEY;
      gradientActual.setGradientFor(bKey,biasGradient);
      gradientExpected.setGradientFor(bKey,biasGradient);
    }
    for (int i=0; i < 2; i++) {
      updater.update(net,gradientActual,i,1);
      if (updaterFunc.equals(org.deeplearning4j.nn.conf.Updater.SGD))       lr=testSGDComputation(gradientActual,gradientExpected,lr,learningRateAfter,i);
 else       if (updaterFunc.equals(org.deeplearning4j.nn.conf.Updater.ADAGRAD))       lr=testAdaGradComputation(gradientActual,gradientExpected,lr,learningRateAfter,i);
 else       if (updaterFunc.equals(org.deeplearning4j.nn.conf.Updater.RMSPROP))       lr=testRMSPropComputation(gradientActual,gradientExpected,lr,learningRateAfter,i);
      assertEquals(lr,net.getLayer(1).conf().getLearningRateByParam("W"),1e-4);
    }
  }
}
