{
  textVectorizer=new TfidfVectorizer.Builder().cache(vocabCache).iterate(sentenceIterator).stopWords(stopWords).stem(stem).build();
  textVectorizer.fit();
  coOccurrences=new CoOccurrences.Builder().cache(vocabCache).iterate(sentenceIterator).tokenizer(tokenizerFactory).windowSize(windowSize).build();
  coOccurrences.fit();
  weightLookupTable=new GloveWeightLookupTable.Builder().xMax(xMax).cache(vocabCache).lr(learningRate).vectorLength(layerSize).build();
  final List<List<VocabWord>> miniBatches=new ArrayList<>();
  new Thread(new Runnable(){
    @Override public void run(){
      for (      String s : coOccurrences.getCoOCurreneCounts().keySet()) {
        for (        String s1 : coOccurrences.getCoOCurreneCounts().getCounter(s).keySet()) {
          VocabWord vocabWord=vocabCache.wordFor(s);
          VocabWord vocabWord1=vocabCache.wordFor(s1);
          miniBatches.add(Arrays.asList(vocabWord,vocabWord1));
          if (miniBatches.size() >= batchSize) {
            jobQueue.add(new ArrayList<>(miniBatches));
            miniBatches.clear();
          }
        }
      }
    }
  }
).start();
  if (!miniBatches.isEmpty())   jobQueue.add(miniBatches);
  final AtomicInteger processed=new AtomicInteger(coOccurrences.getCoOCurreneCounts().size());
  Parallelization.runInParallel(Runtime.getRuntime().availableProcessors(),new Runnable(){
    @Override public void run(){
      while (processed.get() > 0) {
        List<List<VocabWord>> batch=jobQueue.poll();
        if (batch == null)         continue;
        for (        List<VocabWord> list : batch) {
          VocabWord w1=list.get(0);
          VocabWord w2=list.get(1);
          double weight=coOccurrences.getCoOCurreneCounts().getCount(w1.getWord(),w2.getWord());
          weightLookupTable.iterateSample(w1,w2,learningRate,weight);
          processed.decrementAndGet();
        }
      }
    }
  }
);
}
