{
  int totalExamples=10 * miniBatchSize;
  if (totalExamples > 150) {
    totalExamples=miniBatchSize * (150 / miniBatchSize);
  }
  if (miniBatchSize > 150)   fail();
  DataSetIterator iris=new IrisDataSetIterator(miniBatchSize,totalExamples);
  if (!activationFunction.equals("tanh") && !activationFunction.equals("sigmoid"))   fail();
  boolean sigmoid=activationFunction.equals("sigmoid");
  MultiLayerNetwork network=new MultiLayerNetwork(getIrisMLPSimpleConfig(hiddenLayerSizes,activationFunction));
  network.init();
  Layer[] layers=network.getLayers();
  int nLayers=layers.length;
  final float learningRate=0.1f;
  while (iris.hasNext()) {
    DataSet data=iris.next();
    INDArray x=data.getFeatureMatrix();
    INDArray y=data.getLabels();
    INDArray[] layerWeights=new INDArray[nLayers];
    INDArray[] layerBiases=new INDArray[nLayers];
    for (int i=0; i < nLayers; i++) {
      layerWeights[i]=layers[i].getParam(DefaultParamInitializer.WEIGHT_KEY).dup();
      layerBiases[i]=layers[i].getParam(DefaultParamInitializer.BIAS_KEY).dup();
    }
    INDArray[] layerZs=new INDArray[nLayers];
    INDArray[] layerActivations=new INDArray[nLayers];
    for (int i=0; i < nLayers; i++) {
      INDArray layerInput=(i == 0 ? x : layerActivations[i - 1]);
      layerZs[i]=layerInput.mmul(layerWeights[i]).addiRowVector(layerBiases[i]);
      if (sigmoid)       layerActivations[i]=(i == nLayers - 1 ? doSoftmax(layerZs[i].dup(),1) : doSigmoid(layerZs[i].dup()));
 else       layerActivations[i]=(i == nLayers - 1 ? doSoftmax(layerZs[i].dup(),1) : doTanh(layerZs[i].dup()));
    }
    INDArray[] deltas=new INDArray[nLayers];
    deltas[nLayers - 1]=layerActivations[nLayers - 1].sub(y);
    assertArrayEquals(deltas[nLayers - 1].shape(),new int[]{miniBatchSize,3});
    for (int i=nLayers - 2; i >= 0; i--) {
      INDArray sigmaPrimeOfZ;
      if (sigmoid)       sigmaPrimeOfZ=doSigmoidDerivative(layerZs[i]);
 else       sigmaPrimeOfZ=doTanhDerivative(layerZs[i]);
      deltas[i]=deltas[i + 1].mmul(layerWeights[i + 1].transpose()).mul(sigmaPrimeOfZ);
      assertArrayEquals(deltas[i].shape(),new int[]{miniBatchSize,hiddenLayerSizes[i]});
    }
    INDArray[] dLdw=new INDArray[nLayers];
    INDArray[] dLdb=new INDArray[nLayers];
    for (int i=0; i < nLayers; i++) {
      INDArray prevActivations=(i == 0 ? x : layerActivations[i - 1]);
      dLdw[i]=deltas[i].transpose().mmul(prevActivations).div(miniBatchSize).transpose();
      dLdb[i]=deltas[i].mean(0);
      int nIn=(i == 0 ? 4 : hiddenLayerSizes[i - 1]);
      int nOut=(i < nLayers - 1 ? hiddenLayerSizes[i] : 3);
      assertArrayEquals(dLdw[i].shape(),new int[]{nIn,nOut});
      assertArrayEquals(dLdb[i].shape(),new int[]{1,nOut});
    }
    INDArray[] expectedWeights=new INDArray[nLayers];
    INDArray[] expectedBiases=new INDArray[nLayers];
    for (int i=0; i < nLayers; i++) {
      expectedWeights[i]=layerWeights[i].sub(dLdw[i].mul(learningRate));
      expectedBiases[i]=layerBiases[i].sub(dLdb[i].mul(learningRate));
      int nIn=(i == 0 ? 4 : hiddenLayerSizes[i - 1]);
      int nOut=(i < nLayers - 1 ? hiddenLayerSizes[i] : 3);
      assertArrayEquals(expectedWeights[i].shape(),new int[]{nIn,nOut});
      assertArrayEquals(expectedBiases[i].shape(),new int[]{1,nOut});
    }
    network.fit(data);
    INDArray[] layerWeightsAfter=new INDArray[nLayers];
    INDArray[] layerBiasesAfter=new INDArray[nLayers];
    for (int i=0; i < nLayers; i++) {
      layerWeightsAfter[i]=layers[i].getParam(DefaultParamInitializer.WEIGHT_KEY).dup();
      layerBiasesAfter[i]=layers[i].getParam(DefaultParamInitializer.BIAS_KEY).dup();
    }
    float eps=1e-1f;
    for (int i=0; i < nLayers; i++) {
      float[] expWeights=asFloat(expectedWeights[i]);
      float[] actWeights=asFloat(layerWeightsAfter[i]);
      assertArrayEquals(expWeights,actWeights,eps);
      float[] expBiases=asFloat(expectedBiases[i]);
      float[] actBiases=asFloat(layerBiasesAfter[i]);
      assertArrayEquals(expBiases,actBiases,eps);
    }
  }
}
