{
  LinAlgExceptions.assertRows(input,labels);
  adaGrad.setMasterStepSize(lr);
  biasAdaGrad.setMasterStepSize(lr);
  INDArray netOut=output(input);
  INDArray dy=labels.sub(netOut);
  dy.divi(input.rows());
  INDArray wGradient=getWeightGradient();
  if (conf.isUseAdaGrad())   wGradient.muli(adaGrad.getLearningRates(wGradient));
 else   wGradient.muli(lr);
  if (conf.isUseAdaGrad())   dy.muliRowVector(biasAdaGrad.getLearningRates(dy.mean(0)));
 else   dy.muli(lr);
  dy.divi(input.rows());
  INDArray bGradient=dy.mean(0);
  if (conf.isConstrainGradientToUnitNorm()) {
    wGradient.divi(wGradient.norm2(Integer.MAX_VALUE));
    bGradient.divi(bGradient.norm2(Integer.MAX_VALUE));
  }
  return new OutputLayerGradient(wGradient,bGradient);
}
