{
  List<Pair<INDArray,INDArray>> deltaRet=new ArrayList<>();
  List<INDArray> activations=feedForward();
  INDArray[] deltas=new INDArray[activations.size() - 1];
  INDArray[] preCons=new INDArray[activations.size() - 1];
  INDArray ix=activations.get(activations.size() - 1).sub(labels).div(labels.rows());
  List<INDArray> weights=new ArrayList<>();
  List<INDArray> biases=new ArrayList<>();
  List<ActivationFunction> activationFunctions=new ArrayList<>();
  for (int j=0; j < getLayers().length; j++) {
    weights.add(getLayers()[j].getParam(DefaultParamInitializer.WEIGHT_KEY));
    biases.add(getLayers()[j].getParam(DefaultParamInitializer.BIAS_KEY));
    activationFunctions.add(getLayers()[j].conf().getActivationFunction());
  }
  for (int i=weights.size() - 1; i >= 0; i--) {
    deltas[i]=activations.get(i).transpose().mmul(ix);
    preCons[i]=Transforms.pow(activations.get(i).transpose(),2).mmul(Transforms.pow(ix,2)).muli(labels.rows());
    applyDropConnectIfNecessary(deltas[i]);
    if (i > 0) {
      ix=ix.mmul(weights.get(i).transpose()).muli(activationFunctions.get(i - 1).applyDerivative(activations.get(i)));
    }
  }
  for (int i=0; i < deltas.length; i++) {
    if (defaultConfiguration.isConstrainGradientToUnitNorm())     deltaRet.add(new Pair<>(deltas[i].divi(deltas[i].norm2(Integer.MAX_VALUE)),preCons[i]));
 else     deltaRet.add(new Pair<>(deltas[i],preCons[i]));
  }
  return deltaRet;
}
