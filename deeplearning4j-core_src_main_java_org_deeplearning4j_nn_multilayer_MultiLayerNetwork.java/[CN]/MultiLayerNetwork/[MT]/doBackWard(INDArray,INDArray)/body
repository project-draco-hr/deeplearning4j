{
  setInput(input);
  this.labels=labels;
  Gradient nextGradients=new DefaultGradient();
  if (!(getOutputLayer() instanceof OutputLayer)) {
    log.warn("Warning: final layer isn't output layer. You can ignore this message if you just intend on using a a deep neural network with no output layer.");
    return;
  }
  OutputLayer output=(OutputLayer)getOutputLayer();
  if (labels == null)   throw new IllegalStateException("No labels found");
  if (output.conf().getWeightInit() == WeightInit.ZERO) {
    throw new IllegalStateException("Output layer weights cannot be intialized to zero when using backprop.");
  }
  ;
  output.setLabels(labels);
  for (int i=0; i < getLayerWiseConfigurations().getConf(0).getNumIterations(); i++) {
    int numLayers=getnLayers() - 1;
    List<Gradient> weightUpdates=new ArrayList<>();
    Pair<List<INDArray>,List<INDArray>> zsAndActivations=zsAndActivations();
    List<INDArray> activations=zsAndActivations.getSecond();
    INDArray outputActivation=activations.get(activations.size() - 1);
    List<INDArray> derivatives=zsAndActivations.getFirst();
    INDArray activationDeriv=derivatives.get(derivatives.size() - 1);
    INDArray layerInput=activations.get(activations.size() - 2);
    INDArray delta=outputActivation.sub(labels).transpose();
    if (output.conf().getLossFunction() != LossFunctions.LossFunction.XENT) {
      delta.muli(activationDeriv);
    }
    nextGradients.gradientForVariable().put(DefaultParamInitializer.WEIGHT_KEY,delta.mmul(layerInput).transpose());
    nextGradients.gradientForVariable().put(DefaultParamInitializer.BIAS_KEY,delta.transpose());
    weightUpdates.add(nextGradients);
    for (int j=numLayers - 1; j >= 0; j--) {
      INDArray currDerivative=derivatives.get(j);
      INDArray currActivation=activations.get(j);
      Layer nextLayer=getLayers()[j + 1];
      nextGradients=getLayers()[j].backwardGradient(currDerivative,nextLayer,nextGradients,currActivation);
      weightUpdates.add(nextGradients);
    }
    Collections.reverse(weightUpdates);
    for (int k=0; k < numLayers; k++) {
      Layer currLayer=getLayers()[k];
      for (      String paramType : weightUpdates.get(k).gradientForVariable().keySet()) {
        INDArray gradients=weightUpdates.get(k).getGradientFor(paramType);
        INDArray adjustedGradient=GradientAdjustment.updateGradientAccordingToParams(i,input.slices(),currLayer.conf(),currLayer.getParam(paramType),gradients,currLayer.getOptimizer().adaGradForVariables().get(paramType),currLayer.getOptimizer().getLastStep().get(paramType));
        currLayer.update(adjustedGradient,paramType);
      }
    }
    for (    IterationListener listener : listeners)     listener.iterationDone(getOutputLayer(),i);
  }
}
