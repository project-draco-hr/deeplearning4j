{
  setInput(input);
  this.labels=labels;
  feedForward();
  if (!(getOutputLayer() instanceof OutputLayer)) {
    log.warn("Warning: final layer isn't output layer. You can ignore this message if you just intend on using a a deep neural network with no output layer.");
    return;
  }
  OutputLayer output=(OutputLayer)getOutputLayer();
  if (labels == null)   throw new IllegalStateException("No labels found");
  output.setLabels(labels);
  INDArray outputActivate=output.activate();
  INDArray ixInitial=labels.sub(outputActivate).subi(Nd4j.getExecutioner().execAndReturn(Nd4j.getOpFactory().createTransform(getOutputLayer().conf().getActivationFunction(),outputActivate).derivative()));
  Gradient ix=new DefaultGradient();
  ix.gradientForVariable().put(DefaultParamInitializer.WEIGHT_KEY,ixInitial);
  for (int i=0; i < getLayerWiseConfigurations().getConf(0).getNumIterations(); i++) {
    List<Pair<Gradient,Gradient>> backwards=new ArrayList<>();
    List<INDArray> activations=feedForward();
    Pair<Gradient,Gradient> ixDelta=new Pair<>(ix,null);
    for (int layer=layers.length - 1; layer >= 0; layer--) {
      String activationFunction=layer > 0 ? layerWiseConfigurations.getConf(layer - 1).getActivationFunction() : layerWiseConfigurations.getConf(0).getActivationFunction();
      ixDelta=layers[layer].backWard(ixDelta.getFirst(),ixDelta.getSecond(),activations.get(layer),activationFunction);
      backwards.add(ixDelta);
    }
    Collections.reverse(backwards);
    for (int j=0; j < layers.length; j++)     layers[j].update(backwards.get(j).getSecond());
    for (    IterationListener listener : output.getIterationListeners())     listener.iterationDone(getOutputLayer(),i);
  }
}
