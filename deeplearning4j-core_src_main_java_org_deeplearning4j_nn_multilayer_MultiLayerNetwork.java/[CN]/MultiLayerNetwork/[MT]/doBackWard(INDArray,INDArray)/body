{
  setInput(input);
  this.labels=labels;
  if (!(getOutputLayer() instanceof OutputLayer)) {
    log.warn("Warning: final layer isn't output layer. You can ignore this message if you just intend on using a a deep neural network with no output layer.");
    return;
  }
  OutputLayer output=(OutputLayer)getOutputLayer();
  if (labels == null)   throw new IllegalStateException("No labels found");
  output.setLabels(labels);
  Gradient[] errors=new Gradient[getnLayers()];
  for (int i=0; i < getLayerWiseConfigurations().getConf(0).getNumIterations(); i++) {
    Pair<List<INDArray>,List<INDArray>> zsAndAtivations=zsAndActivations();
    List<INDArray> activations=zsAndAtivations.getSecond();
    List<INDArray> zs=zsAndAtivations.getFirst();
    INDArray outputActivate=activations.get(activations.size() - 1);
    INDArray ixInitial=outputActivate.sub(labels);
    Gradient ix=new DefaultGradient();
    ix.gradientForVariable().put(DefaultParamInitializer.WEIGHT_KEY,ixInitial);
    errors[errors.length - 1]=ix;
    for (int j=getnLayers() - 1; j >= 0; j--) {
      ix=getLayers()[j].backwardGradient(zs.get(j),ix);
      errors[j]=ix;
    }
    for (int k=0; k < getnLayers(); k++) {
      Gradient update=getLayers()[k].calcGradient(errors[k],activations.get(k));
      GradientAdjustment.updateGradientAccordingToParams(getLayers()[k].conf(),i,update,input.slices(),getLayers()[k].getOptimizer().adaGradForVariables(),getLayers()[k].getOptimizer().getLastStep(),getLayers()[k]);
      getLayers()[k].update(update);
    }
    for (    IterationListener listener : listeners)     listener.iterationDone(getOutputLayer(),i);
  }
}
