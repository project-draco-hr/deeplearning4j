{
  setInput(input);
  this.labels=labels;
  if (!(getOutputLayer() instanceof OutputLayer)) {
    log.warn("Warning: final layer isn't output layer. You can ignore this message if you just intend on using a a deep neural network with no output layer.");
    return;
  }
  OutputLayer outputLayer=(OutputLayer)getOutputLayer();
  if (labels == null)   throw new IllegalStateException("No labels found");
  if (outputLayer.conf().getWeightInit() == WeightInit.ZERO) {
    throw new IllegalStateException("Output layer weights cannot be initialized to zero when using backprop.");
  }
  outputLayer.setLabels(labels);
  for (int i=0; i < getLayerWiseConfigurations().getConf(getLayerWiseConfigurations().getConfs().size() - 1).getNumIterations(); i++) {
    int numLayers=getnLayers();
    INDArray delta=outputLayer.gradient().getGradientFor(DefaultParamInitializer.WEIGHT_KEY);
    List<Gradient> gradientUpdates=new ArrayList<>();
    Gradient outputLayerGradients=new DefaultGradient();
    outputLayerGradients.gradientForVariable().put(DefaultParamInitializer.WEIGHT_KEY,delta.mmul(outputLayer.getInput()).transpose());
    outputLayerGradients.gradientForVariable().put(DefaultParamInitializer.BIAS_KEY,delta.transpose().sum(0));
    gradientUpdates.add(outputLayerGradients);
    Pair<Gradient,INDArray> pair=new Pair<>(outputLayerGradients,outputLayer.getParam(DefaultParamInitializer.WEIGHT_KEY));
    for (int j=numLayers - 2; j >= 0; j--) {
      pair=getLayers()[j].backwardGradient(pair.getFirst(),pair.getSecond());
      gradientUpdates.add(pair.getFirst());
    }
    Collections.reverse(gradientUpdates);
    for (int k=0; k < numLayers; k++) {
      Layer currLayer=getLayers()[k];
      for (      String paramType : gradientUpdates.get(k).gradientForVariable().keySet()) {
        currLayer.getOptimizer().updateGradientAccordingToParams(gradientUpdates.get(k).getGradientFor(paramType),currLayer,input.size(0),paramType,i);
        INDArray update=gradientUpdates.get(k).getGradientFor(paramType);
        if (update != null)         currLayer.update(update,paramType);
      }
    }
    getOutputLayer().computeGradientAndScore();
    for (    IterationListener listener : listeners)     listener.iterationDone(getOutputLayer(),i);
  }
}
